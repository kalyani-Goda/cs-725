{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2N2S3KvdW3m",
        "outputId": "5b9dce04-f9b5-402c-898f-7b75aab6e868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  from audioop import bias\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import math\n",
        "\n",
        "# The seed will be fixed to 42 for this assignment.\n",
        "np.random.seed(35)\n",
        "\n",
        "NUM_FEATS = 90\n",
        "\n",
        "\n",
        "class Net(object):\n",
        "    '''\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_layers, hidden_size):\n",
        "        '''\n",
        "        Initialize the neural network.\n",
        "        Create weights and biases.\n",
        "\n",
        "        Here, we have provided an example structure for the weights and biases.\n",
        "        It is a list of weight and bias matrices, in which, the\n",
        "        dimensions of weights and biases are (assuming 1 input layer, 2 hidden layers, and 1 output layer):\n",
        "        weights: [(NUM_FEATS, num_units), (num_units, num_units), (num_units, num_units), (num_units, 1)]\n",
        "        biases: [(num_units, 1), (num_units, 1), (num_units, 1), (num_units, 1)]\n",
        "\n",
        "        Please note that this is just an example.\n",
        "        You are free to modify or entirely ignore this initialization as per your need.\n",
        "        Also you can add more state-tracking variables that might be useful to compute\n",
        "        the gradients efficiently.\n",
        "\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "                num_layers : Number of HIDDEN layers.\n",
        "                num_units : Number of units in each Hidden layer.\n",
        "        '''\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.a_states = []\n",
        "        self.h_states = []\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(\n",
        "            hidden_size[:-1], hidden_size[1:])]\n",
        "        self.biases = [np.random.randn(y, 1) for y in hidden_size[1:]]\n",
        "\n",
        "        # for w in self.biases:\n",
        "        # \tprint(w.shape)\n",
        "        # self.biases = []\n",
        "        # self.weights = []\n",
        "        # for i in range(self.num_layers - 1):\n",
        "        # \tif i==0:\n",
        "        # \t\t# Input layer\n",
        "        # \t\tself.weights.append(np.random.uniform(-1, 1, size=(NUM_FEATS, self.hidden_size[i])))\n",
        "        # \telse:\n",
        "        # \t\t# Hidden layer\n",
        "        # \t\tself.weights.append(np.random.uniform(-1, 1, size=(self.hidden_size[i], self.hidden_size[i+1])))\n",
        "\n",
        "        # \tself.biases.append(np.random.uniform(-1, 1, size=(self.hidden_size[i], 1)))\n",
        "\n",
        "        # Output layer\n",
        "        # self.biases.append(np.random.uniform(-1, 1, size=(1, 1)))\n",
        "        # self.weights.append(np.random.uniform(-1, 1, size=(self.hidden_size[-1], 1)))\n",
        "\n",
        "    def relu(self, X):\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def d_relu(self, h):\n",
        "        return (h > 0) * 1\n",
        "\n",
        "    def d_loss(self, y_hat, y):\n",
        "        return y_hat - y\n",
        "\n",
        "    def __call__(self, X):\n",
        "        '''\n",
        "        Forward propagate the input X through the network,\n",
        "        and return the output.\n",
        "\n",
        "        Note that for a classification task, the output layer should\n",
        "        be a softmax layer. So perform the computations accordingly\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "                X : Input to the network, numpy array of shape m x d\n",
        "        Returns\n",
        "        ----------\n",
        "                y : Output of the network, numpy array of shape m x 1\n",
        "        '''\n",
        "        self.a_states = []\n",
        "        self.h_states = []\n",
        "        out = X\n",
        "        self.a_states.append(X)\n",
        "        # self.h_states.append(X)\n",
        "        # print(self.num_layers)\n",
        "        h = 0\n",
        "        count = 0\n",
        "        for i in range(self.num_layers):\n",
        "            # print(f'weights = {self.weights[i].shape}  biases = {self.biases[i].shape}')\n",
        "            h = np.dot(out, self.weights[i].T) + self.biases[i].T\n",
        "            # print(\"in loop h:\", self.weights[i].T.shape)\n",
        "            out = self.relu(h)\n",
        "            self.a_states.append(out)\n",
        "            self.h_states.append(h)\n",
        "            count += 1\n",
        "        # print(\"count\", count)\n",
        "        # h = np.dot(out, self.weights[-1]) + self.biases[-1].T\n",
        "        # print(\"H_shape:\", h.shape)\n",
        "        return h\n",
        "\n",
        "    def backward(self, X, y, y_hat, lamda):\n",
        "        '''\n",
        "        Compute and return gradients loss with respect to weights and biases.\n",
        "        (dL/dW and dL/db)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "                X : Input to the network, numpy array of shape m x d\n",
        "                y : Output of the network, numpy array of shape m x 1\n",
        "                lamda : Regularization parameter.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "                del_W : derivative of loss w.r.t. all weight values (a list of matrices).\n",
        "                del_b : derivative of loss w.r.t. all bias values (a list of vectors).\n",
        "\n",
        "        Hint: You need to do a forward pass before performing backward pass.\n",
        "        '''\n",
        "        # d_W and d_b saves the accumulated gradients\n",
        "        d_W = [np.zeros(w.shape) for w in self.weights]\n",
        "        d_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        # derivative = self.d_loss(y_hat, y)\n",
        "        # y = y.values\n",
        "        for i, (xi, yi) in enumerate(zip(X, y)):\n",
        "            # print(y_hat[i][0],yi)\n",
        "            # print(\"backward\",y_hat[i].shape, yi)\n",
        "            derivative = (self.d_loss(y_hat[i], yi)).reshape(1, 1)\n",
        "            # print(i)\n",
        "            del_W = [np.zeros(w.shape) for w in self.weights]\n",
        "            del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\n",
        "            del_b[-1] = derivative\n",
        "            del_W[-1] = np.dot(self.a_states[-2][i].reshape(-1, 1), derivative).T\n",
        "            # print(\"delw shape:\", del_W[-1].shape)\n",
        "            for j in range(2, self.num_layers + 1):\n",
        "                h = self.h_states[-j][i]\n",
        "                relu_dash = self.d_relu(h)\n",
        "                # print(relu_dash.shape, derivative.shape, self.weights[-j+1].shape)\n",
        "                derivative = np.dot(self.weights[-j + 1].T, derivative) * relu_dash.reshape(-1, 1)\n",
        "                del_b[-j] = derivative\n",
        "                # print(j, self.a_states[-4])\n",
        "                del_W[-j] = np.dot(derivative, self.a_states[-j - 1][i].reshape(1, -1))\n",
        "                # print(\"delw shape:\", j, del_W[-j].shape)\n",
        "\n",
        "            d_W = [dw + delw for dw, delw in zip(d_W, del_W)]\n",
        "            d_b = [db + delb for db, delb in zip(d_b, del_b)]\n",
        "            d_W = [(dw + 2 * lamda * w) / X.shape[0] for w, dw in zip(self.weights, d_W)]\n",
        "            d_b = [(db + 2 * lamda * b) / X.shape[0] for b, db in zip(self.biases, d_b)]\n",
        "            # instead of accumulating, can create a list of all derivative of weights\n",
        "        return d_W, d_b\n",
        "\n",
        "\n",
        "class Optimizer(object):\n",
        "    '''\n",
        "    '''\n",
        "\n",
        "    def __init__(self, learning_rate, net, gamma=0.0, alpha=1):\n",
        "        '''\n",
        "        Create a Gradient Descent based optimizer with given\n",
        "        learning rate.\n",
        "\n",
        "        Other parameters can also be passed to create different types of\n",
        "        optimizers.\n",
        "\n",
        "        Hint: You can use the class members to track various states of the\n",
        "        optimizer.\n",
        "        '''\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_state = [np.zeros(w.shape) for w in net.weights]\n",
        "        self.bias_state = [np.zeros(b.shape) for b in net.biases]\n",
        "\n",
        "    def step(self, weights, biases, delta_weights, delta_biases, lamda, t, epoch):\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "                weights: Current weights of the network.\n",
        "                biases: Current biases of the network.\n",
        "                delta_weights: Gradients of weights with respect to loss.\n",
        "                delta_biases: Gradients of biases with respect to loss.\n",
        "                To do: RMS prop, momentum, ADAM,adagrad, adaboost\n",
        "        '''\n",
        "        # right now this function receives sum of all gradients\n",
        "        eps = 1e-9\n",
        "        # right now this function receives sum of all gradients\n",
        "        # self.weight_state = [self.gamma * sw + (1 - self.gamma) * (dw ** 2) for sw, dw in\n",
        "        #                      zip(self.weight_state, delta_weights)]\n",
        "        # self.bias_state = [self.gamma * b + (1 - self.gamma) * (db ** 2) for b, db in\n",
        "        #                    zip(self.bias_state, delta_biases)]\n",
        "        weights = [w -\n",
        "                   self.learning_rate * dw for w, dw in\n",
        "                   zip(weights, delta_weights)]\n",
        "        biases = [b -\n",
        "                  self.learning_rate * db for b, db in\n",
        "                  zip(biases, delta_biases)]\n",
        "        # weights = [w-lamda/t * w for w in weights]\n",
        "        # if epoch % 25 == 0:\n",
        "        #     self.learning_rate *= 0.95\n",
        "        '''weights = [(1 - self.learning_rate * lamda) * w -\n",
        "                   self.learning_rate * dw for w, dw in zip(weights, delta_weights)]\n",
        "        biases = [(1 - self.learning_rate * lamda) * b -\n",
        "                  self.learning_rate * db for b, db in zip(biases, delta_biases)]\n",
        "        return weights, biases'''\n",
        "        return weights, biases\n",
        "\n",
        "\n",
        "def loss_mse(y, y_hat):\n",
        "    '''\n",
        "    Compute Mean Squared Error (MSE) loss between ground-truth and predicted values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "            y : targets, numpy array of shape m x 1\n",
        "            y_hat : predictions, numpy array of shape m x 1\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "            MSE loss between y and y_hat.\n",
        "    '''\n",
        "    return np.mean((y_hat - y) ** 2)\n",
        "\n",
        "\n",
        "def loss_regularization(weights, biases):\n",
        "    '''\n",
        "    Compute l2 regularization loss.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "            weights and biases of the network.\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "            l2 regularization loss\n",
        "    '''\n",
        "    sum = 0\n",
        "    for w in weights:\n",
        "        sum += np.sum(w ** 2)\n",
        "    for b in biases:\n",
        "        sum += np.sum(b ** 2)\n",
        "    # print(f'Sum = {sum}')\n",
        "    return sum\n",
        "\n",
        "\n",
        "def loss_fn(y, y_hat, weights, biases, lamda):\n",
        "    '''\n",
        "    Compute loss =  loss_mse(..) + lamda * loss_regularization(..)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "            y : targets, numpy array of shape m x 1\n",
        "            y_hat : predictions, numpy array of shape m x 1\n",
        "            weights and biases of the network\n",
        "            lamda: Regularization parameter\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "            l2 regularization loss\n",
        "    '''\n",
        "    # print(loss_mse(y, y_hat))\n",
        "    return loss_mse(y, y_hat) + lamda * loss_regularization(weights, biases)\n",
        "\n",
        "\n",
        "def rmse(y, y_hat):\n",
        "    '''\n",
        "    Compute Root Mean Squared Error (RMSE) loss between ground-truth and predicted values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "            y : targets, numpy array of shape m x 1\n",
        "            y_hat : predictions, numpy array of shape m x 1\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "            RMSE between y and y_hat.\n",
        "    '''\n",
        "    rmse_loss = np.sqrt(loss_mse(y, y_hat))\n",
        "    return rmse_loss\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y, y_hat):\n",
        "    '''\n",
        "    Compute cross entropy loss\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "            y : targets, numpy array of shape m x 1\n",
        "            y_hat : predictions, numpy array of shape m x 1\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "            cross entropy loss\n",
        "    '''\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def train(\n",
        "        net, optimizer, lamda, batch_size, max_epochs,\n",
        "        train_input, train_target,\n",
        "        dev_input, dev_target\n",
        "):\n",
        "    '''\n",
        "        In this function, you will perform following steps:\n",
        "            1. Run gradient descent algorithm for `max_epochs` epochs.\n",
        "            2. For each bach of the training data\n",
        "                1.1 Compute gradients\n",
        "                1.2 Update weights and biases using step() of optimizer.\n",
        "            3. Compute RMSE on dev data after running `max_epochs` epochs.\n",
        "\n",
        "        Here we have added the code to loop over batches and perform backward pass\n",
        "        for each batch in the loop.\n",
        "        For this code also, you are free to heavily modify it.\n",
        "        '''\n",
        "    m = train_input.shape[0]\n",
        "    history_dev_rmse = []\n",
        "    history_epoch = []\n",
        "    history_train_rmse = []\n",
        "    tolerance = 0\n",
        "    for e in range(max_epochs):\n",
        "        epoch_loss = 0.\n",
        "        step = 1\n",
        "        history_epoch.append(e)\n",
        "        for i in range(0, m, batch_size):\n",
        "            # print(\"Step=\", i)\n",
        "            batch_input = train_input[i:i + batch_size]\n",
        "            # print(\"shape\", batch_input.shape)\n",
        "            batch_target = train_target[i:i + batch_size]\n",
        "            # print(\"shape\", batch_target.shape)\n",
        "            pred = net(batch_input)\n",
        "            # print(\"Pred_shape\", pred.shape)\n",
        "            # batch_target = batch_target.reshape(-1,1)\n",
        "            # print(pred.shape)\n",
        "            # print(batch_target.shape)\n",
        "            # Compute gradients of loss w.r.t. weights and biases\n",
        "            dW, db = net.backward(batch_input, batch_target, pred, lamda)\n",
        "            # print('backweights',dW)\n",
        "            # print('backbiases',db)\n",
        "\n",
        "            # Get updated weights based on current weights and gradients\n",
        "            weights_updated, biases_updated = optimizer.step(\n",
        "                net.weights, net.biases, dW, db, lamda, batch_size, e + 1)\n",
        "\n",
        "            # Update model's weights and biases\n",
        "            net.weights = weights_updated\n",
        "            net.biases = biases_updated\n",
        "            # print('weights',np.array(net.weights))\n",
        "            # print('biases',np.array(net.biases))\n",
        "\n",
        "            # Compute loss for the batch\n",
        "            batch_loss = loss_fn(batch_target, pred,\n",
        "                                 net.weights, net.biases, lamda)\n",
        "            epoch_loss += batch_loss\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                # print(e,step,batch_loss.value)\n",
        "                print(f'Epochs:{e + 1}\\tstep:{step}\\tbatch_loss:{batch_loss:.4f}')\n",
        "                # print('Epochs: '+e+' Step: '+step+' batch_loss: '+batch_loss)\n",
        "\n",
        "            step += 1\n",
        "        print(f'Epoch loss: {epoch_loss}')\n",
        "        if (e + 1) % 25 == 0:\n",
        "            optimizer.learning_rate *= 0.9\n",
        "\n",
        "        # Write any early stopping conditions required (only for Part 2)\n",
        "        # Hint: You can also compute dev_rmse here and use it in the early\n",
        "        # \t\tstopping condition.\n",
        "\n",
        "        # After running `max_epochs` (for Part 1) epochs OR early stopping (for Part 2), compute the RMSE on dev data.\n",
        "        dev_pred = net(dev_input)\n",
        "        train_pred = net(train_input)\n",
        "\n",
        "        dev_rmse = rmse(dev_target.reshape(-1, 1), dev_pred)\n",
        "        # print(train_target.shape)\n",
        "        train_rmse = rmse(train_target.reshape(-1, 1), train_pred)\n",
        "        history_train_rmse.append(train_rmse)\n",
        "        history_dev_rmse.append(dev_rmse)\n",
        "\n",
        "        print('RMSE on dev data: {:.5f}'.format(dev_rmse))\n",
        "        print('RMSE on train data: {:.5f}'.format(train_rmse))\n",
        "        if len(history_dev_rmse) > 1 and history_dev_rmse[e] > history_dev_rmse[e - 1]:\n",
        "            tolerance += 1\n",
        "        else:\n",
        "            tolerance = 0\n",
        "        if tolerance > 4:\n",
        "            break\n",
        "    # print('len of train',len(history_train_rmse))\n",
        "    # print('len of train',len(history_dev_rmse))\n",
        "    xpoint = np.array(history_epoch)\n",
        "    ypoint = np.array(history_train_rmse)\n",
        "    plt.xlabel(\"epoch\")\n",
        "    zpoint = np.array(history_dev_rmse)\n",
        "    plt.title(f'dev vs rmse with rmsprop alpha = {optimizer.alpha},gamma = {optimizer.gamma}')\n",
        "    plt.plot(xpoint, ypoint, color='g', label='train_rmse')\n",
        "    plt.plot(xpoint, zpoint, color='r', label='dev_rmse')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_test_data_predictions(net, inputs, target):\n",
        "    '''\n",
        "    Perform forward pass on test data and get the final predictions that can\n",
        "    be submitted on Kaggle.\n",
        "    Write the final predictions to the part2.csv file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        net : trained neural network\n",
        "        inputs : test input, numpy array of shape m x d\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "        predictions (optional): Predictions obtained from forward pass\n",
        "                                on test data, numpy array of shape m x 1\n",
        "    '''\n",
        "    pred = net(inputs)\n",
        "    # pred = (pred * (2011 - 1922)) + 1922\n",
        "    # target = (target * (2011 - 1922)) + 1922\n",
        "    # print(np.max(pred), np.min(pred))\n",
        "    history_prediction = []\n",
        "    history_target = []\n",
        "    history_count = []\n",
        "    correct_pred = 0\n",
        "    wrong_pred = 0\n",
        "    f = open('22d0367_1050.csv', \"w\", newline='')\n",
        "    writer = csv.writer(f)\n",
        "    init_list = [\"Id\", \"Predictions\"]\n",
        "    writer.writerow(init_list)\n",
        "    for i, p in enumerate(pred):\n",
        "        write_list = [i + 1, p[0]]\n",
        "        writer.writerow(write_list)\n",
        "        history_prediction.append(p)\n",
        "        # history_target.append(t)\n",
        "        # history_count.append(i)\n",
        "        # if abs(int(float(p[0])) - int(t)) < 6:\n",
        "        #     correct_pred = correct_pred + 1\n",
        "        # else:\n",
        "        #     wrong_pred = wrong_pred + 1\n",
        "\n",
        "        # if i > 100:\n",
        "        # break\n",
        "    f.close()\n",
        "    # accuracy = (correct_pred / (correct_pred + wrong_pred)) * 100\n",
        "    # print('accuracy', accuracy)\n",
        "    # xpoint = np.array(history_count)\n",
        "    # ypoint = np.array(history_prediction)\n",
        "    # zpoint = np.array(history_target)\n",
        "    # plt.plot(xpoint, ypoint, color='g', label='pred')\n",
        "    # plt.plot(xpoint, zpoint, color='r', label='target')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "def min_max_scaler(x):\n",
        "    # x_min = np.min(x, axis=0)\n",
        "    # x_max = np.max(x, axis=0)\n",
        "    # x = np.divide((x - x_min), (x_max - x_min))\n",
        "    x_mean = np.mean(x, axis=0)\n",
        "    x_var = np.var(x, axis=0)\n",
        "    x = (x - x_mean) / x_var\n",
        "    return x\n",
        "\n",
        "def read_data():\n",
        "    '''\n",
        "    Read the train, dev, and test datasets\n",
        "    '''\n",
        "    TRAIN_FILE = \"/content/drive/MyDrive/data/train_selected_features.csv\"\n",
        "    DEV_FILE = \"/content/drive/MyDrive/data/dev_selected_features.csv\"\n",
        "    TEST_FILE = \"/content/drive/MyDrive/data/test_selected_features.csv\"\n",
        "    TEST_FILE2 = \"/content/drive/MyDrive/data/test2.csv\"\n",
        "    train_df = pd.read_csv(TRAIN_FILE)\n",
        "    test_df = pd.read_csv(TEST_FILE)\n",
        "    test_df2 = pd.read_csv(TEST_FILE2)\n",
        "    dev_df = pd.read_csv(DEV_FILE)\n",
        "\n",
        "    temp = train_df.to_numpy()\n",
        "    np.random.shuffle(temp)\n",
        "    train_input = min_max_scaler(temp[:, 1:])\n",
        "    # train_input = min_max_scaler(train_df.iloc[:, 1:])\n",
        "    # train_target = min_max_scaler(train_df[train_df.columns[0]])\n",
        "    # train_target = np.divide((train_df.iloc[:, :1] - 1922), (2011 - 1922))\n",
        "    train_target = temp[:, 0]\n",
        "    dev_input = min_max_scaler(dev_df.to_numpy()[:, 1:])\n",
        "    # dev_input = min_max_scaler(dev_df.iloc[:, 1:])\n",
        "    # dev_target = min_max_scaler(dev_df[dev_df.columns[0]])\n",
        "    # dev_target = np.divide((dev_df.iloc[:, 1] - 1922), (2011 - 1922))\n",
        "    dev_target = dev_df.to_numpy()[:, 0]\n",
        "    # test_input = min_max_scaler(test_df.to_numpy())\n",
        "    # test_input = min_max_scaler(test_df.iloc[:, :])\n",
        "    # dev_target = dev_df[dev_df.columns[0]]\n",
        "    test_input = min_max_scaler(test_df.to_numpy())\n",
        "    test_input2 = min_max_scaler(test_df2.to_numpy())\n",
        "    # print(train_input.shape)\n",
        "    # print(train_target.shape)\n",
        "    # print(train_input.shape,train_target.shape)\n",
        "    return train_input, train_target, dev_input, dev_target, test_input, test_input2\n",
        "\n",
        "def main():\n",
        "    # Hyper-parameters\n",
        "    max_epochs = 1000\n",
        "    batch_size = 64\n",
        "    learning_rate = 0.0003\n",
        "    num_layers = 3\n",
        "    hidden_size = [67, 34, 17, 1]\n",
        "    lamda = 1  # Regularization Parameter\n",
        "    gamma = 0.95\n",
        "    alpha = 0.001\n",
        "\n",
        "    train_input, train_target, dev_input, dev_target, test_input, test_input2 = read_data()\n",
        "    # train_input = np.load(\"/content/drive/My Drive/SEM1/foundations of ML/ml_project/PCA Train Dataset_50.npy\").T\n",
        "    # dev_input = np.load(\"/content/drive/My Drive/SEM1/foundations of ML/ml_project/PCA Dev Dataset_50.npy\").T\n",
        "    # test_input = np.load(\"/content/drive/My Drive/SEM1/foundations of ML/ml_project/PCA Test Dataset_50.npy\").T\n",
        "\n",
        "\n",
        "    net = Net(num_layers, hidden_size)\n",
        "    optimizer = Optimizer(learning_rate, net, gamma, alpha)\n",
        "    train(\n",
        "        net, optimizer, lamda, batch_size, max_epochs,\n",
        "        train_input, train_target,\n",
        "        dev_input, dev_target\n",
        "    )\n",
        "    get_test_data_predictions(net, test_input, dev_target)\n",
        "    # print(dev_target)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sDDoiYeQBdaE",
        "outputId": "3aa3d69d-673a-4f42-8cde-2f08db92b487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epochs:667\tstep:550\tbatch_loss:823.4980\n",
            "Epochs:667\tstep:600\tbatch_loss:755.7612\n",
            "Epoch loss: 500815.16004451446\n",
            "RMSE on dev data: 9.89427\n",
            "RMSE on train data: 10.00891\n",
            "Epochs:668\tstep:50\tbatch_loss:769.0082\n",
            "Epochs:668\tstep:100\tbatch_loss:803.0481\n",
            "Epochs:668\tstep:150\tbatch_loss:800.6793\n",
            "Epochs:668\tstep:200\tbatch_loss:789.6467\n",
            "Epochs:668\tstep:250\tbatch_loss:949.5110\n",
            "Epochs:668\tstep:300\tbatch_loss:819.5674\n",
            "Epochs:668\tstep:350\tbatch_loss:793.1839\n",
            "Epochs:668\tstep:400\tbatch_loss:794.9355\n",
            "Epochs:668\tstep:450\tbatch_loss:803.3734\n",
            "Epochs:668\tstep:500\tbatch_loss:746.6901\n",
            "Epochs:668\tstep:550\tbatch_loss:823.3710\n",
            "Epochs:668\tstep:600\tbatch_loss:755.6352\n",
            "Epoch loss: 500734.3257612395\n",
            "RMSE on dev data: 9.89425\n",
            "RMSE on train data: 10.00889\n",
            "Epochs:669\tstep:50\tbatch_loss:768.8804\n",
            "Epochs:669\tstep:100\tbatch_loss:802.9216\n",
            "Epochs:669\tstep:150\tbatch_loss:800.5541\n",
            "Epochs:669\tstep:200\tbatch_loss:789.5206\n",
            "Epochs:669\tstep:250\tbatch_loss:949.3821\n",
            "Epochs:669\tstep:300\tbatch_loss:819.4402\n",
            "Epochs:669\tstep:350\tbatch_loss:793.0574\n",
            "Epochs:669\tstep:400\tbatch_loss:794.8087\n",
            "Epochs:669\tstep:450\tbatch_loss:803.2450\n",
            "Epochs:669\tstep:500\tbatch_loss:746.5640\n",
            "Epochs:669\tstep:550\tbatch_loss:823.2441\n",
            "Epochs:669\tstep:600\tbatch_loss:755.5094\n",
            "Epoch loss: 500653.61244225746\n",
            "RMSE on dev data: 9.89423\n",
            "RMSE on train data: 10.00887\n",
            "Epochs:670\tstep:50\tbatch_loss:768.7527\n",
            "Epochs:670\tstep:100\tbatch_loss:802.7954\n",
            "Epochs:670\tstep:150\tbatch_loss:800.4284\n",
            "Epochs:670\tstep:200\tbatch_loss:789.3947\n",
            "Epochs:670\tstep:250\tbatch_loss:949.2535\n",
            "Epochs:670\tstep:300\tbatch_loss:819.3132\n",
            "Epochs:670\tstep:350\tbatch_loss:792.9309\n",
            "Epochs:670\tstep:400\tbatch_loss:794.6823\n",
            "Epochs:670\tstep:450\tbatch_loss:803.1167\n",
            "Epochs:670\tstep:500\tbatch_loss:746.4380\n",
            "Epochs:670\tstep:550\tbatch_loss:823.1173\n",
            "Epochs:670\tstep:600\tbatch_loss:755.3836\n",
            "Epoch loss: 500572.94267915067\n",
            "RMSE on dev data: 9.89421\n",
            "RMSE on train data: 10.00885\n",
            "Epochs:671\tstep:50\tbatch_loss:768.6251\n",
            "Epochs:671\tstep:100\tbatch_loss:802.6691\n",
            "Epochs:671\tstep:150\tbatch_loss:800.3034\n",
            "Epochs:671\tstep:200\tbatch_loss:789.2688\n",
            "Epochs:671\tstep:250\tbatch_loss:949.1249\n",
            "Epochs:671\tstep:300\tbatch_loss:819.1862\n",
            "Epochs:671\tstep:350\tbatch_loss:792.8046\n",
            "Epochs:671\tstep:400\tbatch_loss:794.5557\n",
            "Epochs:671\tstep:450\tbatch_loss:802.9885\n",
            "Epochs:671\tstep:500\tbatch_loss:746.3121\n",
            "Epochs:671\tstep:550\tbatch_loss:822.9907\n",
            "Epochs:671\tstep:600\tbatch_loss:755.2580\n",
            "Epoch loss: 500492.3603267382\n",
            "RMSE on dev data: 9.89418\n",
            "RMSE on train data: 10.00883\n",
            "Epochs:672\tstep:50\tbatch_loss:768.4977\n",
            "Epochs:672\tstep:100\tbatch_loss:802.5430\n",
            "Epochs:672\tstep:150\tbatch_loss:800.1779\n",
            "Epochs:672\tstep:200\tbatch_loss:789.1430\n",
            "Epochs:672\tstep:250\tbatch_loss:948.9963\n",
            "Epochs:672\tstep:300\tbatch_loss:819.0592\n",
            "Epochs:672\tstep:350\tbatch_loss:792.6784\n",
            "Epochs:672\tstep:400\tbatch_loss:794.4293\n",
            "Epochs:672\tstep:450\tbatch_loss:802.8603\n",
            "Epochs:672\tstep:500\tbatch_loss:746.1863\n",
            "Epochs:672\tstep:550\tbatch_loss:822.8640\n",
            "Epochs:672\tstep:600\tbatch_loss:755.1325\n",
            "Epoch loss: 500411.8162500413\n",
            "RMSE on dev data: 9.89416\n",
            "RMSE on train data: 10.00881\n",
            "Epochs:673\tstep:50\tbatch_loss:768.3702\n",
            "Epochs:673\tstep:100\tbatch_loss:802.4169\n",
            "Epochs:673\tstep:150\tbatch_loss:800.0531\n",
            "Epochs:673\tstep:200\tbatch_loss:789.0172\n",
            "Epochs:673\tstep:250\tbatch_loss:948.8680\n",
            "Epochs:673\tstep:300\tbatch_loss:818.9325\n",
            "Epochs:673\tstep:350\tbatch_loss:792.5522\n",
            "Epochs:673\tstep:400\tbatch_loss:794.3030\n",
            "Epochs:673\tstep:450\tbatch_loss:802.7323\n",
            "Epochs:673\tstep:500\tbatch_loss:746.0606\n",
            "Epochs:673\tstep:550\tbatch_loss:822.7375\n",
            "Epochs:673\tstep:600\tbatch_loss:755.0071\n",
            "Epoch loss: 500331.35454769334\n",
            "RMSE on dev data: 9.89414\n",
            "RMSE on train data: 10.00879\n",
            "Epochs:674\tstep:50\tbatch_loss:768.2431\n",
            "Epochs:674\tstep:100\tbatch_loss:802.2912\n",
            "Epochs:674\tstep:150\tbatch_loss:799.9278\n",
            "Epochs:674\tstep:200\tbatch_loss:788.8918\n",
            "Epochs:674\tstep:250\tbatch_loss:948.7400\n",
            "Epochs:674\tstep:300\tbatch_loss:818.8058\n",
            "Epochs:674\tstep:350\tbatch_loss:792.4262\n",
            "Epochs:674\tstep:400\tbatch_loss:794.1770\n",
            "Epochs:674\tstep:450\tbatch_loss:802.6044\n",
            "Epochs:674\tstep:500\tbatch_loss:745.9350\n",
            "Epochs:674\tstep:550\tbatch_loss:822.6111\n",
            "Epochs:674\tstep:600\tbatch_loss:754.8818\n",
            "Epoch loss: 500250.98317348177\n",
            "RMSE on dev data: 9.89412\n",
            "RMSE on train data: 10.00877\n",
            "Epochs:675\tstep:50\tbatch_loss:768.1159\n",
            "Epochs:675\tstep:100\tbatch_loss:802.1654\n",
            "Epochs:675\tstep:150\tbatch_loss:799.8032\n",
            "Epochs:675\tstep:200\tbatch_loss:788.7664\n",
            "Epochs:675\tstep:250\tbatch_loss:948.6119\n",
            "Epochs:675\tstep:300\tbatch_loss:818.6792\n",
            "Epochs:675\tstep:350\tbatch_loss:792.3002\n",
            "Epochs:675\tstep:400\tbatch_loss:794.0509\n",
            "Epochs:675\tstep:450\tbatch_loss:802.4767\n",
            "Epochs:675\tstep:500\tbatch_loss:745.8095\n",
            "Epochs:675\tstep:550\tbatch_loss:822.4848\n",
            "Epochs:675\tstep:600\tbatch_loss:754.7567\n",
            "Epoch loss: 500170.69169166964\n",
            "RMSE on dev data: 9.89409\n",
            "RMSE on train data: 10.00875\n",
            "Epochs:676\tstep:50\tbatch_loss:767.9753\n",
            "Epochs:676\tstep:100\tbatch_loss:802.0604\n",
            "Epochs:676\tstep:150\tbatch_loss:798.8202\n",
            "Epochs:676\tstep:200\tbatch_loss:788.6703\n",
            "Epochs:676\tstep:250\tbatch_loss:948.2414\n",
            "Epochs:676\tstep:300\tbatch_loss:818.4135\n",
            "Epochs:676\tstep:350\tbatch_loss:792.2607\n",
            "Epochs:676\tstep:400\tbatch_loss:793.9426\n",
            "Epochs:676\tstep:450\tbatch_loss:802.3256\n",
            "Epochs:676\tstep:500\tbatch_loss:745.7883\n",
            "Epochs:676\tstep:550\tbatch_loss:822.1277\n",
            "Epochs:676\tstep:600\tbatch_loss:754.8785\n",
            "Epoch loss: 500016.65576312674\n",
            "RMSE on dev data: 9.89237\n",
            "RMSE on train data: 10.00847\n",
            "Epochs:677\tstep:50\tbatch_loss:767.8019\n",
            "Epochs:677\tstep:100\tbatch_loss:801.9517\n",
            "Epochs:677\tstep:150\tbatch_loss:798.6981\n",
            "Epochs:677\tstep:200\tbatch_loss:788.5528\n",
            "Epochs:677\tstep:250\tbatch_loss:948.1252\n",
            "Epochs:677\tstep:300\tbatch_loss:818.2991\n",
            "Epochs:677\tstep:350\tbatch_loss:792.1469\n",
            "Epochs:677\tstep:400\tbatch_loss:793.8286\n",
            "Epochs:677\tstep:450\tbatch_loss:802.2092\n",
            "Epochs:677\tstep:500\tbatch_loss:745.6747\n",
            "Epochs:677\tstep:550\tbatch_loss:822.0137\n",
            "Epochs:677\tstep:600\tbatch_loss:754.7647\n",
            "Epoch loss: 499940.5584633968\n",
            "RMSE on dev data: 9.89235\n",
            "RMSE on train data: 10.00845\n",
            "Epochs:678\tstep:50\tbatch_loss:767.6865\n",
            "Epochs:678\tstep:100\tbatch_loss:801.8371\n",
            "Epochs:678\tstep:150\tbatch_loss:798.5847\n",
            "Epochs:678\tstep:200\tbatch_loss:788.4384\n",
            "Epochs:678\tstep:250\tbatch_loss:948.0081\n",
            "Epochs:678\tstep:300\tbatch_loss:818.1848\n",
            "Epochs:678\tstep:350\tbatch_loss:792.0331\n",
            "Epochs:678\tstep:400\tbatch_loss:793.7150\n",
            "Epochs:678\tstep:450\tbatch_loss:802.0931\n",
            "Epochs:678\tstep:500\tbatch_loss:745.5614\n",
            "Epochs:678\tstep:550\tbatch_loss:821.8999\n",
            "Epochs:678\tstep:600\tbatch_loss:754.6511\n",
            "Epoch loss: 499867.5832524388\n",
            "RMSE on dev data: 9.89233\n",
            "RMSE on train data: 10.00842\n",
            "Epochs:679\tstep:50\tbatch_loss:767.5712\n",
            "Epochs:679\tstep:100\tbatch_loss:801.7228\n",
            "Epochs:679\tstep:150\tbatch_loss:798.4720\n",
            "Epochs:679\tstep:200\tbatch_loss:788.3244\n",
            "Epochs:679\tstep:250\tbatch_loss:947.8912\n",
            "Epochs:679\tstep:300\tbatch_loss:818.0707\n",
            "Epochs:679\tstep:350\tbatch_loss:791.9196\n",
            "Epochs:679\tstep:400\tbatch_loss:793.6013\n",
            "Epochs:679\tstep:450\tbatch_loss:801.9772\n",
            "Epochs:679\tstep:500\tbatch_loss:745.4482\n",
            "Epochs:679\tstep:550\tbatch_loss:821.7862\n",
            "Epochs:679\tstep:600\tbatch_loss:754.5377\n",
            "Epoch loss: 499794.7878646816\n",
            "RMSE on dev data: 9.89230\n",
            "RMSE on train data: 10.00840\n",
            "Epochs:680\tstep:50\tbatch_loss:767.4563\n",
            "Epochs:680\tstep:100\tbatch_loss:801.6088\n",
            "Epochs:680\tstep:150\tbatch_loss:798.3590\n",
            "Epochs:680\tstep:200\tbatch_loss:788.2105\n",
            "Epochs:680\tstep:250\tbatch_loss:947.7745\n",
            "Epochs:680\tstep:300\tbatch_loss:817.9566\n",
            "Epochs:680\tstep:350\tbatch_loss:791.8061\n",
            "Epochs:680\tstep:400\tbatch_loss:793.4877\n",
            "Epochs:680\tstep:450\tbatch_loss:801.8614\n",
            "Epochs:680\tstep:500\tbatch_loss:745.3352\n",
            "Epochs:680\tstep:550\tbatch_loss:821.6726\n",
            "Epochs:680\tstep:600\tbatch_loss:754.4246\n",
            "Epoch loss: 499722.09093818633\n",
            "RMSE on dev data: 9.89228\n",
            "RMSE on train data: 10.00838\n",
            "Epochs:681\tstep:50\tbatch_loss:767.3414\n",
            "Epochs:681\tstep:100\tbatch_loss:801.4950\n",
            "Epochs:681\tstep:150\tbatch_loss:798.2467\n",
            "Epochs:681\tstep:200\tbatch_loss:788.0970\n",
            "Epochs:681\tstep:250\tbatch_loss:947.6584\n",
            "Epochs:681\tstep:300\tbatch_loss:817.8428\n",
            "Epochs:681\tstep:350\tbatch_loss:791.6929\n",
            "Epochs:681\tstep:400\tbatch_loss:793.3746\n",
            "Epochs:681\tstep:450\tbatch_loss:801.7461\n",
            "Epochs:681\tstep:500\tbatch_loss:745.2223\n",
            "Epochs:681\tstep:550\tbatch_loss:821.5593\n",
            "Epochs:681\tstep:600\tbatch_loss:754.3116\n",
            "Epoch loss: 499649.60009202006\n",
            "RMSE on dev data: 9.89226\n",
            "RMSE on train data: 10.00836\n",
            "Epochs:682\tstep:50\tbatch_loss:767.2269\n",
            "Epochs:682\tstep:100\tbatch_loss:801.3814\n",
            "Epochs:682\tstep:150\tbatch_loss:798.1341\n",
            "Epochs:682\tstep:200\tbatch_loss:787.9836\n",
            "Epochs:682\tstep:250\tbatch_loss:947.5422\n",
            "Epochs:682\tstep:300\tbatch_loss:817.7291\n",
            "Epochs:682\tstep:350\tbatch_loss:791.5797\n",
            "Epochs:682\tstep:400\tbatch_loss:793.2613\n",
            "Epochs:682\tstep:450\tbatch_loss:801.6308\n",
            "Epochs:682\tstep:500\tbatch_loss:745.1096\n",
            "Epochs:682\tstep:550\tbatch_loss:821.4459\n",
            "Epochs:682\tstep:600\tbatch_loss:754.1988\n",
            "Epoch loss: 499577.16966077266\n",
            "RMSE on dev data: 9.89223\n",
            "RMSE on train data: 10.00834\n",
            "Epochs:683\tstep:50\tbatch_loss:767.1123\n",
            "Epochs:683\tstep:100\tbatch_loss:801.2680\n",
            "Epochs:683\tstep:150\tbatch_loss:798.0222\n",
            "Epochs:683\tstep:200\tbatch_loss:787.8703\n",
            "Epochs:683\tstep:250\tbatch_loss:947.4263\n",
            "Epochs:683\tstep:300\tbatch_loss:817.6155\n",
            "Epochs:683\tstep:350\tbatch_loss:791.4667\n",
            "Epochs:683\tstep:400\tbatch_loss:793.1481\n",
            "Epochs:683\tstep:450\tbatch_loss:801.5157\n",
            "Epochs:683\tstep:500\tbatch_loss:744.9969\n",
            "Epochs:683\tstep:550\tbatch_loss:821.3327\n",
            "Epochs:683\tstep:600\tbatch_loss:754.0861\n",
            "Epoch loss: 499504.8578990746\n",
            "RMSE on dev data: 9.89221\n",
            "RMSE on train data: 10.00832\n",
            "Epochs:684\tstep:50\tbatch_loss:766.9981\n",
            "Epochs:684\tstep:100\tbatch_loss:801.1548\n",
            "Epochs:684\tstep:150\tbatch_loss:797.9099\n",
            "Epochs:684\tstep:200\tbatch_loss:787.7573\n",
            "Epochs:684\tstep:250\tbatch_loss:947.3106\n",
            "Epochs:684\tstep:300\tbatch_loss:817.5020\n",
            "Epochs:684\tstep:350\tbatch_loss:791.3538\n",
            "Epochs:684\tstep:400\tbatch_loss:793.0351\n",
            "Epochs:684\tstep:450\tbatch_loss:801.4008\n",
            "Epochs:684\tstep:500\tbatch_loss:744.8845\n",
            "Epochs:684\tstep:550\tbatch_loss:821.2196\n",
            "Epochs:684\tstep:600\tbatch_loss:753.9736\n",
            "Epoch loss: 499432.652897255\n",
            "RMSE on dev data: 9.89219\n",
            "RMSE on train data: 10.00830\n",
            "Epochs:685\tstep:50\tbatch_loss:766.8839\n",
            "Epochs:685\tstep:100\tbatch_loss:801.0418\n",
            "Epochs:685\tstep:150\tbatch_loss:797.7982\n",
            "Epochs:685\tstep:200\tbatch_loss:787.6446\n",
            "Epochs:685\tstep:250\tbatch_loss:947.1954\n",
            "Epochs:685\tstep:300\tbatch_loss:817.3887\n",
            "Epochs:685\tstep:350\tbatch_loss:791.2410\n",
            "Epochs:685\tstep:400\tbatch_loss:792.9224\n",
            "Epochs:685\tstep:450\tbatch_loss:801.2862\n",
            "Epochs:685\tstep:500\tbatch_loss:744.7721\n",
            "Epochs:685\tstep:550\tbatch_loss:821.1066\n",
            "Epochs:685\tstep:600\tbatch_loss:753.8613\n",
            "Epoch loss: 499360.61629619874\n",
            "RMSE on dev data: 9.89217\n",
            "RMSE on train data: 10.00828\n",
            "Epochs:686\tstep:50\tbatch_loss:766.7700\n",
            "Epochs:686\tstep:100\tbatch_loss:800.9290\n",
            "Epochs:686\tstep:150\tbatch_loss:797.6862\n",
            "Epochs:686\tstep:200\tbatch_loss:787.5319\n",
            "Epochs:686\tstep:250\tbatch_loss:947.0801\n",
            "Epochs:686\tstep:300\tbatch_loss:817.2754\n",
            "Epochs:686\tstep:350\tbatch_loss:791.1284\n",
            "Epochs:686\tstep:400\tbatch_loss:792.8096\n",
            "Epochs:686\tstep:450\tbatch_loss:801.1716\n",
            "Epochs:686\tstep:500\tbatch_loss:744.6599\n",
            "Epochs:686\tstep:550\tbatch_loss:820.9937\n",
            "Epochs:686\tstep:600\tbatch_loss:753.7491\n",
            "Epoch loss: 499288.61297419725\n",
            "RMSE on dev data: 9.89215\n",
            "RMSE on train data: 10.00827\n",
            "Epochs:687\tstep:50\tbatch_loss:766.6561\n",
            "Epochs:687\tstep:100\tbatch_loss:800.8163\n",
            "Epochs:687\tstep:150\tbatch_loss:797.5748\n",
            "Epochs:687\tstep:200\tbatch_loss:787.4195\n",
            "Epochs:687\tstep:250\tbatch_loss:946.9650\n",
            "Epochs:687\tstep:300\tbatch_loss:817.1624\n",
            "Epochs:687\tstep:350\tbatch_loss:791.0159\n",
            "Epochs:687\tstep:400\tbatch_loss:792.6970\n",
            "Epochs:687\tstep:450\tbatch_loss:801.0572\n",
            "Epochs:687\tstep:500\tbatch_loss:744.5477\n",
            "Epochs:687\tstep:550\tbatch_loss:820.8810\n",
            "Epochs:687\tstep:600\tbatch_loss:753.6371\n",
            "Epoch loss: 499216.7389955035\n",
            "RMSE on dev data: 9.89213\n",
            "RMSE on train data: 10.00825\n",
            "Epochs:688\tstep:50\tbatch_loss:766.5425\n",
            "Epochs:688\tstep:100\tbatch_loss:800.7038\n",
            "Epochs:688\tstep:150\tbatch_loss:797.4630\n",
            "Epochs:688\tstep:200\tbatch_loss:787.3071\n",
            "Epochs:688\tstep:250\tbatch_loss:946.8500\n",
            "Epochs:688\tstep:300\tbatch_loss:817.0493\n",
            "Epochs:688\tstep:350\tbatch_loss:790.9034\n",
            "Epochs:688\tstep:400\tbatch_loss:792.5844\n",
            "Epochs:688\tstep:450\tbatch_loss:800.9429\n",
            "Epochs:688\tstep:500\tbatch_loss:744.4357\n",
            "Epochs:688\tstep:550\tbatch_loss:820.7682\n",
            "Epochs:688\tstep:600\tbatch_loss:753.5251\n",
            "Epoch loss: 499144.898778508\n",
            "RMSE on dev data: 9.89211\n",
            "RMSE on train data: 10.00823\n",
            "Epochs:689\tstep:50\tbatch_loss:766.4288\n",
            "Epochs:689\tstep:100\tbatch_loss:800.5914\n",
            "Epochs:689\tstep:150\tbatch_loss:797.3518\n",
            "Epochs:689\tstep:200\tbatch_loss:787.1950\n",
            "Epochs:689\tstep:250\tbatch_loss:946.7354\n",
            "Epochs:689\tstep:300\tbatch_loss:816.9365\n",
            "Epochs:689\tstep:350\tbatch_loss:790.7911\n",
            "Epochs:689\tstep:400\tbatch_loss:792.4721\n",
            "Epochs:689\tstep:450\tbatch_loss:800.8289\n",
            "Epochs:689\tstep:500\tbatch_loss:744.3237\n",
            "Epochs:689\tstep:550\tbatch_loss:820.6557\n",
            "Epochs:689\tstep:600\tbatch_loss:753.4133\n",
            "Epoch loss: 499073.2090051464\n",
            "RMSE on dev data: 9.89208\n",
            "RMSE on train data: 10.00821\n",
            "Epochs:690\tstep:50\tbatch_loss:766.3153\n",
            "Epochs:690\tstep:100\tbatch_loss:800.4792\n",
            "Epochs:690\tstep:150\tbatch_loss:797.2403\n",
            "Epochs:690\tstep:200\tbatch_loss:787.0828\n",
            "Epochs:690\tstep:250\tbatch_loss:946.6208\n",
            "Epochs:690\tstep:300\tbatch_loss:816.8236\n",
            "Epochs:690\tstep:350\tbatch_loss:790.6788\n",
            "Epochs:690\tstep:400\tbatch_loss:792.3597\n",
            "Epochs:690\tstep:450\tbatch_loss:800.7147\n",
            "Epochs:690\tstep:500\tbatch_loss:744.2119\n",
            "Epochs:690\tstep:550\tbatch_loss:820.5431\n",
            "Epochs:690\tstep:600\tbatch_loss:753.3015\n",
            "Epoch loss: 499001.49202933855\n",
            "RMSE on dev data: 9.89206\n",
            "RMSE on train data: 10.00820\n",
            "Epochs:691\tstep:50\tbatch_loss:766.2020\n",
            "Epochs:691\tstep:100\tbatch_loss:800.3670\n",
            "Epochs:691\tstep:150\tbatch_loss:797.1293\n",
            "Epochs:691\tstep:200\tbatch_loss:786.9709\n",
            "Epochs:691\tstep:250\tbatch_loss:946.5063\n",
            "Epochs:691\tstep:300\tbatch_loss:816.7109\n",
            "Epochs:691\tstep:350\tbatch_loss:790.5666\n",
            "Epochs:691\tstep:400\tbatch_loss:792.2474\n",
            "Epochs:691\tstep:450\tbatch_loss:800.6009\n",
            "Epochs:691\tstep:500\tbatch_loss:744.1001\n",
            "Epochs:691\tstep:550\tbatch_loss:820.4307\n",
            "Epochs:691\tstep:600\tbatch_loss:753.1900\n",
            "Epoch loss: 498929.9289377155\n",
            "RMSE on dev data: 9.89204\n",
            "RMSE on train data: 10.00818\n",
            "Epochs:692\tstep:50\tbatch_loss:766.0888\n",
            "Epochs:692\tstep:100\tbatch_loss:800.2550\n",
            "Epochs:692\tstep:150\tbatch_loss:797.0180\n",
            "Epochs:692\tstep:200\tbatch_loss:786.8590\n",
            "Epochs:692\tstep:250\tbatch_loss:946.3919\n",
            "Epochs:692\tstep:300\tbatch_loss:816.5982\n",
            "Epochs:692\tstep:350\tbatch_loss:790.4546\n",
            "Epochs:692\tstep:400\tbatch_loss:792.1352\n",
            "Epochs:692\tstep:450\tbatch_loss:800.4870\n",
            "Epochs:692\tstep:500\tbatch_loss:743.9885\n",
            "Epochs:692\tstep:550\tbatch_loss:820.3183\n",
            "Epochs:692\tstep:600\tbatch_loss:753.0785\n",
            "Epoch loss: 498858.4053640903\n",
            "RMSE on dev data: 9.89202\n",
            "RMSE on train data: 10.00816\n",
            "Epochs:693\tstep:50\tbatch_loss:765.9756\n",
            "Epochs:693\tstep:100\tbatch_loss:800.1431\n",
            "Epochs:693\tstep:150\tbatch_loss:796.9070\n",
            "Epochs:693\tstep:200\tbatch_loss:786.7474\n",
            "Epochs:693\tstep:250\tbatch_loss:946.2779\n",
            "Epochs:693\tstep:300\tbatch_loss:816.4857\n",
            "Epochs:693\tstep:350\tbatch_loss:790.3426\n",
            "Epochs:693\tstep:400\tbatch_loss:792.0233\n",
            "Epochs:693\tstep:450\tbatch_loss:800.3734\n",
            "Epochs:693\tstep:500\tbatch_loss:743.8769\n",
            "Epochs:693\tstep:550\tbatch_loss:820.2061\n",
            "Epochs:693\tstep:600\tbatch_loss:752.9671\n",
            "Epoch loss: 498786.97919383924\n",
            "RMSE on dev data: 9.89200\n",
            "RMSE on train data: 10.00815\n",
            "Epochs:694\tstep:50\tbatch_loss:765.8626\n",
            "Epochs:694\tstep:100\tbatch_loss:800.0313\n",
            "Epochs:694\tstep:150\tbatch_loss:796.7963\n",
            "Epochs:694\tstep:200\tbatch_loss:786.6358\n",
            "Epochs:694\tstep:250\tbatch_loss:946.1638\n",
            "Epochs:694\tstep:300\tbatch_loss:816.3733\n",
            "Epochs:694\tstep:350\tbatch_loss:790.2307\n",
            "Epochs:694\tstep:400\tbatch_loss:791.9113\n",
            "Epochs:694\tstep:450\tbatch_loss:800.2599\n",
            "Epochs:694\tstep:500\tbatch_loss:743.7655\n",
            "Epochs:694\tstep:550\tbatch_loss:820.0939\n",
            "Epochs:694\tstep:600\tbatch_loss:752.8559\n",
            "Epoch loss: 498715.61873911717\n",
            "RMSE on dev data: 9.89198\n",
            "RMSE on train data: 10.00813\n",
            "Epochs:695\tstep:50\tbatch_loss:765.7497\n",
            "Epochs:695\tstep:100\tbatch_loss:799.9196\n",
            "Epochs:695\tstep:150\tbatch_loss:796.6853\n",
            "Epochs:695\tstep:200\tbatch_loss:786.5242\n",
            "Epochs:695\tstep:250\tbatch_loss:946.0498\n",
            "Epochs:695\tstep:300\tbatch_loss:816.2608\n",
            "Epochs:695\tstep:350\tbatch_loss:790.1189\n",
            "Epochs:695\tstep:400\tbatch_loss:791.7994\n",
            "Epochs:695\tstep:450\tbatch_loss:800.1463\n",
            "Epochs:695\tstep:500\tbatch_loss:743.6541\n",
            "Epochs:695\tstep:550\tbatch_loss:819.9818\n",
            "Epochs:695\tstep:600\tbatch_loss:752.7447\n",
            "Epoch loss: 498644.271734452\n",
            "RMSE on dev data: 9.89196\n",
            "RMSE on train data: 10.00811\n",
            "Epochs:696\tstep:50\tbatch_loss:765.6368\n",
            "Epochs:696\tstep:100\tbatch_loss:799.8080\n",
            "Epochs:696\tstep:150\tbatch_loss:796.5748\n",
            "Epochs:696\tstep:200\tbatch_loss:786.4129\n",
            "Epochs:696\tstep:250\tbatch_loss:945.9360\n",
            "Epochs:696\tstep:300\tbatch_loss:816.1486\n",
            "Epochs:696\tstep:350\tbatch_loss:790.0073\n",
            "Epochs:696\tstep:400\tbatch_loss:791.6876\n",
            "Epochs:696\tstep:450\tbatch_loss:800.0330\n",
            "Epochs:696\tstep:500\tbatch_loss:743.5428\n",
            "Epochs:696\tstep:550\tbatch_loss:819.8699\n",
            "Epochs:696\tstep:600\tbatch_loss:752.6337\n",
            "Epoch loss: 498573.04638727766\n",
            "RMSE on dev data: 9.89194\n",
            "RMSE on train data: 10.00810\n",
            "Epochs:697\tstep:50\tbatch_loss:765.5240\n",
            "Epochs:697\tstep:100\tbatch_loss:799.6966\n",
            "Epochs:697\tstep:150\tbatch_loss:796.4639\n",
            "Epochs:697\tstep:200\tbatch_loss:786.3015\n",
            "Epochs:697\tstep:250\tbatch_loss:945.8224\n",
            "Epochs:697\tstep:300\tbatch_loss:816.0364\n",
            "Epochs:697\tstep:350\tbatch_loss:789.8956\n",
            "Epochs:697\tstep:400\tbatch_loss:791.5759\n",
            "Epochs:697\tstep:450\tbatch_loss:799.9197\n",
            "Epochs:697\tstep:500\tbatch_loss:743.4316\n",
            "Epochs:697\tstep:550\tbatch_loss:819.7579\n",
            "Epochs:697\tstep:600\tbatch_loss:752.5226\n",
            "Epoch loss: 498501.81480700336\n",
            "RMSE on dev data: 9.89192\n",
            "RMSE on train data: 10.00808\n",
            "Epochs:698\tstep:50\tbatch_loss:765.4114\n",
            "Epochs:698\tstep:100\tbatch_loss:799.5852\n",
            "Epochs:698\tstep:150\tbatch_loss:796.3537\n",
            "Epochs:698\tstep:200\tbatch_loss:786.1903\n",
            "Epochs:698\tstep:250\tbatch_loss:945.7088\n",
            "Epochs:698\tstep:300\tbatch_loss:815.9243\n",
            "Epochs:698\tstep:350\tbatch_loss:789.7841\n",
            "Epochs:698\tstep:400\tbatch_loss:791.4643\n",
            "Epochs:698\tstep:450\tbatch_loss:799.8065\n",
            "Epochs:698\tstep:500\tbatch_loss:743.3205\n",
            "Epochs:698\tstep:550\tbatch_loss:819.6461\n",
            "Epochs:698\tstep:600\tbatch_loss:752.4117\n",
            "Epoch loss: 498430.70751046017\n",
            "RMSE on dev data: 9.89190\n",
            "RMSE on train data: 10.00806\n",
            "Epochs:699\tstep:50\tbatch_loss:765.2987\n",
            "Epochs:699\tstep:100\tbatch_loss:799.4739\n",
            "Epochs:699\tstep:150\tbatch_loss:796.2429\n",
            "Epochs:699\tstep:200\tbatch_loss:786.0792\n",
            "Epochs:699\tstep:250\tbatch_loss:945.5953\n",
            "Epochs:699\tstep:300\tbatch_loss:815.8122\n",
            "Epochs:699\tstep:350\tbatch_loss:789.6726\n",
            "Epochs:699\tstep:400\tbatch_loss:791.3527\n",
            "Epochs:699\tstep:450\tbatch_loss:799.6934\n",
            "Epochs:699\tstep:500\tbatch_loss:743.2095\n",
            "Epochs:699\tstep:550\tbatch_loss:819.5343\n",
            "Epochs:699\tstep:600\tbatch_loss:752.3009\n",
            "Epoch loss: 498359.5881182346\n",
            "RMSE on dev data: 9.89188\n",
            "RMSE on train data: 10.00805\n",
            "Epochs:700\tstep:50\tbatch_loss:765.1862\n",
            "Epochs:700\tstep:100\tbatch_loss:799.3626\n",
            "Epochs:700\tstep:150\tbatch_loss:796.1325\n",
            "Epochs:700\tstep:200\tbatch_loss:785.9681\n",
            "Epochs:700\tstep:250\tbatch_loss:945.4818\n",
            "Epochs:700\tstep:300\tbatch_loss:815.7003\n",
            "Epochs:700\tstep:350\tbatch_loss:789.5612\n",
            "Epochs:700\tstep:400\tbatch_loss:791.2412\n",
            "Epochs:700\tstep:450\tbatch_loss:799.5803\n",
            "Epochs:700\tstep:500\tbatch_loss:743.0985\n",
            "Epochs:700\tstep:550\tbatch_loss:819.4226\n",
            "Epochs:700\tstep:600\tbatch_loss:752.1902\n",
            "Epoch loss: 498288.56116399675\n",
            "RMSE on dev data: 9.89186\n",
            "RMSE on train data: 10.00803\n",
            "Epochs:701\tstep:50\tbatch_loss:765.0453\n",
            "Epochs:701\tstep:100\tbatch_loss:799.2765\n",
            "Epochs:701\tstep:150\tbatch_loss:795.2267\n",
            "Epochs:701\tstep:200\tbatch_loss:785.8509\n",
            "Epochs:701\tstep:250\tbatch_loss:945.1411\n",
            "Epochs:701\tstep:300\tbatch_loss:815.4979\n",
            "Epochs:701\tstep:350\tbatch_loss:789.5030\n",
            "Epochs:701\tstep:400\tbatch_loss:791.1487\n",
            "Epochs:701\tstep:450\tbatch_loss:799.4528\n",
            "Epochs:701\tstep:500\tbatch_loss:743.0957\n",
            "Epochs:701\tstep:550\tbatch_loss:819.0402\n",
            "Epochs:701\tstep:600\tbatch_loss:752.3205\n",
            "Epoch loss: 498149.1591681807\n",
            "RMSE on dev data: 9.89037\n",
            "RMSE on train data: 10.00796\n",
            "Epochs:702\tstep:50\tbatch_loss:764.8803\n",
            "Epochs:702\tstep:100\tbatch_loss:799.1827\n",
            "Epochs:702\tstep:150\tbatch_loss:795.1148\n",
            "Epochs:702\tstep:200\tbatch_loss:785.7445\n",
            "Epochs:702\tstep:250\tbatch_loss:945.0389\n",
            "Epochs:702\tstep:300\tbatch_loss:815.3964\n",
            "Epochs:702\tstep:350\tbatch_loss:789.4022\n",
            "Epochs:702\tstep:400\tbatch_loss:791.0476\n",
            "Epochs:702\tstep:450\tbatch_loss:799.3496\n",
            "Epochs:702\tstep:500\tbatch_loss:742.9951\n",
            "Epochs:702\tstep:550\tbatch_loss:818.9390\n",
            "Epochs:702\tstep:600\tbatch_loss:752.2196\n",
            "Epoch loss: 498081.1874755936\n",
            "RMSE on dev data: 9.89034\n",
            "RMSE on train data: 10.00794\n",
            "Epochs:703\tstep:50\tbatch_loss:764.7779\n",
            "Epochs:703\tstep:100\tbatch_loss:799.0811\n",
            "Epochs:703\tstep:150\tbatch_loss:795.0146\n",
            "Epochs:703\tstep:200\tbatch_loss:785.6427\n",
            "Epochs:703\tstep:250\tbatch_loss:944.9348\n",
            "Epochs:703\tstep:300\tbatch_loss:815.2949\n",
            "Epochs:703\tstep:350\tbatch_loss:789.3014\n",
            "Epochs:703\tstep:400\tbatch_loss:790.9467\n",
            "Epochs:703\tstep:450\tbatch_loss:799.2464\n",
            "Epochs:703\tstep:500\tbatch_loss:742.8947\n",
            "Epochs:703\tstep:550\tbatch_loss:818.8379\n",
            "Epochs:703\tstep:600\tbatch_loss:752.1188\n",
            "Epoch loss: 498016.4845228658\n",
            "RMSE on dev data: 9.89032\n",
            "RMSE on train data: 10.00792\n",
            "Epochs:704\tstep:50\tbatch_loss:764.6758\n",
            "Epochs:704\tstep:100\tbatch_loss:798.9798\n",
            "Epochs:704\tstep:150\tbatch_loss:794.9148\n",
            "Epochs:704\tstep:200\tbatch_loss:785.5413\n",
            "Epochs:704\tstep:250\tbatch_loss:944.8310\n",
            "Epochs:704\tstep:300\tbatch_loss:815.1937\n",
            "Epochs:704\tstep:350\tbatch_loss:789.2008\n",
            "Epochs:704\tstep:400\tbatch_loss:790.8460\n",
            "Epochs:704\tstep:450\tbatch_loss:799.1437\n",
            "Epochs:704\tstep:500\tbatch_loss:742.7945\n",
            "Epochs:704\tstep:550\tbatch_loss:818.7371\n",
            "Epochs:704\tstep:600\tbatch_loss:752.0184\n",
            "Epoch loss: 497951.9991253688\n",
            "RMSE on dev data: 9.89029\n",
            "RMSE on train data: 10.00790\n",
            "Epochs:705\tstep:50\tbatch_loss:764.5739\n",
            "Epochs:705\tstep:100\tbatch_loss:798.8788\n",
            "Epochs:705\tstep:150\tbatch_loss:794.8148\n",
            "Epochs:705\tstep:200\tbatch_loss:785.4402\n",
            "Epochs:705\tstep:250\tbatch_loss:944.7277\n",
            "Epochs:705\tstep:300\tbatch_loss:815.0927\n",
            "Epochs:705\tstep:350\tbatch_loss:789.1003\n",
            "Epochs:705\tstep:400\tbatch_loss:790.7455\n",
            "Epochs:705\tstep:450\tbatch_loss:799.0412\n",
            "Epochs:705\tstep:500\tbatch_loss:742.6944\n",
            "Epochs:705\tstep:550\tbatch_loss:818.6363\n",
            "Epochs:705\tstep:600\tbatch_loss:751.9181\n",
            "Epoch loss: 497887.61527346406\n",
            "RMSE on dev data: 9.89027\n",
            "RMSE on train data: 10.00788\n",
            "Epochs:706\tstep:50\tbatch_loss:764.4721\n",
            "Epochs:706\tstep:100\tbatch_loss:798.7780\n",
            "Epochs:706\tstep:150\tbatch_loss:794.7154\n",
            "Epochs:706\tstep:200\tbatch_loss:785.3393\n",
            "Epochs:706\tstep:250\tbatch_loss:944.6245\n",
            "Epochs:706\tstep:300\tbatch_loss:814.9918\n",
            "Epochs:706\tstep:350\tbatch_loss:789.0000\n",
            "Epochs:706\tstep:400\tbatch_loss:790.6450\n",
            "Epochs:706\tstep:450\tbatch_loss:798.9390\n",
            "Epochs:706\tstep:500\tbatch_loss:742.5945\n",
            "Epochs:706\tstep:550\tbatch_loss:818.5357\n",
            "Epochs:706\tstep:600\tbatch_loss:751.8180\n",
            "Epoch loss: 497823.36931880936\n",
            "RMSE on dev data: 9.89025\n",
            "RMSE on train data: 10.00786\n",
            "Epochs:707\tstep:50\tbatch_loss:764.3706\n",
            "Epochs:707\tstep:100\tbatch_loss:798.6774\n",
            "Epochs:707\tstep:150\tbatch_loss:794.6157\n",
            "Epochs:707\tstep:200\tbatch_loss:785.2385\n",
            "Epochs:707\tstep:250\tbatch_loss:944.5214\n",
            "Epochs:707\tstep:300\tbatch_loss:814.8909\n",
            "Epochs:707\tstep:350\tbatch_loss:788.8997\n",
            "Epochs:707\tstep:400\tbatch_loss:790.5447\n",
            "Epochs:707\tstep:450\tbatch_loss:798.8368\n",
            "Epochs:707\tstep:500\tbatch_loss:742.4946\n",
            "Epochs:707\tstep:550\tbatch_loss:818.4351\n",
            "Epochs:707\tstep:600\tbatch_loss:751.7180\n",
            "Epoch loss: 497759.1834483918\n",
            "RMSE on dev data: 9.89022\n",
            "RMSE on train data: 10.00784\n",
            "Epochs:708\tstep:50\tbatch_loss:764.2691\n",
            "Epochs:708\tstep:100\tbatch_loss:798.5769\n",
            "Epochs:708\tstep:150\tbatch_loss:794.5165\n",
            "Epochs:708\tstep:200\tbatch_loss:785.1381\n",
            "Epochs:708\tstep:250\tbatch_loss:944.4185\n",
            "Epochs:708\tstep:300\tbatch_loss:814.7902\n",
            "Epochs:708\tstep:350\tbatch_loss:788.7996\n",
            "Epochs:708\tstep:400\tbatch_loss:790.4444\n",
            "Epochs:708\tstep:450\tbatch_loss:798.7348\n",
            "Epochs:708\tstep:500\tbatch_loss:742.3949\n",
            "Epochs:708\tstep:550\tbatch_loss:818.3348\n",
            "Epochs:708\tstep:600\tbatch_loss:751.6183\n",
            "Epoch loss: 497695.15305882983\n",
            "RMSE on dev data: 9.89020\n",
            "RMSE on train data: 10.00782\n",
            "Epochs:709\tstep:50\tbatch_loss:764.1678\n",
            "Epochs:709\tstep:100\tbatch_loss:798.4766\n",
            "Epochs:709\tstep:150\tbatch_loss:794.4171\n",
            "Epochs:709\tstep:200\tbatch_loss:785.0376\n",
            "Epochs:709\tstep:250\tbatch_loss:944.3161\n",
            "Epochs:709\tstep:300\tbatch_loss:814.6896\n",
            "Epochs:709\tstep:350\tbatch_loss:788.6996\n",
            "Epochs:709\tstep:400\tbatch_loss:790.3444\n",
            "Epochs:709\tstep:450\tbatch_loss:798.6330\n",
            "Epochs:709\tstep:500\tbatch_loss:742.2953\n",
            "Epochs:709\tstep:550\tbatch_loss:818.2344\n",
            "Epochs:709\tstep:600\tbatch_loss:751.5184\n",
            "Epoch loss: 497631.1568414419\n",
            "RMSE on dev data: 9.89018\n",
            "RMSE on train data: 10.00780\n",
            "Epochs:710\tstep:50\tbatch_loss:764.0666\n",
            "Epochs:710\tstep:100\tbatch_loss:798.3765\n",
            "Epochs:710\tstep:150\tbatch_loss:794.3179\n",
            "Epochs:710\tstep:200\tbatch_loss:784.9374\n",
            "Epochs:710\tstep:250\tbatch_loss:944.2135\n",
            "Epochs:710\tstep:300\tbatch_loss:814.5891\n",
            "Epochs:710\tstep:350\tbatch_loss:788.5996\n",
            "Epochs:710\tstep:400\tbatch_loss:790.2444\n",
            "Epochs:710\tstep:450\tbatch_loss:798.5313\n",
            "Epochs:710\tstep:500\tbatch_loss:742.1958\n",
            "Epochs:710\tstep:550\tbatch_loss:818.1342\n",
            "Epochs:710\tstep:600\tbatch_loss:751.4189\n",
            "Epoch loss: 497567.2806113529\n",
            "RMSE on dev data: 9.89015\n",
            "RMSE on train data: 10.00778\n",
            "Epochs:711\tstep:50\tbatch_loss:763.9656\n",
            "Epochs:711\tstep:100\tbatch_loss:798.2765\n",
            "Epochs:711\tstep:150\tbatch_loss:794.2192\n",
            "Epochs:711\tstep:200\tbatch_loss:784.8375\n",
            "Epochs:711\tstep:250\tbatch_loss:944.1113\n",
            "Epochs:711\tstep:300\tbatch_loss:814.4888\n",
            "Epochs:711\tstep:350\tbatch_loss:788.4999\n",
            "Epochs:711\tstep:400\tbatch_loss:790.1445\n",
            "Epochs:711\tstep:450\tbatch_loss:798.4298\n",
            "Epochs:711\tstep:500\tbatch_loss:742.0964\n",
            "Epochs:711\tstep:550\tbatch_loss:818.0341\n",
            "Epochs:711\tstep:600\tbatch_loss:751.3195\n",
            "Epoch loss: 497503.5256833735\n",
            "RMSE on dev data: 9.89013\n",
            "RMSE on train data: 10.00776\n",
            "Epochs:712\tstep:50\tbatch_loss:763.8647\n",
            "Epochs:712\tstep:100\tbatch_loss:798.1767\n",
            "Epochs:712\tstep:150\tbatch_loss:794.1201\n",
            "Epochs:712\tstep:200\tbatch_loss:784.7376\n",
            "Epochs:712\tstep:250\tbatch_loss:944.0091\n",
            "Epochs:712\tstep:300\tbatch_loss:814.3885\n",
            "Epochs:712\tstep:350\tbatch_loss:788.4002\n",
            "Epochs:712\tstep:400\tbatch_loss:790.0447\n",
            "Epochs:712\tstep:450\tbatch_loss:798.3284\n",
            "Epochs:712\tstep:500\tbatch_loss:741.9971\n",
            "Epochs:712\tstep:550\tbatch_loss:817.9341\n",
            "Epochs:712\tstep:600\tbatch_loss:751.2202\n",
            "Epoch loss: 497439.8102761854\n",
            "RMSE on dev data: 9.89011\n",
            "RMSE on train data: 10.00775\n",
            "Epochs:713\tstep:50\tbatch_loss:763.7639\n",
            "Epochs:713\tstep:100\tbatch_loss:798.0770\n",
            "Epochs:713\tstep:150\tbatch_loss:794.0216\n",
            "Epochs:713\tstep:200\tbatch_loss:784.6380\n",
            "Epochs:713\tstep:250\tbatch_loss:943.9073\n",
            "Epochs:713\tstep:300\tbatch_loss:814.2884\n",
            "Epochs:713\tstep:350\tbatch_loss:788.3006\n",
            "Epochs:713\tstep:400\tbatch_loss:789.9451\n",
            "Epochs:713\tstep:450\tbatch_loss:798.2272\n",
            "Epochs:713\tstep:500\tbatch_loss:741.8978\n",
            "Epochs:713\tstep:550\tbatch_loss:817.8343\n",
            "Epochs:713\tstep:600\tbatch_loss:751.1210\n",
            "Epoch loss: 497376.2159627901\n",
            "RMSE on dev data: 9.89009\n",
            "RMSE on train data: 10.00773\n",
            "Epochs:714\tstep:50\tbatch_loss:763.6631\n",
            "Epochs:714\tstep:100\tbatch_loss:797.9774\n",
            "Epochs:714\tstep:150\tbatch_loss:793.9227\n",
            "Epochs:714\tstep:200\tbatch_loss:784.5382\n",
            "Epochs:714\tstep:250\tbatch_loss:943.8054\n",
            "Epochs:714\tstep:300\tbatch_loss:814.1883\n",
            "Epochs:714\tstep:350\tbatch_loss:788.2010\n",
            "Epochs:714\tstep:400\tbatch_loss:789.8454\n",
            "Epochs:714\tstep:450\tbatch_loss:798.1259\n",
            "Epochs:714\tstep:500\tbatch_loss:741.7987\n",
            "Epochs:714\tstep:550\tbatch_loss:817.7344\n",
            "Epochs:714\tstep:600\tbatch_loss:751.0219\n",
            "Epoch loss: 497312.59834310436\n",
            "RMSE on dev data: 9.89007\n",
            "RMSE on train data: 10.00771\n",
            "Epochs:715\tstep:50\tbatch_loss:763.5626\n",
            "Epochs:715\tstep:100\tbatch_loss:797.8779\n",
            "Epochs:715\tstep:150\tbatch_loss:793.8241\n",
            "Epochs:715\tstep:200\tbatch_loss:784.4388\n",
            "Epochs:715\tstep:250\tbatch_loss:943.7036\n",
            "Epochs:715\tstep:300\tbatch_loss:814.0883\n",
            "Epochs:715\tstep:350\tbatch_loss:788.1015\n",
            "Epochs:715\tstep:400\tbatch_loss:789.7459\n",
            "Epochs:715\tstep:450\tbatch_loss:798.0248\n",
            "Epochs:715\tstep:500\tbatch_loss:741.6996\n",
            "Epochs:715\tstep:550\tbatch_loss:817.6346\n",
            "Epochs:715\tstep:600\tbatch_loss:750.9229\n",
            "Epoch loss: 497249.11735833186\n",
            "RMSE on dev data: 9.89005\n",
            "RMSE on train data: 10.00770\n",
            "Epochs:716\tstep:50\tbatch_loss:763.4621\n",
            "Epochs:716\tstep:100\tbatch_loss:797.7786\n",
            "Epochs:716\tstep:150\tbatch_loss:793.7259\n",
            "Epochs:716\tstep:200\tbatch_loss:784.3396\n",
            "Epochs:716\tstep:250\tbatch_loss:943.6022\n",
            "Epochs:716\tstep:300\tbatch_loss:813.9885\n",
            "Epochs:716\tstep:350\tbatch_loss:788.0022\n",
            "Epochs:716\tstep:400\tbatch_loss:789.6466\n",
            "Epochs:716\tstep:450\tbatch_loss:797.9240\n",
            "Epochs:716\tstep:500\tbatch_loss:741.6007\n",
            "Epochs:716\tstep:550\tbatch_loss:817.5350\n",
            "Epochs:716\tstep:600\tbatch_loss:750.8240\n",
            "Epoch loss: 497185.73617667524\n",
            "RMSE on dev data: 9.89003\n",
            "RMSE on train data: 10.00768\n",
            "Epochs:717\tstep:50\tbatch_loss:763.3617\n",
            "Epochs:717\tstep:100\tbatch_loss:797.6794\n",
            "Epochs:717\tstep:150\tbatch_loss:793.6273\n",
            "Epochs:717\tstep:200\tbatch_loss:784.2403\n",
            "Epochs:717\tstep:250\tbatch_loss:943.5007\n",
            "Epochs:717\tstep:300\tbatch_loss:813.8886\n",
            "Epochs:717\tstep:350\tbatch_loss:787.9029\n",
            "Epochs:717\tstep:400\tbatch_loss:789.5472\n",
            "Epochs:717\tstep:450\tbatch_loss:797.8231\n",
            "Epochs:717\tstep:500\tbatch_loss:741.5018\n",
            "Epochs:717\tstep:550\tbatch_loss:817.4354\n",
            "Epochs:717\tstep:600\tbatch_loss:750.7252\n",
            "Epoch loss: 497122.3552318463\n",
            "RMSE on dev data: 9.89001\n",
            "RMSE on train data: 10.00767\n",
            "Epochs:718\tstep:50\tbatch_loss:763.2614\n",
            "Epochs:718\tstep:100\tbatch_loss:797.5802\n",
            "Epochs:718\tstep:150\tbatch_loss:793.5292\n",
            "Epochs:718\tstep:200\tbatch_loss:784.1413\n",
            "Epochs:718\tstep:250\tbatch_loss:943.3996\n",
            "Epochs:718\tstep:300\tbatch_loss:813.7890\n",
            "Epochs:718\tstep:350\tbatch_loss:787.8037\n",
            "Epochs:718\tstep:400\tbatch_loss:789.4481\n",
            "Epochs:718\tstep:450\tbatch_loss:797.7225\n",
            "Epochs:718\tstep:500\tbatch_loss:741.4030\n",
            "Epochs:718\tstep:550\tbatch_loss:817.3360\n",
            "Epochs:718\tstep:600\tbatch_loss:750.6265\n",
            "Epoch loss: 497059.09920827183\n",
            "RMSE on dev data: 9.88999\n",
            "RMSE on train data: 10.00765\n",
            "Epochs:719\tstep:50\tbatch_loss:763.1613\n",
            "Epochs:719\tstep:100\tbatch_loss:797.4813\n",
            "Epochs:719\tstep:150\tbatch_loss:793.4308\n",
            "Epochs:719\tstep:200\tbatch_loss:784.0422\n",
            "Epochs:719\tstep:250\tbatch_loss:943.2983\n",
            "Epochs:719\tstep:300\tbatch_loss:813.6893\n",
            "Epochs:719\tstep:350\tbatch_loss:787.7046\n",
            "Epochs:719\tstep:400\tbatch_loss:789.3488\n",
            "Epochs:719\tstep:450\tbatch_loss:797.6218\n",
            "Epochs:719\tstep:500\tbatch_loss:741.3043\n",
            "Epochs:719\tstep:550\tbatch_loss:817.2365\n",
            "Epochs:719\tstep:600\tbatch_loss:750.5279\n",
            "Epoch loss: 496995.8442488671\n",
            "RMSE on dev data: 9.88997\n",
            "RMSE on train data: 10.00764\n",
            "Epochs:720\tstep:50\tbatch_loss:763.0611\n",
            "Epochs:720\tstep:100\tbatch_loss:797.3822\n",
            "Epochs:720\tstep:150\tbatch_loss:793.3326\n",
            "Epochs:720\tstep:200\tbatch_loss:783.9432\n",
            "Epochs:720\tstep:250\tbatch_loss:943.1971\n",
            "Epochs:720\tstep:300\tbatch_loss:813.5896\n",
            "Epochs:720\tstep:350\tbatch_loss:787.6055\n",
            "Epochs:720\tstep:400\tbatch_loss:789.2496\n",
            "Epochs:720\tstep:450\tbatch_loss:797.5212\n",
            "Epochs:720\tstep:500\tbatch_loss:741.2056\n",
            "Epochs:720\tstep:550\tbatch_loss:817.1371\n",
            "Epochs:720\tstep:600\tbatch_loss:750.4294\n",
            "Epoch loss: 496932.61498006\n",
            "RMSE on dev data: 9.88996\n",
            "RMSE on train data: 10.00762\n",
            "Epochs:721\tstep:50\tbatch_loss:762.9610\n",
            "Epochs:721\tstep:100\tbatch_loss:797.2833\n",
            "Epochs:721\tstep:150\tbatch_loss:793.2348\n",
            "Epochs:721\tstep:200\tbatch_loss:783.8446\n",
            "Epochs:721\tstep:250\tbatch_loss:943.0963\n",
            "Epochs:721\tstep:300\tbatch_loss:813.4902\n",
            "Epochs:721\tstep:350\tbatch_loss:787.5065\n",
            "Epochs:721\tstep:400\tbatch_loss:789.1507\n",
            "Epochs:721\tstep:450\tbatch_loss:797.4208\n",
            "Epochs:721\tstep:500\tbatch_loss:741.1071\n",
            "Epochs:721\tstep:550\tbatch_loss:817.0379\n",
            "Epochs:721\tstep:600\tbatch_loss:750.3309\n",
            "Epoch loss: 496869.5304001518\n",
            "RMSE on dev data: 9.88994\n",
            "RMSE on train data: 10.00761\n",
            "Epochs:722\tstep:50\tbatch_loss:762.8612\n",
            "Epochs:722\tstep:100\tbatch_loss:797.1846\n",
            "Epochs:722\tstep:150\tbatch_loss:793.1367\n",
            "Epochs:722\tstep:200\tbatch_loss:783.7458\n",
            "Epochs:722\tstep:250\tbatch_loss:942.9953\n",
            "Epochs:722\tstep:300\tbatch_loss:813.3908\n",
            "Epochs:722\tstep:350\tbatch_loss:787.4076\n",
            "Epochs:722\tstep:400\tbatch_loss:789.0517\n",
            "Epochs:722\tstep:450\tbatch_loss:797.3204\n",
            "Epochs:722\tstep:500\tbatch_loss:741.0086\n",
            "Epochs:722\tstep:550\tbatch_loss:816.9387\n",
            "Epochs:722\tstep:600\tbatch_loss:750.2326\n",
            "Epoch loss: 496806.4387749176\n",
            "RMSE on dev data: 9.88992\n",
            "RMSE on train data: 10.00759\n",
            "Epochs:723\tstep:50\tbatch_loss:762.7613\n",
            "Epochs:723\tstep:100\tbatch_loss:797.0859\n",
            "Epochs:723\tstep:150\tbatch_loss:793.0390\n",
            "Epochs:723\tstep:200\tbatch_loss:783.6472\n",
            "Epochs:723\tstep:250\tbatch_loss:942.8945\n",
            "Epochs:723\tstep:300\tbatch_loss:813.2915\n",
            "Epochs:723\tstep:350\tbatch_loss:787.3088\n",
            "Epochs:723\tstep:400\tbatch_loss:788.9528\n",
            "Epochs:723\tstep:450\tbatch_loss:797.2201\n",
            "Epochs:723\tstep:500\tbatch_loss:740.9102\n",
            "Epochs:723\tstep:550\tbatch_loss:816.8396\n",
            "Epochs:723\tstep:600\tbatch_loss:750.1344\n",
            "Epoch loss: 496743.43536612863\n",
            "RMSE on dev data: 9.88990\n",
            "RMSE on train data: 10.00758\n",
            "Epochs:724\tstep:50\tbatch_loss:762.6616\n",
            "Epochs:724\tstep:100\tbatch_loss:796.9874\n",
            "Epochs:724\tstep:150\tbatch_loss:792.9410\n",
            "Epochs:724\tstep:200\tbatch_loss:783.5487\n",
            "Epochs:724\tstep:250\tbatch_loss:942.7940\n",
            "Epochs:724\tstep:300\tbatch_loss:813.1922\n",
            "Epochs:724\tstep:350\tbatch_loss:787.2101\n",
            "Epochs:724\tstep:400\tbatch_loss:788.8541\n",
            "Epochs:724\tstep:450\tbatch_loss:797.1200\n",
            "Epochs:724\tstep:500\tbatch_loss:740.8119\n",
            "Epochs:724\tstep:550\tbatch_loss:816.7406\n",
            "Epochs:724\tstep:600\tbatch_loss:750.0362\n",
            "Epoch loss: 496680.4676831275\n",
            "RMSE on dev data: 9.88988\n",
            "RMSE on train data: 10.00757\n",
            "Epochs:725\tstep:50\tbatch_loss:762.5618\n",
            "Epochs:725\tstep:100\tbatch_loss:796.8888\n",
            "Epochs:725\tstep:150\tbatch_loss:792.8435\n",
            "Epochs:725\tstep:200\tbatch_loss:783.4502\n",
            "Epochs:725\tstep:250\tbatch_loss:942.6934\n",
            "Epochs:725\tstep:300\tbatch_loss:813.0930\n",
            "Epochs:725\tstep:350\tbatch_loss:787.1114\n",
            "Epochs:725\tstep:400\tbatch_loss:788.7553\n",
            "Epochs:725\tstep:450\tbatch_loss:797.0198\n",
            "Epochs:725\tstep:500\tbatch_loss:740.7136\n",
            "Epochs:725\tstep:550\tbatch_loss:816.6416\n",
            "Epochs:725\tstep:600\tbatch_loss:749.9380\n",
            "Epoch loss: 496617.53491701395\n",
            "RMSE on dev data: 9.88986\n",
            "RMSE on train data: 10.00755\n",
            "Epochs:726\tstep:50\tbatch_loss:762.4206\n",
            "Epochs:726\tstep:100\tbatch_loss:796.8204\n",
            "Epochs:726\tstep:150\tbatch_loss:792.0195\n",
            "Epochs:726\tstep:200\tbatch_loss:783.3051\n",
            "Epochs:726\tstep:250\tbatch_loss:942.3911\n",
            "Epochs:726\tstep:300\tbatch_loss:812.9508\n",
            "Epochs:726\tstep:350\tbatch_loss:787.0328\n",
            "Epochs:726\tstep:400\tbatch_loss:788.6790\n",
            "Epochs:726\tstep:450\tbatch_loss:796.9134\n",
            "Epochs:726\tstep:500\tbatch_loss:740.7291\n",
            "Epochs:726\tstep:550\tbatch_loss:816.2332\n",
            "Epochs:726\tstep:600\tbatch_loss:750.0734\n",
            "Epoch loss: 496490.41748839273\n",
            "RMSE on dev data: 9.88862\n",
            "RMSE on train data: 10.00769\n",
            "Epochs:727\tstep:50\tbatch_loss:762.2644\n",
            "Epochs:727\tstep:100\tbatch_loss:796.7399\n",
            "Epochs:727\tstep:150\tbatch_loss:791.9172\n",
            "Epochs:727\tstep:200\tbatch_loss:783.2081\n",
            "Epochs:727\tstep:250\tbatch_loss:942.3018\n",
            "Epochs:727\tstep:300\tbatch_loss:812.8606\n",
            "Epochs:727\tstep:350\tbatch_loss:786.9435\n",
            "Epochs:727\tstep:400\tbatch_loss:788.5893\n",
            "Epochs:727\tstep:450\tbatch_loss:796.8217\n",
            "Epochs:727\tstep:500\tbatch_loss:740.6399\n",
            "Epochs:727\tstep:550\tbatch_loss:816.1433\n",
            "Epochs:727\tstep:600\tbatch_loss:749.9839\n",
            "Epoch loss: 496429.6611867329\n",
            "RMSE on dev data: 9.88859\n",
            "RMSE on train data: 10.00766\n",
            "Epochs:728\tstep:50\tbatch_loss:762.1735\n",
            "Epochs:728\tstep:100\tbatch_loss:796.6498\n",
            "Epochs:728\tstep:150\tbatch_loss:791.8285\n",
            "Epochs:728\tstep:200\tbatch_loss:783.1176\n",
            "Epochs:728\tstep:250\tbatch_loss:942.2093\n",
            "Epochs:728\tstep:300\tbatch_loss:812.7705\n",
            "Epochs:728\tstep:350\tbatch_loss:786.8541\n",
            "Epochs:728\tstep:400\tbatch_loss:788.4997\n",
            "Epochs:728\tstep:450\tbatch_loss:796.7303\n",
            "Epochs:728\tstep:500\tbatch_loss:740.5509\n",
            "Epochs:728\tstep:550\tbatch_loss:816.0535\n",
            "Epochs:728\tstep:600\tbatch_loss:749.8946\n",
            "Epoch loss: 496372.3047599224\n",
            "RMSE on dev data: 9.88856\n",
            "RMSE on train data: 10.00764\n",
            "Epochs:729\tstep:50\tbatch_loss:762.0828\n",
            "Epochs:729\tstep:100\tbatch_loss:796.5599\n",
            "Epochs:729\tstep:150\tbatch_loss:791.7396\n",
            "Epochs:729\tstep:200\tbatch_loss:783.0272\n",
            "Epochs:729\tstep:250\tbatch_loss:942.1172\n",
            "Epochs:729\tstep:300\tbatch_loss:812.6805\n",
            "Epochs:729\tstep:350\tbatch_loss:786.7648\n",
            "Epochs:729\tstep:400\tbatch_loss:788.4104\n",
            "Epochs:729\tstep:450\tbatch_loss:796.6390\n",
            "Epochs:729\tstep:500\tbatch_loss:740.4620\n",
            "Epochs:729\tstep:550\tbatch_loss:815.9639\n",
            "Epochs:729\tstep:600\tbatch_loss:749.8054\n",
            "Epoch loss: 496315.04274777864\n",
            "RMSE on dev data: 9.88854\n",
            "RMSE on train data: 10.00762\n",
            "Epochs:730\tstep:50\tbatch_loss:761.9923\n",
            "Epochs:730\tstep:100\tbatch_loss:796.4702\n",
            "Epochs:730\tstep:150\tbatch_loss:791.6511\n",
            "Epochs:730\tstep:200\tbatch_loss:782.9371\n",
            "Epochs:730\tstep:250\tbatch_loss:942.0251\n",
            "Epochs:730\tstep:300\tbatch_loss:812.5907\n",
            "Epochs:730\tstep:350\tbatch_loss:786.6756\n",
            "Epochs:730\tstep:400\tbatch_loss:788.3210\n",
            "Epochs:730\tstep:450\tbatch_loss:796.5479\n",
            "Epochs:730\tstep:500\tbatch_loss:740.3732\n",
            "Epochs:730\tstep:550\tbatch_loss:815.8743\n",
            "Epochs:730\tstep:600\tbatch_loss:749.7164\n",
            "Epoch loss: 496257.8884584954\n",
            "RMSE on dev data: 9.88851\n",
            "RMSE on train data: 10.00760\n",
            "Epochs:731\tstep:50\tbatch_loss:761.9018\n",
            "Epochs:731\tstep:100\tbatch_loss:796.3807\n",
            "Epochs:731\tstep:150\tbatch_loss:791.5628\n",
            "Epochs:731\tstep:200\tbatch_loss:782.8471\n",
            "Epochs:731\tstep:250\tbatch_loss:941.9332\n",
            "Epochs:731\tstep:300\tbatch_loss:812.5010\n",
            "Epochs:731\tstep:350\tbatch_loss:786.5864\n",
            "Epochs:731\tstep:400\tbatch_loss:788.2318\n",
            "Epochs:731\tstep:450\tbatch_loss:796.4570\n",
            "Epochs:731\tstep:500\tbatch_loss:740.2845\n",
            "Epochs:731\tstep:550\tbatch_loss:815.7849\n",
            "Epochs:731\tstep:600\tbatch_loss:749.6275\n",
            "Epoch loss: 496200.8236050361\n",
            "RMSE on dev data: 9.88849\n",
            "RMSE on train data: 10.00758\n",
            "Epochs:732\tstep:50\tbatch_loss:761.8116\n",
            "Epochs:732\tstep:100\tbatch_loss:796.2914\n",
            "Epochs:732\tstep:150\tbatch_loss:791.4744\n",
            "Epochs:732\tstep:200\tbatch_loss:782.7574\n",
            "Epochs:732\tstep:250\tbatch_loss:941.8417\n",
            "Epochs:732\tstep:300\tbatch_loss:812.4114\n",
            "Epochs:732\tstep:350\tbatch_loss:786.4974\n",
            "Epochs:732\tstep:400\tbatch_loss:788.1429\n",
            "Epochs:732\tstep:450\tbatch_loss:796.3663\n",
            "Epochs:732\tstep:500\tbatch_loss:740.1959\n",
            "Epochs:732\tstep:550\tbatch_loss:815.6956\n",
            "Epochs:732\tstep:600\tbatch_loss:749.5388\n",
            "Epoch loss: 496143.8760018315\n",
            "RMSE on dev data: 9.88846\n",
            "RMSE on train data: 10.00756\n",
            "Epochs:733\tstep:50\tbatch_loss:761.7215\n",
            "Epochs:733\tstep:100\tbatch_loss:796.2022\n",
            "Epochs:733\tstep:150\tbatch_loss:791.3865\n",
            "Epochs:733\tstep:200\tbatch_loss:782.6679\n",
            "Epochs:733\tstep:250\tbatch_loss:941.7503\n",
            "Epochs:733\tstep:300\tbatch_loss:812.3220\n",
            "Epochs:733\tstep:350\tbatch_loss:786.4086\n",
            "Epochs:733\tstep:400\tbatch_loss:788.0539\n",
            "Epochs:733\tstep:450\tbatch_loss:796.2757\n",
            "Epochs:733\tstep:500\tbatch_loss:740.1074\n",
            "Epochs:733\tstep:550\tbatch_loss:815.6064\n",
            "Epochs:733\tstep:600\tbatch_loss:749.4502\n",
            "Epoch loss: 496087.03653284843\n",
            "RMSE on dev data: 9.88844\n",
            "RMSE on train data: 10.00754\n",
            "Epochs:734\tstep:50\tbatch_loss:761.6316\n",
            "Epochs:734\tstep:100\tbatch_loss:796.1132\n",
            "Epochs:734\tstep:150\tbatch_loss:791.2982\n",
            "Epochs:734\tstep:200\tbatch_loss:782.5785\n",
            "Epochs:734\tstep:250\tbatch_loss:941.6590\n",
            "Epochs:734\tstep:300\tbatch_loss:812.2326\n",
            "Epochs:734\tstep:350\tbatch_loss:786.3198\n",
            "Epochs:734\tstep:400\tbatch_loss:787.9650\n",
            "Epochs:734\tstep:450\tbatch_loss:796.1853\n",
            "Epochs:734\tstep:500\tbatch_loss:740.0190\n",
            "Epochs:734\tstep:550\tbatch_loss:815.5173\n",
            "Epochs:734\tstep:600\tbatch_loss:749.3618\n",
            "Epoch loss: 496030.2422542478\n",
            "RMSE on dev data: 9.88842\n",
            "RMSE on train data: 10.00752\n",
            "Epochs:735\tstep:50\tbatch_loss:761.5417\n",
            "Epochs:735\tstep:100\tbatch_loss:796.0243\n",
            "Epochs:735\tstep:150\tbatch_loss:791.2103\n",
            "Epochs:735\tstep:200\tbatch_loss:782.4894\n",
            "Epochs:735\tstep:250\tbatch_loss:941.5680\n",
            "Epochs:735\tstep:300\tbatch_loss:812.1434\n",
            "Epochs:735\tstep:350\tbatch_loss:786.2311\n",
            "Epochs:735\tstep:400\tbatch_loss:787.8764\n",
            "Epochs:735\tstep:450\tbatch_loss:796.0950\n",
            "Epochs:735\tstep:500\tbatch_loss:739.9307\n",
            "Epochs:735\tstep:550\tbatch_loss:815.4283\n",
            "Epochs:735\tstep:600\tbatch_loss:749.2734\n",
            "Epoch loss: 495973.5610630882\n",
            "RMSE on dev data: 9.88839\n",
            "RMSE on train data: 10.00750\n",
            "Epochs:736\tstep:50\tbatch_loss:761.4519\n",
            "Epochs:736\tstep:100\tbatch_loss:795.9355\n",
            "Epochs:736\tstep:150\tbatch_loss:791.1227\n",
            "Epochs:736\tstep:200\tbatch_loss:782.4003\n",
            "Epochs:736\tstep:250\tbatch_loss:941.4770\n",
            "Epochs:736\tstep:300\tbatch_loss:812.0542\n",
            "Epochs:736\tstep:350\tbatch_loss:786.1425\n",
            "Epochs:736\tstep:400\tbatch_loss:787.7877\n",
            "Epochs:736\tstep:450\tbatch_loss:796.0048\n",
            "Epochs:736\tstep:500\tbatch_loss:739.8425\n",
            "Epochs:736\tstep:550\tbatch_loss:815.3394\n",
            "Epochs:736\tstep:600\tbatch_loss:749.1851\n",
            "Epoch loss: 495916.92923245695\n",
            "RMSE on dev data: 9.88837\n",
            "RMSE on train data: 10.00748\n",
            "Epochs:737\tstep:50\tbatch_loss:761.3623\n",
            "Epochs:737\tstep:100\tbatch_loss:795.8469\n",
            "Epochs:737\tstep:150\tbatch_loss:791.0348\n",
            "Epochs:737\tstep:200\tbatch_loss:782.3113\n",
            "Epochs:737\tstep:250\tbatch_loss:941.3861\n",
            "Epochs:737\tstep:300\tbatch_loss:811.9652\n",
            "Epochs:737\tstep:350\tbatch_loss:786.0540\n",
            "Epochs:737\tstep:400\tbatch_loss:787.6991\n",
            "Epochs:737\tstep:450\tbatch_loss:795.9147\n",
            "Epochs:737\tstep:500\tbatch_loss:739.7544\n",
            "Epochs:737\tstep:550\tbatch_loss:815.2505\n",
            "Epochs:737\tstep:600\tbatch_loss:749.0970\n",
            "Epoch loss: 495860.3639003158\n",
            "RMSE on dev data: 9.88835\n",
            "RMSE on train data: 10.00747\n",
            "Epochs:738\tstep:50\tbatch_loss:761.2728\n",
            "Epochs:738\tstep:100\tbatch_loss:795.7584\n",
            "Epochs:738\tstep:150\tbatch_loss:790.9474\n",
            "Epochs:738\tstep:200\tbatch_loss:782.2227\n",
            "Epochs:738\tstep:250\tbatch_loss:941.2956\n",
            "Epochs:738\tstep:300\tbatch_loss:811.8763\n",
            "Epochs:738\tstep:350\tbatch_loss:785.9656\n",
            "Epochs:738\tstep:400\tbatch_loss:787.6107\n",
            "Epochs:738\tstep:450\tbatch_loss:795.8249\n",
            "Epochs:738\tstep:500\tbatch_loss:739.6664\n",
            "Epochs:738\tstep:550\tbatch_loss:815.1619\n",
            "Epochs:738\tstep:600\tbatch_loss:749.0090\n",
            "Epoch loss: 495803.92869001144\n",
            "RMSE on dev data: 9.88833\n",
            "RMSE on train data: 10.00745\n",
            "Epochs:739\tstep:50\tbatch_loss:761.1834\n",
            "Epochs:739\tstep:100\tbatch_loss:795.6701\n",
            "Epochs:739\tstep:150\tbatch_loss:790.8596\n",
            "Epochs:739\tstep:200\tbatch_loss:782.1340\n",
            "Epochs:739\tstep:250\tbatch_loss:941.2050\n",
            "Epochs:739\tstep:300\tbatch_loss:811.7874\n",
            "Epochs:739\tstep:350\tbatch_loss:785.8772\n",
            "Epochs:739\tstep:400\tbatch_loss:787.5223\n",
            "Epochs:739\tstep:450\tbatch_loss:795.7350\n",
            "Epochs:739\tstep:500\tbatch_loss:739.5784\n",
            "Epochs:739\tstep:550\tbatch_loss:815.0732\n",
            "Epochs:739\tstep:600\tbatch_loss:748.9210\n",
            "Epoch loss: 495747.4825236015\n",
            "RMSE on dev data: 9.88831\n",
            "RMSE on train data: 10.00744\n",
            "Epochs:740\tstep:50\tbatch_loss:761.0940\n",
            "Epochs:740\tstep:100\tbatch_loss:795.5818\n",
            "Epochs:740\tstep:150\tbatch_loss:790.7723\n",
            "Epochs:740\tstep:200\tbatch_loss:782.0456\n",
            "Epochs:740\tstep:250\tbatch_loss:941.1148\n",
            "Epochs:740\tstep:300\tbatch_loss:811.6986\n",
            "Epochs:740\tstep:350\tbatch_loss:785.7889\n",
            "Epochs:740\tstep:400\tbatch_loss:787.4341\n",
            "Epochs:740\tstep:450\tbatch_loss:795.6454\n",
            "Epochs:740\tstep:500\tbatch_loss:739.4905\n",
            "Epochs:740\tstep:550\tbatch_loss:814.9846\n",
            "Epochs:740\tstep:600\tbatch_loss:748.8331\n",
            "Epoch loss: 495691.15922351694\n",
            "RMSE on dev data: 9.88829\n",
            "RMSE on train data: 10.00742\n",
            "Epochs:741\tstep:50\tbatch_loss:761.0049\n",
            "Epochs:741\tstep:100\tbatch_loss:795.4937\n",
            "Epochs:741\tstep:150\tbatch_loss:790.6848\n",
            "Epochs:741\tstep:200\tbatch_loss:781.9571\n",
            "Epochs:741\tstep:250\tbatch_loss:941.0245\n",
            "Epochs:741\tstep:300\tbatch_loss:811.6099\n",
            "Epochs:741\tstep:350\tbatch_loss:785.7008\n",
            "Epochs:741\tstep:400\tbatch_loss:787.3458\n",
            "Epochs:741\tstep:450\tbatch_loss:795.5557\n",
            "Epochs:741\tstep:500\tbatch_loss:739.4027\n",
            "Epochs:741\tstep:550\tbatch_loss:814.8961\n",
            "Epochs:741\tstep:600\tbatch_loss:748.7454\n",
            "Epoch loss: 495634.857182428\n",
            "RMSE on dev data: 9.88827\n",
            "RMSE on train data: 10.00740\n",
            "Epochs:742\tstep:50\tbatch_loss:760.9156\n",
            "Epochs:742\tstep:100\tbatch_loss:795.4055\n",
            "Epochs:742\tstep:150\tbatch_loss:790.5975\n",
            "Epochs:742\tstep:200\tbatch_loss:781.8688\n",
            "Epochs:742\tstep:250\tbatch_loss:940.9342\n",
            "Epochs:742\tstep:300\tbatch_loss:811.5212\n",
            "Epochs:742\tstep:350\tbatch_loss:785.6126\n",
            "Epochs:742\tstep:400\tbatch_loss:787.2576\n",
            "Epochs:742\tstep:450\tbatch_loss:795.4661\n",
            "Epochs:742\tstep:500\tbatch_loss:739.3149\n",
            "Epochs:742\tstep:550\tbatch_loss:814.8076\n",
            "Epochs:742\tstep:600\tbatch_loss:748.6576\n",
            "Epoch loss: 495578.5925581588\n",
            "RMSE on dev data: 9.88825\n",
            "RMSE on train data: 10.00739\n",
            "Epochs:743\tstep:50\tbatch_loss:760.8266\n",
            "Epochs:743\tstep:100\tbatch_loss:795.3175\n",
            "Epochs:743\tstep:150\tbatch_loss:790.5105\n",
            "Epochs:743\tstep:200\tbatch_loss:781.7808\n",
            "Epochs:743\tstep:250\tbatch_loss:940.8443\n",
            "Epochs:743\tstep:300\tbatch_loss:811.4327\n",
            "Epochs:743\tstep:350\tbatch_loss:785.5245\n",
            "Epochs:743\tstep:400\tbatch_loss:787.1696\n",
            "Epochs:743\tstep:450\tbatch_loss:795.3768\n",
            "Epochs:743\tstep:500\tbatch_loss:739.2273\n",
            "Epochs:743\tstep:550\tbatch_loss:814.7193\n",
            "Epochs:743\tstep:600\tbatch_loss:748.5701\n",
            "Epoch loss: 495522.4457323104\n",
            "RMSE on dev data: 9.88823\n",
            "RMSE on train data: 10.00738\n",
            "Epochs:744\tstep:50\tbatch_loss:760.7376\n",
            "Epochs:744\tstep:100\tbatch_loss:795.2296\n",
            "Epochs:744\tstep:150\tbatch_loss:790.4232\n",
            "Epochs:744\tstep:200\tbatch_loss:781.6926\n",
            "Epochs:744\tstep:250\tbatch_loss:940.7543\n",
            "Epochs:744\tstep:300\tbatch_loss:811.3442\n",
            "Epochs:744\tstep:350\tbatch_loss:785.4365\n",
            "Epochs:744\tstep:400\tbatch_loss:787.0815\n",
            "Epochs:744\tstep:450\tbatch_loss:795.2873\n",
            "Epochs:744\tstep:500\tbatch_loss:739.1397\n",
            "Epochs:744\tstep:550\tbatch_loss:814.6310\n",
            "Epochs:744\tstep:600\tbatch_loss:748.4825\n",
            "Epoch loss: 495466.2743142465\n",
            "RMSE on dev data: 9.88821\n",
            "RMSE on train data: 10.00736\n",
            "Epochs:745\tstep:50\tbatch_loss:760.6487\n",
            "Epochs:745\tstep:100\tbatch_loss:795.1418\n",
            "Epochs:745\tstep:150\tbatch_loss:790.3364\n",
            "Epochs:745\tstep:200\tbatch_loss:781.6047\n",
            "Epochs:745\tstep:250\tbatch_loss:940.6644\n",
            "Epochs:745\tstep:300\tbatch_loss:811.2558\n",
            "Epochs:745\tstep:350\tbatch_loss:785.3486\n",
            "Epochs:745\tstep:400\tbatch_loss:786.9936\n",
            "Epochs:745\tstep:450\tbatch_loss:795.1981\n",
            "Epochs:745\tstep:500\tbatch_loss:739.0522\n",
            "Epochs:745\tstep:550\tbatch_loss:814.5428\n",
            "Epochs:745\tstep:600\tbatch_loss:748.3951\n",
            "Epoch loss: 495410.2229290002\n",
            "RMSE on dev data: 9.88820\n",
            "RMSE on train data: 10.00735\n",
            "Epochs:746\tstep:50\tbatch_loss:760.5598\n",
            "Epochs:746\tstep:100\tbatch_loss:795.0540\n",
            "Epochs:746\tstep:150\tbatch_loss:790.2492\n",
            "Epochs:746\tstep:200\tbatch_loss:781.5168\n",
            "Epochs:746\tstep:250\tbatch_loss:940.5747\n",
            "Epochs:746\tstep:300\tbatch_loss:811.1674\n",
            "Epochs:746\tstep:350\tbatch_loss:785.2607\n",
            "Epochs:746\tstep:400\tbatch_loss:786.9058\n",
            "Epochs:746\tstep:450\tbatch_loss:795.1088\n",
            "Epochs:746\tstep:500\tbatch_loss:738.9647\n",
            "Epochs:746\tstep:550\tbatch_loss:814.4547\n",
            "Epochs:746\tstep:600\tbatch_loss:748.3076\n",
            "Epoch loss: 495354.1565771047\n",
            "RMSE on dev data: 9.88818\n",
            "RMSE on train data: 10.00733\n",
            "Epochs:747\tstep:50\tbatch_loss:760.4711\n",
            "Epochs:747\tstep:100\tbatch_loss:794.9663\n",
            "Epochs:747\tstep:150\tbatch_loss:790.1623\n",
            "Epochs:747\tstep:200\tbatch_loss:781.4290\n",
            "Epochs:747\tstep:250\tbatch_loss:940.4850\n",
            "Epochs:747\tstep:300\tbatch_loss:811.0791\n",
            "Epochs:747\tstep:350\tbatch_loss:785.1729\n",
            "Epochs:747\tstep:400\tbatch_loss:786.8179\n",
            "Epochs:747\tstep:450\tbatch_loss:795.0197\n",
            "Epochs:747\tstep:500\tbatch_loss:738.8773\n",
            "Epochs:747\tstep:550\tbatch_loss:814.3666\n",
            "Epochs:747\tstep:600\tbatch_loss:748.2203\n",
            "Epoch loss: 495298.17920122365\n",
            "RMSE on dev data: 9.88816\n",
            "RMSE on train data: 10.00732\n",
            "Epochs:748\tstep:50\tbatch_loss:760.3824\n",
            "Epochs:748\tstep:100\tbatch_loss:794.8787\n",
            "Epochs:748\tstep:150\tbatch_loss:790.0757\n",
            "Epochs:748\tstep:200\tbatch_loss:781.3414\n",
            "Epochs:748\tstep:250\tbatch_loss:940.3954\n",
            "Epochs:748\tstep:300\tbatch_loss:810.9910\n",
            "Epochs:748\tstep:350\tbatch_loss:785.0852\n",
            "Epochs:748\tstep:400\tbatch_loss:786.7302\n",
            "Epochs:748\tstep:450\tbatch_loss:794.9307\n",
            "Epochs:748\tstep:500\tbatch_loss:738.7899\n",
            "Epochs:748\tstep:550\tbatch_loss:814.2786\n",
            "Epochs:748\tstep:600\tbatch_loss:748.1331\n",
            "Epoch loss: 495242.26674339664\n",
            "RMSE on dev data: 9.88814\n",
            "RMSE on train data: 10.00731\n",
            "Epochs:749\tstep:50\tbatch_loss:760.2938\n",
            "Epochs:749\tstep:100\tbatch_loss:794.7912\n",
            "Epochs:749\tstep:150\tbatch_loss:789.9887\n",
            "Epochs:749\tstep:200\tbatch_loss:781.2537\n",
            "Epochs:749\tstep:250\tbatch_loss:940.3060\n",
            "Epochs:749\tstep:300\tbatch_loss:810.9028\n",
            "Epochs:749\tstep:350\tbatch_loss:784.9975\n",
            "Epochs:749\tstep:400\tbatch_loss:786.6426\n",
            "Epochs:749\tstep:450\tbatch_loss:794.8416\n",
            "Epochs:749\tstep:500\tbatch_loss:738.7027\n",
            "Epochs:749\tstep:550\tbatch_loss:814.1906\n",
            "Epochs:749\tstep:600\tbatch_loss:748.0459\n",
            "Epoch loss: 495186.33401031274\n",
            "RMSE on dev data: 9.88813\n",
            "RMSE on train data: 10.00729\n",
            "Epochs:750\tstep:50\tbatch_loss:760.2052\n",
            "Epochs:750\tstep:100\tbatch_loss:794.7037\n",
            "Epochs:750\tstep:150\tbatch_loss:789.9023\n",
            "Epochs:750\tstep:200\tbatch_loss:781.1663\n",
            "Epochs:750\tstep:250\tbatch_loss:940.2165\n",
            "Epochs:750\tstep:300\tbatch_loss:810.8147\n",
            "Epochs:750\tstep:350\tbatch_loss:784.9099\n",
            "Epochs:750\tstep:400\tbatch_loss:786.5550\n",
            "Epochs:750\tstep:450\tbatch_loss:794.7528\n",
            "Epochs:750\tstep:500\tbatch_loss:738.6155\n",
            "Epochs:750\tstep:550\tbatch_loss:814.1028\n",
            "Epochs:750\tstep:600\tbatch_loss:747.9589\n",
            "Epoch loss: 495130.513635862\n",
            "RMSE on dev data: 9.88811\n",
            "RMSE on train data: 10.00728\n",
            "Epochs:751\tstep:50\tbatch_loss:760.0643\n",
            "Epochs:751\tstep:100\tbatch_loss:794.6521\n",
            "Epochs:751\tstep:150\tbatch_loss:789.1629\n",
            "Epochs:751\tstep:200\tbatch_loss:780.9903\n",
            "Epochs:751\tstep:250\tbatch_loss:939.9609\n",
            "Epochs:751\tstep:300\tbatch_loss:810.7279\n",
            "Epochs:751\tstep:350\tbatch_loss:784.8124\n",
            "Epochs:751\tstep:400\tbatch_loss:786.4953\n",
            "Epochs:751\tstep:450\tbatch_loss:794.6659\n",
            "Epochs:751\tstep:500\tbatch_loss:738.6499\n",
            "Epochs:751\tstep:550\tbatch_loss:813.6680\n",
            "Epochs:751\tstep:600\tbatch_loss:748.0985\n",
            "Epoch loss: 495013.84694709163\n",
            "RMSE on dev data: 9.88712\n",
            "RMSE on train data: 10.00759\n",
            "Epochs:752\tstep:50\tbatch_loss:759.9180\n",
            "Epochs:752\tstep:100\tbatch_loss:794.5834\n",
            "Epochs:752\tstep:150\tbatch_loss:789.0692\n",
            "Epochs:752\tstep:200\tbatch_loss:780.9009\n",
            "Epochs:752\tstep:250\tbatch_loss:939.8832\n",
            "Epochs:752\tstep:300\tbatch_loss:810.6474\n",
            "Epochs:752\tstep:350\tbatch_loss:784.7332\n",
            "Epochs:752\tstep:400\tbatch_loss:786.4154\n",
            "Epochs:752\tstep:450\tbatch_loss:794.5843\n",
            "Epochs:752\tstep:500\tbatch_loss:738.5706\n",
            "Epochs:752\tstep:550\tbatch_loss:813.5880\n",
            "Epochs:752\tstep:600\tbatch_loss:748.0189\n",
            "Epoch loss: 494959.4556252948\n",
            "RMSE on dev data: 9.88708\n",
            "RMSE on train data: 10.00757\n",
            "Epochs:753\tstep:50\tbatch_loss:759.8371\n",
            "Epochs:753\tstep:100\tbatch_loss:794.5032\n",
            "Epochs:753\tstep:150\tbatch_loss:788.9904\n",
            "Epochs:753\tstep:200\tbatch_loss:780.8198\n",
            "Epochs:753\tstep:250\tbatch_loss:939.8008\n",
            "Epochs:753\tstep:300\tbatch_loss:810.5671\n",
            "Epochs:753\tstep:350\tbatch_loss:784.6537\n",
            "Epochs:753\tstep:400\tbatch_loss:786.3357\n",
            "Epochs:753\tstep:450\tbatch_loss:794.5028\n",
            "Epochs:753\tstep:500\tbatch_loss:738.4914\n",
            "Epochs:753\tstep:550\tbatch_loss:813.5080\n",
            "Epochs:753\tstep:600\tbatch_loss:747.9394\n",
            "Epoch loss: 494908.42431689246\n",
            "RMSE on dev data: 9.88706\n",
            "RMSE on train data: 10.00754\n",
            "Epochs:754\tstep:50\tbatch_loss:759.7565\n",
            "Epochs:754\tstep:100\tbatch_loss:794.4233\n",
            "Epochs:754\tstep:150\tbatch_loss:788.9114\n",
            "Epochs:754\tstep:200\tbatch_loss:780.7389\n",
            "Epochs:754\tstep:250\tbatch_loss:939.7187\n",
            "Epochs:754\tstep:300\tbatch_loss:810.4870\n",
            "Epochs:754\tstep:350\tbatch_loss:784.5742\n",
            "Epochs:754\tstep:400\tbatch_loss:786.2563\n",
            "Epochs:754\tstep:450\tbatch_loss:794.4216\n",
            "Epochs:754\tstep:500\tbatch_loss:738.4124\n",
            "Epochs:754\tstep:550\tbatch_loss:813.4282\n",
            "Epochs:754\tstep:600\tbatch_loss:747.8601\n",
            "Epoch loss: 494857.50182229246\n",
            "RMSE on dev data: 9.88703\n",
            "RMSE on train data: 10.00752\n",
            "Epochs:755\tstep:50\tbatch_loss:759.6758\n",
            "Epochs:755\tstep:100\tbatch_loss:794.3434\n",
            "Epochs:755\tstep:150\tbatch_loss:788.8329\n",
            "Epochs:755\tstep:200\tbatch_loss:780.6583\n",
            "Epochs:755\tstep:250\tbatch_loss:939.6366\n",
            "Epochs:755\tstep:300\tbatch_loss:810.4070\n",
            "Epochs:755\tstep:350\tbatch_loss:784.4949\n",
            "Epochs:755\tstep:400\tbatch_loss:786.1768\n",
            "Epochs:755\tstep:450\tbatch_loss:794.3405\n",
            "Epochs:755\tstep:500\tbatch_loss:738.3334\n",
            "Epochs:755\tstep:550\tbatch_loss:813.3485\n",
            "Epochs:755\tstep:600\tbatch_loss:747.7809\n",
            "Epoch loss: 494806.659021015\n",
            "RMSE on dev data: 9.88700\n",
            "RMSE on train data: 10.00750\n",
            "Epochs:756\tstep:50\tbatch_loss:759.5955\n",
            "Epochs:756\tstep:100\tbatch_loss:794.2639\n",
            "Epochs:756\tstep:150\tbatch_loss:788.7541\n",
            "Epochs:756\tstep:200\tbatch_loss:780.5778\n",
            "Epochs:756\tstep:250\tbatch_loss:939.5548\n",
            "Epochs:756\tstep:300\tbatch_loss:810.3272\n",
            "Epochs:756\tstep:350\tbatch_loss:784.4157\n",
            "Epochs:756\tstep:400\tbatch_loss:786.0974\n",
            "Epochs:756\tstep:450\tbatch_loss:794.2596\n",
            "Epochs:756\tstep:500\tbatch_loss:738.2546\n",
            "Epochs:756\tstep:550\tbatch_loss:813.2688\n",
            "Epochs:756\tstep:600\tbatch_loss:747.7019\n",
            "Epoch loss: 494755.91310412786\n",
            "RMSE on dev data: 9.88697\n",
            "RMSE on train data: 10.00748\n",
            "Epochs:757\tstep:50\tbatch_loss:759.5151\n",
            "Epochs:757\tstep:100\tbatch_loss:794.1844\n",
            "Epochs:757\tstep:150\tbatch_loss:788.6759\n",
            "Epochs:757\tstep:200\tbatch_loss:780.4976\n",
            "Epochs:757\tstep:250\tbatch_loss:939.4732\n",
            "Epochs:757\tstep:300\tbatch_loss:810.2475\n",
            "Epochs:757\tstep:350\tbatch_loss:784.3366\n",
            "Epochs:757\tstep:400\tbatch_loss:786.0183\n",
            "Epochs:757\tstep:450\tbatch_loss:794.1789\n",
            "Epochs:757\tstep:500\tbatch_loss:738.1758\n",
            "Epochs:757\tstep:550\tbatch_loss:813.1894\n",
            "Epochs:757\tstep:600\tbatch_loss:747.6230\n",
            "Epoch loss: 494705.2886127569\n",
            "RMSE on dev data: 9.88695\n",
            "RMSE on train data: 10.00746\n",
            "Epochs:758\tstep:50\tbatch_loss:759.4349\n",
            "Epochs:758\tstep:100\tbatch_loss:794.1051\n",
            "Epochs:758\tstep:150\tbatch_loss:788.5973\n",
            "Epochs:758\tstep:200\tbatch_loss:780.4173\n",
            "Epochs:758\tstep:250\tbatch_loss:939.3917\n",
            "Epochs:758\tstep:300\tbatch_loss:810.1678\n",
            "Epochs:758\tstep:350\tbatch_loss:784.2575\n",
            "Epochs:758\tstep:400\tbatch_loss:785.9392\n",
            "Epochs:758\tstep:450\tbatch_loss:794.0982\n",
            "Epochs:758\tstep:500\tbatch_loss:738.0971\n",
            "Epochs:758\tstep:550\tbatch_loss:813.1099\n",
            "Epochs:758\tstep:600\tbatch_loss:747.5442\n",
            "Epoch loss: 494654.66277862067\n",
            "RMSE on dev data: 9.88692\n",
            "RMSE on train data: 10.00744\n",
            "Epochs:759\tstep:50\tbatch_loss:759.3548\n",
            "Epochs:759\tstep:100\tbatch_loss:794.0259\n",
            "Epochs:759\tstep:150\tbatch_loss:788.5191\n",
            "Epochs:759\tstep:200\tbatch_loss:780.3374\n",
            "Epochs:759\tstep:250\tbatch_loss:939.3103\n",
            "Epochs:759\tstep:300\tbatch_loss:810.0882\n",
            "Epochs:759\tstep:350\tbatch_loss:784.1785\n",
            "Epochs:759\tstep:400\tbatch_loss:785.8601\n",
            "Epochs:759\tstep:450\tbatch_loss:794.0177\n",
            "Epochs:759\tstep:500\tbatch_loss:738.0186\n",
            "Epochs:759\tstep:550\tbatch_loss:813.0305\n",
            "Epochs:759\tstep:600\tbatch_loss:747.4655\n",
            "Epoch loss: 494604.1689898003\n",
            "RMSE on dev data: 9.88690\n",
            "RMSE on train data: 10.00742\n",
            "Epochs:760\tstep:50\tbatch_loss:759.2748\n",
            "Epochs:760\tstep:100\tbatch_loss:793.9468\n",
            "Epochs:760\tstep:150\tbatch_loss:788.4411\n",
            "Epochs:760\tstep:200\tbatch_loss:780.2577\n",
            "Epochs:760\tstep:250\tbatch_loss:939.2292\n",
            "Epochs:760\tstep:300\tbatch_loss:810.0088\n",
            "Epochs:760\tstep:350\tbatch_loss:784.0996\n",
            "Epochs:760\tstep:400\tbatch_loss:785.7813\n",
            "Epochs:760\tstep:450\tbatch_loss:793.9373\n",
            "Epochs:760\tstep:500\tbatch_loss:737.9400\n",
            "Epochs:760\tstep:550\tbatch_loss:812.9513\n",
            "Epochs:760\tstep:600\tbatch_loss:747.3869\n",
            "Epoch loss: 494553.7465957752\n",
            "RMSE on dev data: 9.88687\n",
            "RMSE on train data: 10.00740\n",
            "Epochs:761\tstep:50\tbatch_loss:759.1950\n",
            "Epochs:761\tstep:100\tbatch_loss:793.8679\n",
            "Epochs:761\tstep:150\tbatch_loss:788.3629\n",
            "Epochs:761\tstep:200\tbatch_loss:780.1780\n",
            "Epochs:761\tstep:250\tbatch_loss:939.1481\n",
            "Epochs:761\tstep:300\tbatch_loss:809.9294\n",
            "Epochs:761\tstep:350\tbatch_loss:784.0208\n",
            "Epochs:761\tstep:400\tbatch_loss:785.7024\n",
            "Epochs:761\tstep:450\tbatch_loss:793.8571\n",
            "Epochs:761\tstep:500\tbatch_loss:737.8616\n",
            "Epochs:761\tstep:550\tbatch_loss:812.8722\n",
            "Epochs:761\tstep:600\tbatch_loss:747.3084\n",
            "Epoch loss: 494503.3700521441\n",
            "RMSE on dev data: 9.88685\n",
            "RMSE on train data: 10.00738\n",
            "Epochs:762\tstep:50\tbatch_loss:759.1152\n",
            "Epochs:762\tstep:100\tbatch_loss:793.7890\n",
            "Epochs:762\tstep:150\tbatch_loss:788.2851\n",
            "Epochs:762\tstep:200\tbatch_loss:780.0985\n",
            "Epochs:762\tstep:250\tbatch_loss:939.0672\n",
            "Epochs:762\tstep:300\tbatch_loss:809.8502\n",
            "Epochs:762\tstep:350\tbatch_loss:783.9421\n",
            "Epochs:762\tstep:400\tbatch_loss:785.6237\n",
            "Epochs:762\tstep:450\tbatch_loss:793.7769\n",
            "Epochs:762\tstep:500\tbatch_loss:737.7833\n",
            "Epochs:762\tstep:550\tbatch_loss:812.7931\n",
            "Epochs:762\tstep:600\tbatch_loss:747.2300\n",
            "Epoch loss: 494453.0704230232\n",
            "RMSE on dev data: 9.88683\n",
            "RMSE on train data: 10.00736\n",
            "Epochs:763\tstep:50\tbatch_loss:759.0356\n",
            "Epochs:763\tstep:100\tbatch_loss:793.7103\n",
            "Epochs:763\tstep:150\tbatch_loss:788.2071\n",
            "Epochs:763\tstep:200\tbatch_loss:780.0192\n",
            "Epochs:763\tstep:250\tbatch_loss:938.9865\n",
            "Epochs:763\tstep:300\tbatch_loss:809.7710\n",
            "Epochs:763\tstep:350\tbatch_loss:783.8635\n",
            "Epochs:763\tstep:400\tbatch_loss:785.5451\n",
            "Epochs:763\tstep:450\tbatch_loss:793.6969\n",
            "Epochs:763\tstep:500\tbatch_loss:737.7050\n",
            "Epochs:763\tstep:550\tbatch_loss:812.7142\n",
            "Epochs:763\tstep:600\tbatch_loss:747.1516\n",
            "Epoch loss: 494402.8415372332\n",
            "RMSE on dev data: 9.88681\n",
            "RMSE on train data: 10.00735\n",
            "Epochs:764\tstep:50\tbatch_loss:758.9559\n",
            "Epochs:764\tstep:100\tbatch_loss:793.6317\n",
            "Epochs:764\tstep:150\tbatch_loss:788.1292\n",
            "Epochs:764\tstep:200\tbatch_loss:779.9399\n",
            "Epochs:764\tstep:250\tbatch_loss:938.9058\n",
            "Epochs:764\tstep:300\tbatch_loss:809.6919\n",
            "Epochs:764\tstep:350\tbatch_loss:783.7848\n",
            "Epochs:764\tstep:400\tbatch_loss:785.4665\n",
            "Epochs:764\tstep:450\tbatch_loss:793.6169\n",
            "Epochs:764\tstep:500\tbatch_loss:737.6268\n",
            "Epochs:764\tstep:550\tbatch_loss:812.6352\n",
            "Epochs:764\tstep:600\tbatch_loss:747.0734\n",
            "Epoch loss: 494352.6322944394\n",
            "RMSE on dev data: 9.88679\n",
            "RMSE on train data: 10.00733\n",
            "Epochs:765\tstep:50\tbatch_loss:758.8764\n",
            "Epochs:765\tstep:100\tbatch_loss:793.5531\n",
            "Epochs:765\tstep:150\tbatch_loss:788.0517\n",
            "Epochs:765\tstep:200\tbatch_loss:779.8609\n",
            "Epochs:765\tstep:250\tbatch_loss:938.8252\n",
            "Epochs:765\tstep:300\tbatch_loss:809.6129\n",
            "Epochs:765\tstep:350\tbatch_loss:783.7064\n",
            "Epochs:765\tstep:400\tbatch_loss:785.3880\n",
            "Epochs:765\tstep:450\tbatch_loss:793.5371\n",
            "Epochs:765\tstep:500\tbatch_loss:737.5487\n",
            "Epochs:765\tstep:550\tbatch_loss:812.5564\n",
            "Epochs:765\tstep:600\tbatch_loss:746.9953\n",
            "Epoch loss: 494302.54031682864\n",
            "RMSE on dev data: 9.88677\n",
            "RMSE on train data: 10.00732\n",
            "Epochs:766\tstep:50\tbatch_loss:758.7970\n",
            "Epochs:766\tstep:100\tbatch_loss:793.4747\n",
            "Epochs:766\tstep:150\tbatch_loss:787.9739\n",
            "Epochs:766\tstep:200\tbatch_loss:779.7818\n",
            "Epochs:766\tstep:250\tbatch_loss:938.7449\n",
            "Epochs:766\tstep:300\tbatch_loss:809.5340\n",
            "Epochs:766\tstep:350\tbatch_loss:783.6279\n",
            "Epochs:766\tstep:400\tbatch_loss:785.3096\n",
            "Epochs:766\tstep:450\tbatch_loss:793.4573\n",
            "Epochs:766\tstep:500\tbatch_loss:737.4706\n",
            "Epochs:766\tstep:550\tbatch_loss:812.4777\n",
            "Epochs:766\tstep:600\tbatch_loss:746.9172\n",
            "Epoch loss: 494252.44373940246\n",
            "RMSE on dev data: 9.88675\n",
            "RMSE on train data: 10.00730\n",
            "Epochs:767\tstep:50\tbatch_loss:758.7177\n",
            "Epochs:767\tstep:100\tbatch_loss:793.3964\n",
            "Epochs:767\tstep:150\tbatch_loss:787.8965\n",
            "Epochs:767\tstep:200\tbatch_loss:779.7030\n",
            "Epochs:767\tstep:250\tbatch_loss:938.6645\n",
            "Epochs:767\tstep:300\tbatch_loss:809.4552\n",
            "Epochs:767\tstep:350\tbatch_loss:783.5495\n",
            "Epochs:767\tstep:400\tbatch_loss:785.2313\n",
            "Epochs:767\tstep:450\tbatch_loss:793.3777\n",
            "Epochs:767\tstep:500\tbatch_loss:737.3927\n",
            "Epochs:767\tstep:550\tbatch_loss:812.3990\n",
            "Epochs:767\tstep:600\tbatch_loss:746.8392\n",
            "Epoch loss: 494202.45970224787\n",
            "RMSE on dev data: 9.88672\n",
            "RMSE on train data: 10.00729\n",
            "Epochs:768\tstep:50\tbatch_loss:758.6384\n",
            "Epochs:768\tstep:100\tbatch_loss:793.3181\n",
            "Epochs:768\tstep:150\tbatch_loss:787.8189\n",
            "Epochs:768\tstep:200\tbatch_loss:779.6241\n",
            "Epochs:768\tstep:250\tbatch_loss:938.5842\n",
            "Epochs:768\tstep:300\tbatch_loss:809.3763\n",
            "Epochs:768\tstep:350\tbatch_loss:783.4712\n",
            "Epochs:768\tstep:400\tbatch_loss:785.1529\n",
            "Epochs:768\tstep:450\tbatch_loss:793.2981\n",
            "Epochs:768\tstep:500\tbatch_loss:737.3147\n",
            "Epochs:768\tstep:550\tbatch_loss:812.3204\n",
            "Epochs:768\tstep:600\tbatch_loss:746.7613\n",
            "Epoch loss: 494152.4572466742\n",
            "RMSE on dev data: 9.88671\n",
            "RMSE on train data: 10.00727\n",
            "Epochs:769\tstep:50\tbatch_loss:758.5592\n",
            "Epochs:769\tstep:100\tbatch_loss:793.2399\n",
            "Epochs:769\tstep:150\tbatch_loss:787.7416\n",
            "Epochs:769\tstep:200\tbatch_loss:779.5455\n",
            "Epochs:769\tstep:250\tbatch_loss:938.5042\n",
            "Epochs:769\tstep:300\tbatch_loss:809.2977\n",
            "Epochs:769\tstep:350\tbatch_loss:783.3930\n",
            "Epochs:769\tstep:400\tbatch_loss:785.0748\n",
            "Epochs:769\tstep:450\tbatch_loss:793.2186\n",
            "Epochs:769\tstep:500\tbatch_loss:737.2369\n",
            "Epochs:769\tstep:550\tbatch_loss:812.2419\n",
            "Epochs:769\tstep:600\tbatch_loss:746.6835\n",
            "Epoch loss: 494102.57207592763\n",
            "RMSE on dev data: 9.88669\n",
            "RMSE on train data: 10.00726\n",
            "Epochs:770\tstep:50\tbatch_loss:758.4802\n",
            "Epochs:770\tstep:100\tbatch_loss:793.1619\n",
            "Epochs:770\tstep:150\tbatch_loss:787.6641\n",
            "Epochs:770\tstep:200\tbatch_loss:779.4669\n",
            "Epochs:770\tstep:250\tbatch_loss:938.4242\n",
            "Epochs:770\tstep:300\tbatch_loss:809.2190\n",
            "Epochs:770\tstep:350\tbatch_loss:783.3148\n",
            "Epochs:770\tstep:400\tbatch_loss:784.9966\n",
            "Epochs:770\tstep:450\tbatch_loss:793.1392\n",
            "Epochs:770\tstep:500\tbatch_loss:737.1591\n",
            "Epochs:770\tstep:550\tbatch_loss:812.1635\n",
            "Epochs:770\tstep:600\tbatch_loss:746.6057\n",
            "Epoch loss: 494052.69548120705\n",
            "RMSE on dev data: 9.88667\n",
            "RMSE on train data: 10.00724\n",
            "Epochs:771\tstep:50\tbatch_loss:758.4011\n",
            "Epochs:771\tstep:100\tbatch_loss:793.0838\n",
            "Epochs:771\tstep:150\tbatch_loss:787.5868\n",
            "Epochs:771\tstep:200\tbatch_loss:779.3884\n",
            "Epochs:771\tstep:250\tbatch_loss:938.3441\n",
            "Epochs:771\tstep:300\tbatch_loss:809.1404\n",
            "Epochs:771\tstep:350\tbatch_loss:783.2367\n",
            "Epochs:771\tstep:400\tbatch_loss:784.9185\n",
            "Epochs:771\tstep:450\tbatch_loss:793.0598\n",
            "Epochs:771\tstep:500\tbatch_loss:737.0813\n",
            "Epochs:771\tstep:550\tbatch_loss:812.0850\n",
            "Epochs:771\tstep:600\tbatch_loss:746.5279\n",
            "Epoch loss: 494002.8424070152\n",
            "RMSE on dev data: 9.88665\n",
            "RMSE on train data: 10.00723\n",
            "Epochs:772\tstep:50\tbatch_loss:758.3221\n",
            "Epochs:772\tstep:100\tbatch_loss:793.0058\n",
            "Epochs:772\tstep:150\tbatch_loss:787.5098\n",
            "Epochs:772\tstep:200\tbatch_loss:779.3101\n",
            "Epochs:772\tstep:250\tbatch_loss:938.2644\n",
            "Epochs:772\tstep:300\tbatch_loss:809.0620\n",
            "Epochs:772\tstep:350\tbatch_loss:783.1586\n",
            "Epochs:772\tstep:400\tbatch_loss:784.8406\n",
            "Epochs:772\tstep:450\tbatch_loss:792.9806\n",
            "Epochs:772\tstep:500\tbatch_loss:737.0036\n",
            "Epochs:772\tstep:550\tbatch_loss:812.0067\n",
            "Epochs:772\tstep:600\tbatch_loss:746.4503\n",
            "Epoch loss: 493953.0887252468\n",
            "RMSE on dev data: 9.88663\n",
            "RMSE on train data: 10.00722\n",
            "Epochs:773\tstep:50\tbatch_loss:758.2432\n",
            "Epochs:773\tstep:100\tbatch_loss:792.9280\n",
            "Epochs:773\tstep:150\tbatch_loss:787.4325\n",
            "Epochs:773\tstep:200\tbatch_loss:779.2316\n",
            "Epochs:773\tstep:250\tbatch_loss:938.1846\n",
            "Epochs:773\tstep:300\tbatch_loss:808.9835\n",
            "Epochs:773\tstep:350\tbatch_loss:783.0806\n",
            "Epochs:773\tstep:400\tbatch_loss:784.7626\n",
            "Epochs:773\tstep:450\tbatch_loss:792.9013\n",
            "Epochs:773\tstep:500\tbatch_loss:736.9259\n",
            "Epochs:773\tstep:550\tbatch_loss:811.9284\n",
            "Epochs:773\tstep:600\tbatch_loss:746.3727\n",
            "Epoch loss: 493903.3082058099\n",
            "RMSE on dev data: 9.88662\n",
            "RMSE on train data: 10.00720\n",
            "Epochs:774\tstep:50\tbatch_loss:758.1644\n",
            "Epochs:774\tstep:100\tbatch_loss:792.8501\n",
            "Epochs:774\tstep:150\tbatch_loss:787.3555\n",
            "Epochs:774\tstep:200\tbatch_loss:779.1535\n",
            "Epochs:774\tstep:250\tbatch_loss:938.1048\n",
            "Epochs:774\tstep:300\tbatch_loss:808.9051\n",
            "Epochs:774\tstep:350\tbatch_loss:783.0026\n",
            "Epochs:774\tstep:400\tbatch_loss:784.6846\n",
            "Epochs:774\tstep:450\tbatch_loss:792.8222\n",
            "Epochs:774\tstep:500\tbatch_loss:736.8484\n",
            "Epochs:774\tstep:550\tbatch_loss:811.8502\n",
            "Epochs:774\tstep:600\tbatch_loss:746.2952\n",
            "Epoch loss: 493853.6264929829\n",
            "RMSE on dev data: 9.88660\n",
            "RMSE on train data: 10.00719\n",
            "Epochs:775\tstep:50\tbatch_loss:758.0856\n",
            "Epochs:775\tstep:100\tbatch_loss:792.7724\n",
            "Epochs:775\tstep:150\tbatch_loss:787.2783\n",
            "Epochs:775\tstep:200\tbatch_loss:779.0753\n",
            "Epochs:775\tstep:250\tbatch_loss:938.0253\n",
            "Epochs:775\tstep:300\tbatch_loss:808.8268\n",
            "Epochs:775\tstep:350\tbatch_loss:782.9247\n",
            "Epochs:775\tstep:400\tbatch_loss:784.6069\n",
            "Epochs:775\tstep:450\tbatch_loss:792.7431\n",
            "Epochs:775\tstep:500\tbatch_loss:736.7708\n",
            "Epochs:775\tstep:550\tbatch_loss:811.7721\n",
            "Epochs:775\tstep:600\tbatch_loss:746.2177\n",
            "Epoch loss: 493803.9387720466\n",
            "RMSE on dev data: 9.88658\n",
            "RMSE on train data: 10.00718\n",
            "Epochs:776\tstep:50\tbatch_loss:757.9461\n",
            "Epochs:776\tstep:100\tbatch_loss:792.7366\n",
            "Epochs:776\tstep:150\tbatch_loss:786.6239\n",
            "Epochs:776\tstep:200\tbatch_loss:778.8681\n",
            "Epochs:776\tstep:250\tbatch_loss:937.8226\n",
            "Epochs:776\tstep:300\tbatch_loss:808.7883\n",
            "Epochs:776\tstep:350\tbatch_loss:782.8122\n",
            "Epochs:776\tstep:400\tbatch_loss:784.5639\n",
            "Epochs:776\tstep:450\tbatch_loss:792.6748\n",
            "Epochs:776\tstep:500\tbatch_loss:736.8249\n",
            "Epochs:776\tstep:550\tbatch_loss:811.3108\n",
            "Epochs:776\tstep:600\tbatch_loss:746.3627\n",
            "Epoch loss: 493696.30726360716\n",
            "RMSE on dev data: 9.88583\n",
            "RMSE on train data: 10.00761\n",
            "Epochs:777\tstep:50\tbatch_loss:757.8114\n",
            "Epochs:777\tstep:100\tbatch_loss:792.6781\n",
            "Epochs:777\tstep:150\tbatch_loss:786.5387\n",
            "Epochs:777\tstep:200\tbatch_loss:778.7855\n",
            "Epochs:777\tstep:250\tbatch_loss:937.7553\n",
            "Epochs:777\tstep:300\tbatch_loss:808.7163\n",
            "Epochs:777\tstep:350\tbatch_loss:782.7419\n",
            "Epochs:777\tstep:400\tbatch_loss:784.4926\n",
            "Epochs:777\tstep:450\tbatch_loss:792.6020\n",
            "Epochs:777\tstep:500\tbatch_loss:736.7542\n",
            "Epochs:777\tstep:550\tbatch_loss:811.2396\n",
            "Epochs:777\tstep:600\tbatch_loss:746.2917\n",
            "Epoch loss: 493647.5947280451\n",
            "RMSE on dev data: 9.88579\n",
            "RMSE on train data: 10.00759\n",
            "Epochs:778\tstep:50\tbatch_loss:757.7393\n",
            "Epochs:778\tstep:100\tbatch_loss:792.6068\n",
            "Epochs:778\tstep:150\tbatch_loss:786.4682\n",
            "Epochs:778\tstep:200\tbatch_loss:778.7128\n",
            "Epochs:778\tstep:250\tbatch_loss:937.6818\n",
            "Epochs:778\tstep:300\tbatch_loss:808.6447\n",
            "Epochs:778\tstep:350\tbatch_loss:782.6711\n",
            "Epochs:778\tstep:400\tbatch_loss:784.4216\n",
            "Epochs:778\tstep:450\tbatch_loss:792.5294\n",
            "Epochs:778\tstep:500\tbatch_loss:736.6838\n",
            "Epochs:778\tstep:550\tbatch_loss:811.1682\n",
            "Epochs:778\tstep:600\tbatch_loss:746.2210\n",
            "Epoch loss: 493602.12745877873\n",
            "RMSE on dev data: 9.88576\n",
            "RMSE on train data: 10.00756\n",
            "Epochs:779\tstep:50\tbatch_loss:757.6673\n",
            "Epochs:779\tstep:100\tbatch_loss:792.5355\n",
            "Epochs:779\tstep:150\tbatch_loss:786.3982\n",
            "Epochs:779\tstep:200\tbatch_loss:778.6405\n",
            "Epochs:779\tstep:250\tbatch_loss:937.6083\n",
            "Epochs:779\tstep:300\tbatch_loss:808.5732\n",
            "Epochs:779\tstep:350\tbatch_loss:782.6003\n",
            "Epochs:779\tstep:400\tbatch_loss:784.3507\n",
            "Epochs:779\tstep:450\tbatch_loss:792.4570\n",
            "Epochs:779\tstep:500\tbatch_loss:736.6133\n",
            "Epochs:779\tstep:550\tbatch_loss:811.0970\n",
            "Epochs:779\tstep:600\tbatch_loss:746.1503\n",
            "Epoch loss: 493556.75445922546\n",
            "RMSE on dev data: 9.88573\n",
            "RMSE on train data: 10.00754\n",
            "Epochs:780\tstep:50\tbatch_loss:757.5956\n",
            "Epochs:780\tstep:100\tbatch_loss:792.4645\n",
            "Epochs:780\tstep:150\tbatch_loss:786.3280\n",
            "Epochs:780\tstep:200\tbatch_loss:778.5683\n",
            "Epochs:780\tstep:250\tbatch_loss:937.5352\n",
            "Epochs:780\tstep:300\tbatch_loss:808.5019\n",
            "Epochs:780\tstep:350\tbatch_loss:782.5296\n",
            "Epochs:780\tstep:400\tbatch_loss:784.2800\n",
            "Epochs:780\tstep:450\tbatch_loss:792.3847\n",
            "Epochs:780\tstep:500\tbatch_loss:736.5431\n",
            "Epochs:780\tstep:550\tbatch_loss:811.0259\n",
            "Epochs:780\tstep:600\tbatch_loss:746.0798\n",
            "Epoch loss: 493511.4666279367\n",
            "RMSE on dev data: 9.88570\n",
            "RMSE on train data: 10.00751\n",
            "Epochs:781\tstep:50\tbatch_loss:757.5238\n",
            "Epochs:781\tstep:100\tbatch_loss:792.3935\n",
            "Epochs:781\tstep:150\tbatch_loss:786.2582\n",
            "Epochs:781\tstep:200\tbatch_loss:778.4963\n",
            "Epochs:781\tstep:250\tbatch_loss:937.4622\n",
            "Epochs:781\tstep:300\tbatch_loss:808.4307\n",
            "Epochs:781\tstep:350\tbatch_loss:782.4590\n",
            "Epochs:781\tstep:400\tbatch_loss:784.2093\n",
            "Epochs:781\tstep:450\tbatch_loss:792.3126\n",
            "Epochs:781\tstep:500\tbatch_loss:736.4728\n",
            "Epochs:781\tstep:550\tbatch_loss:810.9549\n",
            "Epochs:781\tstep:600\tbatch_loss:746.0094\n",
            "Epoch loss: 493466.2584973597\n",
            "RMSE on dev data: 9.88568\n",
            "RMSE on train data: 10.00749\n",
            "Epochs:782\tstep:50\tbatch_loss:757.4523\n",
            "Epochs:782\tstep:100\tbatch_loss:792.3228\n",
            "Epochs:782\tstep:150\tbatch_loss:786.1882\n",
            "Epochs:782\tstep:200\tbatch_loss:778.4245\n",
            "Epochs:782\tstep:250\tbatch_loss:937.3893\n",
            "Epochs:782\tstep:300\tbatch_loss:808.3596\n",
            "Epochs:782\tstep:350\tbatch_loss:782.3885\n",
            "Epochs:782\tstep:400\tbatch_loss:784.1387\n",
            "Epochs:782\tstep:450\tbatch_loss:792.2406\n",
            "Epochs:782\tstep:500\tbatch_loss:736.4027\n",
            "Epochs:782\tstep:550\tbatch_loss:810.8840\n",
            "Epochs:782\tstep:600\tbatch_loss:745.9391\n",
            "Epoch loss: 493421.12466979853\n",
            "RMSE on dev data: 9.88565\n",
            "RMSE on train data: 10.00747\n",
            "Epochs:783\tstep:50\tbatch_loss:757.3807\n",
            "Epochs:783\tstep:100\tbatch_loss:792.2520\n",
            "Epochs:783\tstep:150\tbatch_loss:786.1184\n",
            "Epochs:783\tstep:200\tbatch_loss:778.3528\n",
            "Epochs:783\tstep:250\tbatch_loss:937.3166\n",
            "Epochs:783\tstep:300\tbatch_loss:808.2885\n",
            "Epochs:783\tstep:350\tbatch_loss:782.3181\n",
            "Epochs:783\tstep:400\tbatch_loss:784.0683\n",
            "Epochs:783\tstep:450\tbatch_loss:792.1687\n",
            "Epochs:783\tstep:500\tbatch_loss:736.3327\n",
            "Epochs:783\tstep:550\tbatch_loss:810.8131\n",
            "Epochs:783\tstep:600\tbatch_loss:745.8689\n",
            "Epoch loss: 493376.0489116265\n",
            "RMSE on dev data: 9.88562\n",
            "RMSE on train data: 10.00745\n",
            "Epochs:784\tstep:50\tbatch_loss:757.3094\n",
            "Epochs:784\tstep:100\tbatch_loss:792.1815\n",
            "Epochs:784\tstep:150\tbatch_loss:786.0489\n",
            "Epochs:784\tstep:200\tbatch_loss:778.2815\n",
            "Epochs:784\tstep:250\tbatch_loss:937.2440\n",
            "Epochs:784\tstep:300\tbatch_loss:808.2176\n",
            "Epochs:784\tstep:350\tbatch_loss:782.2478\n",
            "Epochs:784\tstep:400\tbatch_loss:783.9979\n",
            "Epochs:784\tstep:450\tbatch_loss:792.0970\n",
            "Epochs:784\tstep:500\tbatch_loss:736.2627\n",
            "Epochs:784\tstep:550\tbatch_loss:810.7424\n",
            "Epochs:784\tstep:600\tbatch_loss:745.7988\n",
            "Epoch loss: 493331.08334044577\n",
            "RMSE on dev data: 9.88560\n",
            "RMSE on train data: 10.00743\n",
            "Epochs:785\tstep:50\tbatch_loss:757.2381\n",
            "Epochs:785\tstep:100\tbatch_loss:792.1111\n",
            "Epochs:785\tstep:150\tbatch_loss:785.9792\n",
            "Epochs:785\tstep:200\tbatch_loss:778.2100\n",
            "Epochs:785\tstep:250\tbatch_loss:937.1715\n",
            "Epochs:785\tstep:300\tbatch_loss:808.1467\n",
            "Epochs:785\tstep:350\tbatch_loss:782.1775\n",
            "Epochs:785\tstep:400\tbatch_loss:783.9276\n",
            "Epochs:785\tstep:450\tbatch_loss:792.0253\n",
            "Epochs:785\tstep:500\tbatch_loss:736.1928\n",
            "Epochs:785\tstep:550\tbatch_loss:810.6717\n",
            "Epochs:785\tstep:600\tbatch_loss:745.7288\n",
            "Epoch loss: 493286.1196295442\n",
            "RMSE on dev data: 9.88557\n",
            "RMSE on train data: 10.00741\n",
            "Epochs:786\tstep:50\tbatch_loss:757.1669\n",
            "Epochs:786\tstep:100\tbatch_loss:792.0408\n",
            "Epochs:786\tstep:150\tbatch_loss:785.9098\n",
            "Epochs:786\tstep:200\tbatch_loss:778.1389\n",
            "Epochs:786\tstep:250\tbatch_loss:937.0993\n",
            "Epochs:786\tstep:300\tbatch_loss:808.0761\n",
            "Epochs:786\tstep:350\tbatch_loss:782.1073\n",
            "Epochs:786\tstep:400\tbatch_loss:783.8575\n",
            "Epochs:786\tstep:450\tbatch_loss:791.9538\n",
            "Epochs:786\tstep:500\tbatch_loss:736.1230\n",
            "Epochs:786\tstep:550\tbatch_loss:810.6012\n",
            "Epochs:786\tstep:600\tbatch_loss:745.6589\n",
            "Epoch loss: 493241.2956550332\n",
            "RMSE on dev data: 9.88555\n",
            "RMSE on train data: 10.00739\n",
            "Epochs:787\tstep:50\tbatch_loss:757.0958\n",
            "Epochs:787\tstep:100\tbatch_loss:791.9706\n",
            "Epochs:787\tstep:150\tbatch_loss:785.8403\n",
            "Epochs:787\tstep:200\tbatch_loss:778.0677\n",
            "Epochs:787\tstep:250\tbatch_loss:937.0270\n",
            "Epochs:787\tstep:300\tbatch_loss:808.0054\n",
            "Epochs:787\tstep:350\tbatch_loss:782.0372\n",
            "Epochs:787\tstep:400\tbatch_loss:783.7874\n",
            "Epochs:787\tstep:450\tbatch_loss:791.8823\n",
            "Epochs:787\tstep:500\tbatch_loss:736.0532\n",
            "Epochs:787\tstep:550\tbatch_loss:810.5307\n",
            "Epochs:787\tstep:600\tbatch_loss:745.5890\n",
            "Epoch loss: 493196.4569621103\n",
            "RMSE on dev data: 9.88552\n",
            "RMSE on train data: 10.00738\n",
            "Epochs:788\tstep:50\tbatch_loss:757.0248\n",
            "Epochs:788\tstep:100\tbatch_loss:791.9004\n",
            "Epochs:788\tstep:150\tbatch_loss:785.7711\n",
            "Epochs:788\tstep:200\tbatch_loss:777.9969\n",
            "Epochs:788\tstep:250\tbatch_loss:936.9549\n",
            "Epochs:788\tstep:300\tbatch_loss:807.9348\n",
            "Epochs:788\tstep:350\tbatch_loss:781.9672\n",
            "Epochs:788\tstep:400\tbatch_loss:783.7173\n",
            "Epochs:788\tstep:450\tbatch_loss:791.8110\n",
            "Epochs:788\tstep:500\tbatch_loss:735.9835\n",
            "Epochs:788\tstep:550\tbatch_loss:810.4604\n",
            "Epochs:788\tstep:600\tbatch_loss:745.5193\n",
            "Epoch loss: 493151.7281324681\n",
            "RMSE on dev data: 9.88550\n",
            "RMSE on train data: 10.00736\n",
            "Epochs:789\tstep:50\tbatch_loss:756.9538\n",
            "Epochs:789\tstep:100\tbatch_loss:791.8304\n",
            "Epochs:789\tstep:150\tbatch_loss:785.7017\n",
            "Epochs:789\tstep:200\tbatch_loss:777.9260\n",
            "Epochs:789\tstep:250\tbatch_loss:936.8831\n",
            "Epochs:789\tstep:300\tbatch_loss:807.8643\n",
            "Epochs:789\tstep:350\tbatch_loss:781.8971\n",
            "Epochs:789\tstep:400\tbatch_loss:783.6474\n",
            "Epochs:789\tstep:450\tbatch_loss:791.7398\n",
            "Epochs:789\tstep:500\tbatch_loss:735.9139\n",
            "Epochs:789\tstep:550\tbatch_loss:810.3900\n",
            "Epochs:789\tstep:600\tbatch_loss:745.4496\n",
            "Epoch loss: 493107.0107219152\n",
            "RMSE on dev data: 9.88548\n",
            "RMSE on train data: 10.00734\n",
            "Epochs:790\tstep:50\tbatch_loss:756.8830\n",
            "Epochs:790\tstep:100\tbatch_loss:791.7605\n",
            "Epochs:790\tstep:150\tbatch_loss:785.6325\n",
            "Epochs:790\tstep:200\tbatch_loss:777.8553\n",
            "Epochs:790\tstep:250\tbatch_loss:936.8111\n",
            "Epochs:790\tstep:300\tbatch_loss:807.7939\n",
            "Epochs:790\tstep:350\tbatch_loss:781.8272\n",
            "Epochs:790\tstep:400\tbatch_loss:783.5775\n",
            "Epochs:790\tstep:450\tbatch_loss:791.6686\n",
            "Epochs:790\tstep:500\tbatch_loss:735.8444\n",
            "Epochs:790\tstep:550\tbatch_loss:810.3197\n",
            "Epochs:790\tstep:600\tbatch_loss:745.3800\n",
            "Epoch loss: 493062.3657980448\n",
            "RMSE on dev data: 9.88546\n",
            "RMSE on train data: 10.00733\n",
            "Epochs:791\tstep:50\tbatch_loss:756.8122\n",
            "Epochs:791\tstep:100\tbatch_loss:791.6906\n",
            "Epochs:791\tstep:150\tbatch_loss:785.5636\n",
            "Epochs:791\tstep:200\tbatch_loss:777.7848\n",
            "Epochs:791\tstep:250\tbatch_loss:936.7393\n",
            "Epochs:791\tstep:300\tbatch_loss:807.7236\n",
            "Epochs:791\tstep:350\tbatch_loss:781.7574\n",
            "Epochs:791\tstep:400\tbatch_loss:783.5077\n",
            "Epochs:791\tstep:450\tbatch_loss:791.5976\n",
            "Epochs:791\tstep:500\tbatch_loss:735.7749\n",
            "Epochs:791\tstep:550\tbatch_loss:810.2496\n",
            "Epochs:791\tstep:600\tbatch_loss:745.3105\n",
            "Epoch loss: 493017.796721243\n",
            "RMSE on dev data: 9.88544\n",
            "RMSE on train data: 10.00731\n",
            "Epochs:792\tstep:50\tbatch_loss:756.7415\n",
            "Epochs:792\tstep:100\tbatch_loss:791.6209\n",
            "Epochs:792\tstep:150\tbatch_loss:785.4944\n",
            "Epochs:792\tstep:200\tbatch_loss:777.7143\n",
            "Epochs:792\tstep:250\tbatch_loss:936.6678\n",
            "Epochs:792\tstep:300\tbatch_loss:807.6533\n",
            "Epochs:792\tstep:350\tbatch_loss:781.6876\n",
            "Epochs:792\tstep:400\tbatch_loss:783.4380\n",
            "Epochs:792\tstep:450\tbatch_loss:791.5266\n",
            "Epochs:792\tstep:500\tbatch_loss:735.7054\n",
            "Epochs:792\tstep:550\tbatch_loss:810.1795\n",
            "Epochs:792\tstep:600\tbatch_loss:745.2410\n",
            "Epoch loss: 492973.2260423717\n",
            "RMSE on dev data: 9.88542\n",
            "RMSE on train data: 10.00730\n",
            "Epochs:793\tstep:50\tbatch_loss:756.6709\n",
            "Epochs:793\tstep:100\tbatch_loss:791.5511\n",
            "Epochs:793\tstep:150\tbatch_loss:785.4255\n",
            "Epochs:793\tstep:200\tbatch_loss:777.6440\n",
            "Epochs:793\tstep:250\tbatch_loss:936.5961\n",
            "Epochs:793\tstep:300\tbatch_loss:807.5831\n",
            "Epochs:793\tstep:350\tbatch_loss:781.6178\n",
            "Epochs:793\tstep:400\tbatch_loss:783.3683\n",
            "Epochs:793\tstep:450\tbatch_loss:791.4557\n",
            "Epochs:793\tstep:500\tbatch_loss:735.6361\n",
            "Epochs:793\tstep:550\tbatch_loss:810.1094\n",
            "Epochs:793\tstep:600\tbatch_loss:745.1716\n",
            "Epoch loss: 492928.7406158484\n",
            "RMSE on dev data: 9.88539\n",
            "RMSE on train data: 10.00728\n",
            "Epochs:794\tstep:50\tbatch_loss:756.6003\n",
            "Epochs:794\tstep:100\tbatch_loss:791.4815\n",
            "Epochs:794\tstep:150\tbatch_loss:785.3565\n",
            "Epochs:794\tstep:200\tbatch_loss:777.5736\n",
            "Epochs:794\tstep:250\tbatch_loss:936.5246\n",
            "Epochs:794\tstep:300\tbatch_loss:807.5130\n",
            "Epochs:794\tstep:350\tbatch_loss:781.5481\n",
            "Epochs:794\tstep:400\tbatch_loss:783.2986\n",
            "Epochs:794\tstep:450\tbatch_loss:791.3848\n",
            "Epochs:794\tstep:500\tbatch_loss:735.5667\n",
            "Epochs:794\tstep:550\tbatch_loss:810.0394\n",
            "Epochs:794\tstep:600\tbatch_loss:745.1023\n",
            "Epoch loss: 492884.24742821633\n",
            "RMSE on dev data: 9.88538\n",
            "RMSE on train data: 10.00727\n",
            "Epochs:795\tstep:50\tbatch_loss:756.5298\n",
            "Epochs:795\tstep:100\tbatch_loss:791.4120\n",
            "Epochs:795\tstep:150\tbatch_loss:785.2878\n",
            "Epochs:795\tstep:200\tbatch_loss:777.5036\n",
            "Epochs:795\tstep:250\tbatch_loss:936.4533\n",
            "Epochs:795\tstep:300\tbatch_loss:807.4429\n",
            "Epochs:795\tstep:350\tbatch_loss:781.4785\n",
            "Epochs:795\tstep:400\tbatch_loss:783.2291\n",
            "Epochs:795\tstep:450\tbatch_loss:791.3142\n",
            "Epochs:795\tstep:500\tbatch_loss:735.4975\n",
            "Epochs:795\tstep:550\tbatch_loss:809.9696\n",
            "Epochs:795\tstep:600\tbatch_loss:745.0330\n",
            "Epoch loss: 492839.8588599553\n",
            "RMSE on dev data: 9.88536\n",
            "RMSE on train data: 10.00725\n",
            "Epochs:796\tstep:50\tbatch_loss:756.4593\n",
            "Epochs:796\tstep:100\tbatch_loss:791.3425\n",
            "Epochs:796\tstep:150\tbatch_loss:785.2188\n",
            "Epochs:796\tstep:200\tbatch_loss:777.4333\n",
            "Epochs:796\tstep:250\tbatch_loss:936.3820\n",
            "Epochs:796\tstep:300\tbatch_loss:807.3729\n",
            "Epochs:796\tstep:350\tbatch_loss:781.4089\n",
            "Epochs:796\tstep:400\tbatch_loss:783.1595\n",
            "Epochs:796\tstep:450\tbatch_loss:791.2434\n",
            "Epochs:796\tstep:500\tbatch_loss:735.4282\n",
            "Epochs:796\tstep:550\tbatch_loss:809.8997\n",
            "Epochs:796\tstep:600\tbatch_loss:744.9638\n",
            "Epoch loss: 492795.4484454617\n",
            "RMSE on dev data: 9.88534\n",
            "RMSE on train data: 10.00724\n",
            "Epochs:797\tstep:50\tbatch_loss:756.3890\n",
            "Epochs:797\tstep:100\tbatch_loss:791.2731\n",
            "Epochs:797\tstep:150\tbatch_loss:785.1501\n",
            "Epochs:797\tstep:200\tbatch_loss:777.3633\n",
            "Epochs:797\tstep:250\tbatch_loss:936.3107\n",
            "Epochs:797\tstep:300\tbatch_loss:807.3029\n",
            "Epochs:797\tstep:350\tbatch_loss:781.3394\n",
            "Epochs:797\tstep:400\tbatch_loss:783.0901\n",
            "Epochs:797\tstep:450\tbatch_loss:791.1728\n",
            "Epochs:797\tstep:500\tbatch_loss:735.3590\n",
            "Epochs:797\tstep:550\tbatch_loss:809.8298\n",
            "Epochs:797\tstep:600\tbatch_loss:744.8946\n",
            "Epoch loss: 492751.1147818492\n",
            "RMSE on dev data: 9.88532\n",
            "RMSE on train data: 10.00722\n",
            "Epochs:798\tstep:50\tbatch_loss:756.3186\n",
            "Epochs:798\tstep:100\tbatch_loss:791.2037\n",
            "Epochs:798\tstep:150\tbatch_loss:785.0815\n",
            "Epochs:798\tstep:200\tbatch_loss:777.2935\n",
            "Epochs:798\tstep:250\tbatch_loss:936.2396\n",
            "Epochs:798\tstep:300\tbatch_loss:807.2331\n",
            "Epochs:798\tstep:350\tbatch_loss:781.2699\n",
            "Epochs:798\tstep:400\tbatch_loss:783.0207\n",
            "Epochs:798\tstep:450\tbatch_loss:791.1022\n",
            "Epochs:798\tstep:500\tbatch_loss:735.2899\n",
            "Epochs:798\tstep:550\tbatch_loss:809.7601\n",
            "Epochs:798\tstep:600\tbatch_loss:744.8255\n",
            "Epoch loss: 492706.8161979635\n",
            "RMSE on dev data: 9.88530\n",
            "RMSE on train data: 10.00721\n",
            "Epochs:799\tstep:50\tbatch_loss:756.2484\n",
            "Epochs:799\tstep:100\tbatch_loss:791.1345\n",
            "Epochs:799\tstep:150\tbatch_loss:785.0128\n",
            "Epochs:799\tstep:200\tbatch_loss:777.2236\n",
            "Epochs:799\tstep:250\tbatch_loss:936.1685\n",
            "Epochs:799\tstep:300\tbatch_loss:807.1632\n",
            "Epochs:799\tstep:350\tbatch_loss:781.2005\n",
            "Epochs:799\tstep:400\tbatch_loss:782.9514\n",
            "Epochs:799\tstep:450\tbatch_loss:791.0317\n",
            "Epochs:799\tstep:500\tbatch_loss:735.2209\n",
            "Epochs:799\tstep:550\tbatch_loss:809.6904\n",
            "Epochs:799\tstep:600\tbatch_loss:744.7565\n",
            "Epoch loss: 492662.5458098033\n",
            "RMSE on dev data: 9.88528\n",
            "RMSE on train data: 10.00720\n",
            "Epochs:800\tstep:50\tbatch_loss:756.1782\n",
            "Epochs:800\tstep:100\tbatch_loss:791.0652\n",
            "Epochs:800\tstep:150\tbatch_loss:784.9443\n",
            "Epochs:800\tstep:200\tbatch_loss:777.1538\n",
            "Epochs:800\tstep:250\tbatch_loss:936.0974\n",
            "Epochs:800\tstep:300\tbatch_loss:807.0934\n",
            "Epochs:800\tstep:350\tbatch_loss:781.1311\n",
            "Epochs:800\tstep:400\tbatch_loss:782.8820\n",
            "Epochs:800\tstep:450\tbatch_loss:790.9613\n",
            "Epochs:800\tstep:500\tbatch_loss:735.1518\n",
            "Epochs:800\tstep:550\tbatch_loss:809.6208\n",
            "Epochs:800\tstep:600\tbatch_loss:744.6875\n",
            "Epoch loss: 492618.3075350239\n",
            "RMSE on dev data: 9.88527\n",
            "RMSE on train data: 10.00719\n",
            "Epochs:801\tstep:50\tbatch_loss:756.0417\n",
            "Epochs:801\tstep:100\tbatch_loss:791.0444\n",
            "Epochs:801\tstep:150\tbatch_loss:784.3730\n",
            "Epochs:801\tstep:200\tbatch_loss:776.9174\n",
            "Epochs:801\tstep:250\tbatch_loss:935.9522\n",
            "Epochs:801\tstep:300\tbatch_loss:807.0946\n",
            "Epochs:801\tstep:350\tbatch_loss:781.0089\n",
            "Epochs:801\tstep:400\tbatch_loss:782.8551\n",
            "Epochs:801\tstep:450\tbatch_loss:790.9107\n",
            "Epochs:801\tstep:500\tbatch_loss:735.2262\n",
            "Epochs:801\tstep:550\tbatch_loss:809.1336\n",
            "Epochs:801\tstep:600\tbatch_loss:744.8399\n",
            "Epoch loss: 492518.5185859788\n",
            "RMSE on dev data: 9.88472\n",
            "RMSE on train data: 10.00768\n",
            "Epochs:802\tstep:50\tbatch_loss:755.9202\n",
            "Epochs:802\tstep:100\tbatch_loss:790.9945\n",
            "Epochs:802\tstep:150\tbatch_loss:784.2961\n",
            "Epochs:802\tstep:200\tbatch_loss:776.8410\n",
            "Epochs:802\tstep:250\tbatch_loss:935.8942\n",
            "Epochs:802\tstep:300\tbatch_loss:807.0298\n",
            "Epochs:802\tstep:350\tbatch_loss:780.9466\n",
            "Epochs:802\tstep:400\tbatch_loss:782.7912\n",
            "Epochs:802\tstep:450\tbatch_loss:790.8456\n",
            "Epochs:802\tstep:500\tbatch_loss:735.1630\n",
            "Epochs:802\tstep:550\tbatch_loss:809.0701\n",
            "Epochs:802\tstep:600\tbatch_loss:744.7765\n",
            "Epoch loss: 492474.92791273835\n",
            "RMSE on dev data: 9.88469\n",
            "RMSE on train data: 10.00765\n",
            "Epochs:803\tstep:50\tbatch_loss:755.8559\n",
            "Epochs:803\tstep:100\tbatch_loss:790.9308\n",
            "Epochs:803\tstep:150\tbatch_loss:784.2335\n",
            "Epochs:803\tstep:200\tbatch_loss:776.7760\n",
            "Epochs:803\tstep:250\tbatch_loss:935.8284\n",
            "Epochs:803\tstep:300\tbatch_loss:806.9659\n",
            "Epochs:803\tstep:350\tbatch_loss:780.8834\n",
            "Epochs:803\tstep:400\tbatch_loss:782.7279\n",
            "Epochs:803\tstep:450\tbatch_loss:790.7808\n",
            "Epochs:803\tstep:500\tbatch_loss:735.1002\n",
            "Epochs:803\tstep:550\tbatch_loss:809.0063\n",
            "Epochs:803\tstep:600\tbatch_loss:744.7135\n",
            "Epoch loss: 492434.3807597903\n",
            "RMSE on dev data: 9.88465\n",
            "RMSE on train data: 10.00763\n",
            "Epochs:804\tstep:50\tbatch_loss:755.7916\n",
            "Epochs:804\tstep:100\tbatch_loss:790.8672\n",
            "Epochs:804\tstep:150\tbatch_loss:784.1707\n",
            "Epochs:804\tstep:200\tbatch_loss:776.7111\n",
            "Epochs:804\tstep:250\tbatch_loss:935.7629\n",
            "Epochs:804\tstep:300\tbatch_loss:806.9020\n",
            "Epochs:804\tstep:350\tbatch_loss:780.8202\n",
            "Epochs:804\tstep:400\tbatch_loss:782.6647\n",
            "Epochs:804\tstep:450\tbatch_loss:790.7161\n",
            "Epochs:804\tstep:500\tbatch_loss:735.0374\n",
            "Epochs:804\tstep:550\tbatch_loss:808.9427\n",
            "Epochs:804\tstep:600\tbatch_loss:744.6504\n",
            "Epoch loss: 492393.875507133\n",
            "RMSE on dev data: 9.88462\n",
            "RMSE on train data: 10.00760\n",
            "Epochs:805\tstep:50\tbatch_loss:755.7275\n",
            "Epochs:805\tstep:100\tbatch_loss:790.8038\n",
            "Epochs:805\tstep:150\tbatch_loss:784.1084\n",
            "Epochs:805\tstep:200\tbatch_loss:776.6465\n",
            "Epochs:805\tstep:250\tbatch_loss:935.6974\n",
            "Epochs:805\tstep:300\tbatch_loss:806.8383\n",
            "Epochs:805\tstep:350\tbatch_loss:780.7572\n",
            "Epochs:805\tstep:400\tbatch_loss:782.6015\n",
            "Epochs:805\tstep:450\tbatch_loss:790.6516\n",
            "Epochs:805\tstep:500\tbatch_loss:734.9747\n",
            "Epochs:805\tstep:550\tbatch_loss:808.8792\n",
            "Epochs:805\tstep:600\tbatch_loss:744.5875\n",
            "Epoch loss: 492353.4823936032\n",
            "RMSE on dev data: 9.88459\n",
            "RMSE on train data: 10.00758\n",
            "Epochs:806\tstep:50\tbatch_loss:755.6635\n",
            "Epochs:806\tstep:100\tbatch_loss:790.7405\n",
            "Epochs:806\tstep:150\tbatch_loss:784.0459\n",
            "Epochs:806\tstep:200\tbatch_loss:776.5818\n",
            "Epochs:806\tstep:250\tbatch_loss:935.6321\n",
            "Epochs:806\tstep:300\tbatch_loss:806.7746\n",
            "Epochs:806\tstep:350\tbatch_loss:780.6942\n",
            "Epochs:806\tstep:400\tbatch_loss:782.5385\n",
            "Epochs:806\tstep:450\tbatch_loss:790.5872\n",
            "Epochs:806\tstep:500\tbatch_loss:734.9120\n",
            "Epochs:806\tstep:550\tbatch_loss:808.8157\n",
            "Epochs:806\tstep:600\tbatch_loss:744.5247\n",
            "Epoch loss: 492313.1199779224\n",
            "RMSE on dev data: 9.88456\n",
            "RMSE on train data: 10.00756\n",
            "Epochs:807\tstep:50\tbatch_loss:755.5996\n",
            "Epochs:807\tstep:100\tbatch_loss:790.6773\n",
            "Epochs:807\tstep:150\tbatch_loss:783.9837\n",
            "Epochs:807\tstep:200\tbatch_loss:776.5176\n",
            "Epochs:807\tstep:250\tbatch_loss:935.5671\n",
            "Epochs:807\tstep:300\tbatch_loss:806.7112\n",
            "Epochs:807\tstep:350\tbatch_loss:780.6314\n",
            "Epochs:807\tstep:400\tbatch_loss:782.4756\n",
            "Epochs:807\tstep:450\tbatch_loss:790.5230\n",
            "Epochs:807\tstep:500\tbatch_loss:734.8495\n",
            "Epochs:807\tstep:550\tbatch_loss:808.7524\n",
            "Epochs:807\tstep:600\tbatch_loss:744.4620\n",
            "Epoch loss: 492272.8814899394\n",
            "RMSE on dev data: 9.88453\n",
            "RMSE on train data: 10.00754\n",
            "Epochs:808\tstep:50\tbatch_loss:755.5358\n",
            "Epochs:808\tstep:100\tbatch_loss:790.6143\n",
            "Epochs:808\tstep:150\tbatch_loss:783.9213\n",
            "Epochs:808\tstep:200\tbatch_loss:776.4533\n",
            "Epochs:808\tstep:250\tbatch_loss:935.5020\n",
            "Epochs:808\tstep:300\tbatch_loss:806.6477\n",
            "Epochs:808\tstep:350\tbatch_loss:780.5685\n",
            "Epochs:808\tstep:400\tbatch_loss:782.4127\n",
            "Epochs:808\tstep:450\tbatch_loss:790.4587\n",
            "Epochs:808\tstep:500\tbatch_loss:734.7870\n",
            "Epochs:808\tstep:550\tbatch_loss:808.6891\n",
            "Epochs:808\tstep:600\tbatch_loss:744.3994\n",
            "Epoch loss: 492232.6432871575\n",
            "RMSE on dev data: 9.88451\n",
            "RMSE on train data: 10.00751\n",
            "Epochs:809\tstep:50\tbatch_loss:755.4721\n",
            "Epochs:809\tstep:100\tbatch_loss:790.5513\n",
            "Epochs:809\tstep:150\tbatch_loss:783.8592\n",
            "Epochs:809\tstep:200\tbatch_loss:776.3892\n",
            "Epochs:809\tstep:250\tbatch_loss:935.4371\n",
            "Epochs:809\tstep:300\tbatch_loss:806.5843\n",
            "Epochs:809\tstep:350\tbatch_loss:780.5058\n",
            "Epochs:809\tstep:400\tbatch_loss:782.3499\n",
            "Epochs:809\tstep:450\tbatch_loss:790.3946\n",
            "Epochs:809\tstep:500\tbatch_loss:734.7246\n",
            "Epochs:809\tstep:550\tbatch_loss:808.6259\n",
            "Epochs:809\tstep:600\tbatch_loss:744.3368\n",
            "Epoch loss: 492192.5024763607\n",
            "RMSE on dev data: 9.88448\n",
            "RMSE on train data: 10.00749\n",
            "Epochs:810\tstep:50\tbatch_loss:755.4084\n",
            "Epochs:810\tstep:100\tbatch_loss:790.4884\n",
            "Epochs:810\tstep:150\tbatch_loss:783.7972\n",
            "Epochs:810\tstep:200\tbatch_loss:776.3254\n",
            "Epochs:810\tstep:250\tbatch_loss:935.3724\n",
            "Epochs:810\tstep:300\tbatch_loss:806.5211\n",
            "Epochs:810\tstep:350\tbatch_loss:780.4431\n",
            "Epochs:810\tstep:400\tbatch_loss:782.2873\n",
            "Epochs:810\tstep:450\tbatch_loss:790.3307\n",
            "Epochs:810\tstep:500\tbatch_loss:734.6623\n",
            "Epochs:810\tstep:550\tbatch_loss:808.5629\n",
            "Epochs:810\tstep:600\tbatch_loss:744.2744\n",
            "Epoch loss: 492152.4226776947\n",
            "RMSE on dev data: 9.88445\n",
            "RMSE on train data: 10.00747\n",
            "Epochs:811\tstep:50\tbatch_loss:755.3449\n",
            "Epochs:811\tstep:100\tbatch_loss:790.4257\n",
            "Epochs:811\tstep:150\tbatch_loss:783.7352\n",
            "Epochs:811\tstep:200\tbatch_loss:776.2616\n",
            "Epochs:811\tstep:250\tbatch_loss:935.3078\n",
            "Epochs:811\tstep:300\tbatch_loss:806.4579\n",
            "Epochs:811\tstep:350\tbatch_loss:780.3805\n",
            "Epochs:811\tstep:400\tbatch_loss:782.2246\n",
            "Epochs:811\tstep:450\tbatch_loss:790.2668\n",
            "Epochs:811\tstep:500\tbatch_loss:734.6001\n",
            "Epochs:811\tstep:550\tbatch_loss:808.4998\n",
            "Epochs:811\tstep:600\tbatch_loss:744.2120\n",
            "Epoch loss: 492112.38569097075\n",
            "RMSE on dev data: 9.88443\n",
            "RMSE on train data: 10.00746\n",
            "Epochs:812\tstep:50\tbatch_loss:755.2813\n",
            "Epochs:812\tstep:100\tbatch_loss:790.3630\n",
            "Epochs:812\tstep:150\tbatch_loss:783.6734\n",
            "Epochs:812\tstep:200\tbatch_loss:776.1979\n",
            "Epochs:812\tstep:250\tbatch_loss:935.2432\n",
            "Epochs:812\tstep:300\tbatch_loss:806.3948\n",
            "Epochs:812\tstep:350\tbatch_loss:780.3179\n",
            "Epochs:812\tstep:400\tbatch_loss:782.1621\n",
            "Epochs:812\tstep:450\tbatch_loss:790.2031\n",
            "Epochs:812\tstep:500\tbatch_loss:734.5379\n",
            "Epochs:812\tstep:550\tbatch_loss:808.4369\n",
            "Epochs:812\tstep:600\tbatch_loss:744.1497\n",
            "Epoch loss: 492072.4061993902\n",
            "RMSE on dev data: 9.88440\n",
            "RMSE on train data: 10.00744\n",
            "Epochs:813\tstep:50\tbatch_loss:755.2180\n",
            "Epochs:813\tstep:100\tbatch_loss:790.3005\n",
            "Epochs:813\tstep:150\tbatch_loss:783.6114\n",
            "Epochs:813\tstep:200\tbatch_loss:776.1344\n",
            "Epochs:813\tstep:250\tbatch_loss:935.1789\n",
            "Epochs:813\tstep:300\tbatch_loss:806.3318\n",
            "Epochs:813\tstep:350\tbatch_loss:780.2555\n",
            "Epochs:813\tstep:400\tbatch_loss:782.0997\n",
            "Epochs:813\tstep:450\tbatch_loss:790.1394\n",
            "Epochs:813\tstep:500\tbatch_loss:734.4757\n",
            "Epochs:813\tstep:550\tbatch_loss:808.3740\n",
            "Epochs:813\tstep:600\tbatch_loss:744.0875\n",
            "Epoch loss: 492032.48474068224\n",
            "RMSE on dev data: 9.88438\n",
            "RMSE on train data: 10.00742\n",
            "Epochs:814\tstep:50\tbatch_loss:755.1546\n",
            "Epochs:814\tstep:100\tbatch_loss:790.2380\n",
            "Epochs:814\tstep:150\tbatch_loss:783.5498\n",
            "Epochs:814\tstep:200\tbatch_loss:776.0710\n",
            "Epochs:814\tstep:250\tbatch_loss:935.1145\n",
            "Epochs:814\tstep:300\tbatch_loss:806.2689\n",
            "Epochs:814\tstep:350\tbatch_loss:780.1930\n",
            "Epochs:814\tstep:400\tbatch_loss:782.0373\n",
            "Epochs:814\tstep:450\tbatch_loss:790.0758\n",
            "Epochs:814\tstep:500\tbatch_loss:734.4137\n",
            "Epochs:814\tstep:550\tbatch_loss:808.3112\n",
            "Epochs:814\tstep:600\tbatch_loss:744.0254\n",
            "Epoch loss: 491992.6036346788\n",
            "RMSE on dev data: 9.88435\n",
            "RMSE on train data: 10.00740\n",
            "Epochs:815\tstep:50\tbatch_loss:755.0915\n",
            "Epochs:815\tstep:100\tbatch_loss:790.1756\n",
            "Epochs:815\tstep:150\tbatch_loss:783.4880\n",
            "Epochs:815\tstep:200\tbatch_loss:776.0077\n",
            "Epochs:815\tstep:250\tbatch_loss:935.0503\n",
            "Epochs:815\tstep:300\tbatch_loss:806.2060\n",
            "Epochs:815\tstep:350\tbatch_loss:780.1307\n",
            "Epochs:815\tstep:400\tbatch_loss:781.9749\n",
            "Epochs:815\tstep:450\tbatch_loss:790.0123\n",
            "Epochs:815\tstep:500\tbatch_loss:734.3516\n",
            "Epochs:815\tstep:550\tbatch_loss:808.2485\n",
            "Epochs:815\tstep:600\tbatch_loss:743.9633\n",
            "Epoch loss: 491952.7672616265\n",
            "RMSE on dev data: 9.88433\n",
            "RMSE on train data: 10.00738\n",
            "Epochs:816\tstep:50\tbatch_loss:755.0282\n",
            "Epochs:816\tstep:100\tbatch_loss:790.1133\n",
            "Epochs:816\tstep:150\tbatch_loss:783.4263\n",
            "Epochs:816\tstep:200\tbatch_loss:775.9445\n",
            "Epochs:816\tstep:250\tbatch_loss:934.9863\n",
            "Epochs:816\tstep:300\tbatch_loss:806.1431\n",
            "Epochs:816\tstep:350\tbatch_loss:780.0683\n",
            "Epochs:816\tstep:400\tbatch_loss:781.9127\n",
            "Epochs:816\tstep:450\tbatch_loss:789.9488\n",
            "Epochs:816\tstep:500\tbatch_loss:734.2897\n",
            "Epochs:816\tstep:550\tbatch_loss:808.1858\n",
            "Epochs:816\tstep:600\tbatch_loss:743.9012\n",
            "Epoch loss: 491912.96991432423\n",
            "RMSE on dev data: 9.88431\n",
            "RMSE on train data: 10.00737\n",
            "Epochs:817\tstep:50\tbatch_loss:754.9651\n",
            "Epochs:817\tstep:100\tbatch_loss:790.0510\n",
            "Epochs:817\tstep:150\tbatch_loss:783.3649\n",
            "Epochs:817\tstep:200\tbatch_loss:775.8815\n",
            "Epochs:817\tstep:250\tbatch_loss:934.9222\n",
            "Epochs:817\tstep:300\tbatch_loss:806.0804\n",
            "Epochs:817\tstep:350\tbatch_loss:780.0061\n",
            "Epochs:817\tstep:400\tbatch_loss:781.8505\n",
            "Epochs:817\tstep:450\tbatch_loss:789.8855\n",
            "Epochs:817\tstep:500\tbatch_loss:734.2278\n",
            "Epochs:817\tstep:550\tbatch_loss:808.1232\n",
            "Epochs:817\tstep:600\tbatch_loss:743.8393\n",
            "Epoch loss: 491873.2478957536\n",
            "RMSE on dev data: 9.88429\n",
            "RMSE on train data: 10.00735\n",
            "Epochs:818\tstep:50\tbatch_loss:754.9021\n",
            "Epochs:818\tstep:100\tbatch_loss:789.9889\n",
            "Epochs:818\tstep:150\tbatch_loss:783.3033\n",
            "Epochs:818\tstep:200\tbatch_loss:775.8183\n",
            "Epochs:818\tstep:250\tbatch_loss:934.8582\n",
            "Epochs:818\tstep:300\tbatch_loss:806.0177\n",
            "Epochs:818\tstep:350\tbatch_loss:779.9439\n",
            "Epochs:818\tstep:400\tbatch_loss:781.7883\n",
            "Epochs:818\tstep:450\tbatch_loss:789.8222\n",
            "Epochs:818\tstep:500\tbatch_loss:734.1659\n",
            "Epochs:818\tstep:550\tbatch_loss:808.0607\n",
            "Epochs:818\tstep:600\tbatch_loss:743.7774\n",
            "Epoch loss: 491833.51486887137\n",
            "RMSE on dev data: 9.88427\n",
            "RMSE on train data: 10.00734\n",
            "Epochs:819\tstep:50\tbatch_loss:754.8391\n",
            "Epochs:819\tstep:100\tbatch_loss:789.9267\n",
            "Epochs:819\tstep:150\tbatch_loss:783.2419\n",
            "Epochs:819\tstep:200\tbatch_loss:775.7555\n",
            "Epochs:819\tstep:250\tbatch_loss:934.7943\n",
            "Epochs:819\tstep:300\tbatch_loss:805.9551\n",
            "Epochs:819\tstep:350\tbatch_loss:779.8817\n",
            "Epochs:819\tstep:400\tbatch_loss:781.7262\n",
            "Epochs:819\tstep:450\tbatch_loss:789.7590\n",
            "Epochs:819\tstep:500\tbatch_loss:734.1041\n",
            "Epochs:819\tstep:550\tbatch_loss:807.9982\n",
            "Epochs:819\tstep:600\tbatch_loss:743.7156\n",
            "Epoch loss: 491793.8766789985\n",
            "RMSE on dev data: 9.88425\n",
            "RMSE on train data: 10.00732\n",
            "Epochs:820\tstep:50\tbatch_loss:754.7762\n",
            "Epochs:820\tstep:100\tbatch_loss:789.8647\n",
            "Epochs:820\tstep:150\tbatch_loss:783.1804\n",
            "Epochs:820\tstep:200\tbatch_loss:775.6927\n",
            "Epochs:820\tstep:250\tbatch_loss:934.7307\n",
            "Epochs:820\tstep:300\tbatch_loss:805.8926\n",
            "Epochs:820\tstep:350\tbatch_loss:779.8196\n",
            "Epochs:820\tstep:400\tbatch_loss:781.6642\n",
            "Epochs:820\tstep:450\tbatch_loss:789.6958\n",
            "Epochs:820\tstep:500\tbatch_loss:734.0423\n",
            "Epochs:820\tstep:550\tbatch_loss:807.9358\n",
            "Epochs:820\tstep:600\tbatch_loss:743.6538\n",
            "Epoch loss: 491754.23136286973\n",
            "RMSE on dev data: 9.88423\n",
            "RMSE on train data: 10.00731\n",
            "Epochs:821\tstep:50\tbatch_loss:754.7133\n",
            "Epochs:821\tstep:100\tbatch_loss:789.8027\n",
            "Epochs:821\tstep:150\tbatch_loss:783.1192\n",
            "Epochs:821\tstep:200\tbatch_loss:775.6300\n",
            "Epochs:821\tstep:250\tbatch_loss:934.6669\n",
            "Epochs:821\tstep:300\tbatch_loss:805.8301\n",
            "Epochs:821\tstep:350\tbatch_loss:779.7575\n",
            "Epochs:821\tstep:400\tbatch_loss:781.6023\n",
            "Epochs:821\tstep:450\tbatch_loss:789.6328\n",
            "Epochs:821\tstep:500\tbatch_loss:733.9806\n",
            "Epochs:821\tstep:550\tbatch_loss:807.8734\n",
            "Epochs:821\tstep:600\tbatch_loss:743.5921\n",
            "Epoch loss: 491714.66572649276\n",
            "RMSE on dev data: 9.88421\n",
            "RMSE on train data: 10.00729\n",
            "Epochs:822\tstep:50\tbatch_loss:754.6505\n",
            "Epochs:822\tstep:100\tbatch_loss:789.7408\n",
            "Epochs:822\tstep:150\tbatch_loss:783.0578\n",
            "Epochs:822\tstep:200\tbatch_loss:775.5673\n",
            "Epochs:822\tstep:250\tbatch_loss:934.6033\n",
            "Epochs:822\tstep:300\tbatch_loss:805.7676\n",
            "Epochs:822\tstep:350\tbatch_loss:779.6955\n",
            "Epochs:822\tstep:400\tbatch_loss:781.5403\n",
            "Epochs:822\tstep:450\tbatch_loss:789.5697\n",
            "Epochs:822\tstep:500\tbatch_loss:733.9189\n",
            "Epochs:822\tstep:550\tbatch_loss:807.8111\n",
            "Epochs:822\tstep:600\tbatch_loss:743.5304\n",
            "Epoch loss: 491675.0846907439\n",
            "RMSE on dev data: 9.88419\n",
            "RMSE on train data: 10.00728\n",
            "Epochs:823\tstep:50\tbatch_loss:754.5878\n",
            "Epochs:823\tstep:100\tbatch_loss:789.6790\n",
            "Epochs:823\tstep:150\tbatch_loss:782.9966\n",
            "Epochs:823\tstep:200\tbatch_loss:775.5048\n",
            "Epochs:823\tstep:250\tbatch_loss:934.5398\n",
            "Epochs:823\tstep:300\tbatch_loss:805.7052\n",
            "Epochs:823\tstep:350\tbatch_loss:779.6335\n",
            "Epochs:823\tstep:400\tbatch_loss:781.4785\n",
            "Epochs:823\tstep:450\tbatch_loss:789.5068\n",
            "Epochs:823\tstep:500\tbatch_loss:733.8573\n",
            "Epochs:823\tstep:550\tbatch_loss:807.7489\n",
            "Epochs:823\tstep:600\tbatch_loss:743.4688\n",
            "Epoch loss: 491635.58107175544\n",
            "RMSE on dev data: 9.88417\n",
            "RMSE on train data: 10.00726\n",
            "Epochs:824\tstep:50\tbatch_loss:754.5250\n",
            "Epochs:824\tstep:100\tbatch_loss:789.6171\n",
            "Epochs:824\tstep:150\tbatch_loss:782.9355\n",
            "Epochs:824\tstep:200\tbatch_loss:775.4423\n",
            "Epochs:824\tstep:250\tbatch_loss:934.4763\n",
            "Epochs:824\tstep:300\tbatch_loss:805.6429\n",
            "Epochs:824\tstep:350\tbatch_loss:779.5716\n",
            "Epochs:824\tstep:400\tbatch_loss:781.4166\n",
            "Epochs:824\tstep:450\tbatch_loss:789.4438\n",
            "Epochs:824\tstep:500\tbatch_loss:733.7957\n",
            "Epochs:824\tstep:550\tbatch_loss:807.6867\n",
            "Epochs:824\tstep:600\tbatch_loss:743.4072\n",
            "Epoch loss: 491596.0937037942\n",
            "RMSE on dev data: 9.88415\n",
            "RMSE on train data: 10.00725\n",
            "Epochs:825\tstep:50\tbatch_loss:754.4624\n",
            "Epochs:825\tstep:100\tbatch_loss:789.5554\n",
            "Epochs:825\tstep:150\tbatch_loss:782.8742\n",
            "Epochs:825\tstep:200\tbatch_loss:775.3798\n",
            "Epochs:825\tstep:250\tbatch_loss:934.4128\n",
            "Epochs:825\tstep:300\tbatch_loss:805.5806\n",
            "Epochs:825\tstep:350\tbatch_loss:779.5097\n",
            "Epochs:825\tstep:400\tbatch_loss:781.3548\n",
            "Epochs:825\tstep:450\tbatch_loss:789.3809\n",
            "Epochs:825\tstep:500\tbatch_loss:733.7342\n",
            "Epochs:825\tstep:550\tbatch_loss:807.6245\n",
            "Epochs:825\tstep:600\tbatch_loss:743.3456\n",
            "Epoch loss: 491556.6151067496\n",
            "RMSE on dev data: 9.88413\n",
            "RMSE on train data: 10.00724\n",
            "Epochs:826\tstep:50\tbatch_loss:754.3302\n",
            "Epochs:826\tstep:100\tbatch_loss:789.5484\n",
            "Epochs:826\tstep:150\tbatch_loss:782.3836\n",
            "Epochs:826\tstep:200\tbatch_loss:775.1184\n",
            "Epochs:826\tstep:250\tbatch_loss:934.3275\n",
            "Epochs:826\tstep:300\tbatch_loss:805.6115\n",
            "Epochs:826\tstep:350\tbatch_loss:779.3839\n",
            "Epochs:826\tstep:400\tbatch_loss:781.3423\n",
            "Epochs:826\tstep:450\tbatch_loss:789.3474\n",
            "Epochs:826\tstep:500\tbatch_loss:733.8289\n",
            "Epochs:826\tstep:550\tbatch_loss:807.1131\n",
            "Epochs:826\tstep:600\tbatch_loss:743.5080\n",
            "Epoch loss: 491463.849434015\n",
            "RMSE on dev data: 9.88376\n",
            "RMSE on train data: 10.00773\n",
            "Epochs:827\tstep:50\tbatch_loss:754.2238\n",
            "Epochs:827\tstep:100\tbatch_loss:789.5053\n",
            "Epochs:827\tstep:150\tbatch_loss:782.3152\n",
            "Epochs:827\tstep:200\tbatch_loss:775.0482\n",
            "Epochs:827\tstep:250\tbatch_loss:934.2774\n",
            "Epochs:827\tstep:300\tbatch_loss:805.5530\n",
            "Epochs:827\tstep:350\tbatch_loss:779.3286\n",
            "Epochs:827\tstep:400\tbatch_loss:781.2850\n",
            "Epochs:827\tstep:450\tbatch_loss:789.2891\n",
            "Epochs:827\tstep:500\tbatch_loss:733.7723\n",
            "Epochs:827\tstep:550\tbatch_loss:807.0564\n",
            "Epochs:827\tstep:600\tbatch_loss:743.4514\n",
            "Epoch loss: 491424.95194774255\n",
            "RMSE on dev data: 9.88372\n",
            "RMSE on train data: 10.00770\n",
            "Epochs:828\tstep:50\tbatch_loss:754.1663\n",
            "Epochs:828\tstep:100\tbatch_loss:789.4484\n",
            "Epochs:828\tstep:150\tbatch_loss:782.2593\n",
            "Epochs:828\tstep:200\tbatch_loss:774.9899\n",
            "Epochs:828\tstep:250\tbatch_loss:934.2185\n",
            "Epochs:828\tstep:300\tbatch_loss:805.4958\n",
            "Epochs:828\tstep:350\tbatch_loss:779.2721\n",
            "Epochs:828\tstep:400\tbatch_loss:781.2284\n",
            "Epochs:828\tstep:450\tbatch_loss:789.2311\n",
            "Epochs:828\tstep:500\tbatch_loss:733.7162\n",
            "Epochs:828\tstep:550\tbatch_loss:806.9994\n",
            "Epochs:828\tstep:600\tbatch_loss:743.3951\n",
            "Epoch loss: 491388.7251139073\n",
            "RMSE on dev data: 9.88369\n",
            "RMSE on train data: 10.00767\n",
            "Epochs:829\tstep:50\tbatch_loss:754.1089\n",
            "Epochs:829\tstep:100\tbatch_loss:789.3916\n",
            "Epochs:829\tstep:150\tbatch_loss:782.2032\n",
            "Epochs:829\tstep:200\tbatch_loss:774.9316\n",
            "Epochs:829\tstep:250\tbatch_loss:934.1600\n",
            "Epochs:829\tstep:300\tbatch_loss:805.4386\n",
            "Epochs:829\tstep:350\tbatch_loss:779.2157\n",
            "Epochs:829\tstep:400\tbatch_loss:781.1719\n",
            "Epochs:829\tstep:450\tbatch_loss:789.1733\n",
            "Epochs:829\tstep:500\tbatch_loss:733.6601\n",
            "Epochs:829\tstep:550\tbatch_loss:806.9425\n",
            "Epochs:829\tstep:600\tbatch_loss:743.3388\n",
            "Epoch loss: 491352.5283517468\n",
            "RMSE on dev data: 9.88366\n",
            "RMSE on train data: 10.00765\n",
            "Epochs:830\tstep:50\tbatch_loss:754.0516\n",
            "Epochs:830\tstep:100\tbatch_loss:789.3349\n",
            "Epochs:830\tstep:150\tbatch_loss:782.1475\n",
            "Epochs:830\tstep:200\tbatch_loss:774.8736\n",
            "Epochs:830\tstep:250\tbatch_loss:934.1013\n",
            "Epochs:830\tstep:300\tbatch_loss:805.3816\n",
            "Epochs:830\tstep:350\tbatch_loss:779.1594\n",
            "Epochs:830\tstep:400\tbatch_loss:781.1155\n",
            "Epochs:830\tstep:450\tbatch_loss:789.1156\n",
            "Epochs:830\tstep:500\tbatch_loss:733.6042\n",
            "Epochs:830\tstep:550\tbatch_loss:806.8857\n",
            "Epochs:830\tstep:600\tbatch_loss:743.2826\n",
            "Epoch loss: 491316.4380035918\n",
            "RMSE on dev data: 9.88362\n",
            "RMSE on train data: 10.00762\n",
            "Epochs:831\tstep:50\tbatch_loss:753.9943\n",
            "Epochs:831\tstep:100\tbatch_loss:789.2783\n",
            "Epochs:831\tstep:150\tbatch_loss:782.0917\n",
            "Epochs:831\tstep:200\tbatch_loss:774.8155\n",
            "Epochs:831\tstep:250\tbatch_loss:934.0429\n",
            "Epochs:831\tstep:300\tbatch_loss:805.3246\n",
            "Epochs:831\tstep:350\tbatch_loss:779.1031\n",
            "Epochs:831\tstep:400\tbatch_loss:781.0592\n",
            "Epochs:831\tstep:450\tbatch_loss:789.0579\n",
            "Epochs:831\tstep:500\tbatch_loss:733.5482\n",
            "Epochs:831\tstep:550\tbatch_loss:806.8289\n",
            "Epochs:831\tstep:600\tbatch_loss:743.2265\n",
            "Epoch loss: 491280.36315483076\n",
            "RMSE on dev data: 9.88359\n",
            "RMSE on train data: 10.00760\n",
            "Epochs:832\tstep:50\tbatch_loss:753.9372\n",
            "Epochs:832\tstep:100\tbatch_loss:789.2218\n",
            "Epochs:832\tstep:150\tbatch_loss:782.0360\n",
            "Epochs:832\tstep:200\tbatch_loss:774.7578\n",
            "Epochs:832\tstep:250\tbatch_loss:933.9847\n",
            "Epochs:832\tstep:300\tbatch_loss:805.2678\n",
            "Epochs:832\tstep:350\tbatch_loss:779.0469\n",
            "Epochs:832\tstep:400\tbatch_loss:781.0030\n",
            "Epochs:832\tstep:450\tbatch_loss:789.0004\n",
            "Epochs:832\tstep:500\tbatch_loss:733.4924\n",
            "Epochs:832\tstep:550\tbatch_loss:806.7722\n",
            "Epochs:832\tstep:600\tbatch_loss:743.1705\n",
            "Epoch loss: 491244.3870477747\n",
            "RMSE on dev data: 9.88356\n",
            "RMSE on train data: 10.00758\n",
            "Epochs:833\tstep:50\tbatch_loss:753.8801\n",
            "Epochs:833\tstep:100\tbatch_loss:789.1654\n",
            "Epochs:833\tstep:150\tbatch_loss:781.9805\n",
            "Epochs:833\tstep:200\tbatch_loss:774.7002\n",
            "Epochs:833\tstep:250\tbatch_loss:933.9264\n",
            "Epochs:833\tstep:300\tbatch_loss:805.2110\n",
            "Epochs:833\tstep:350\tbatch_loss:778.9908\n",
            "Epochs:833\tstep:400\tbatch_loss:780.9468\n",
            "Epochs:833\tstep:450\tbatch_loss:788.9430\n",
            "Epochs:833\tstep:500\tbatch_loss:733.4366\n",
            "Epochs:833\tstep:550\tbatch_loss:806.7157\n",
            "Epochs:833\tstep:600\tbatch_loss:743.1146\n",
            "Epoch loss: 491208.45509079547\n",
            "RMSE on dev data: 9.88354\n",
            "RMSE on train data: 10.00756\n",
            "Epochs:834\tstep:50\tbatch_loss:753.8232\n",
            "Epochs:834\tstep:100\tbatch_loss:789.1092\n",
            "Epochs:834\tstep:150\tbatch_loss:781.9249\n",
            "Epochs:834\tstep:200\tbatch_loss:774.6427\n",
            "Epochs:834\tstep:250\tbatch_loss:933.8683\n",
            "Epochs:834\tstep:300\tbatch_loss:805.1543\n",
            "Epochs:834\tstep:350\tbatch_loss:778.9347\n",
            "Epochs:834\tstep:400\tbatch_loss:780.8907\n",
            "Epochs:834\tstep:450\tbatch_loss:788.8857\n",
            "Epochs:834\tstep:500\tbatch_loss:733.3809\n",
            "Epochs:834\tstep:550\tbatch_loss:806.6591\n",
            "Epochs:834\tstep:600\tbatch_loss:743.0587\n",
            "Epoch loss: 491172.57558267016\n",
            "RMSE on dev data: 9.88351\n",
            "RMSE on train data: 10.00753\n",
            "Epochs:835\tstep:50\tbatch_loss:753.7662\n",
            "Epochs:835\tstep:100\tbatch_loss:789.0530\n",
            "Epochs:835\tstep:150\tbatch_loss:781.8695\n",
            "Epochs:835\tstep:200\tbatch_loss:774.5854\n",
            "Epochs:835\tstep:250\tbatch_loss:933.8105\n",
            "Epochs:835\tstep:300\tbatch_loss:805.0977\n",
            "Epochs:835\tstep:350\tbatch_loss:778.8787\n",
            "Epochs:835\tstep:400\tbatch_loss:780.8348\n",
            "Epochs:835\tstep:450\tbatch_loss:788.8286\n",
            "Epochs:835\tstep:500\tbatch_loss:733.3252\n",
            "Epochs:835\tstep:550\tbatch_loss:806.6027\n",
            "Epochs:835\tstep:600\tbatch_loss:743.0029\n",
            "Epoch loss: 491136.759239396\n",
            "RMSE on dev data: 9.88348\n",
            "RMSE on train data: 10.00751\n",
            "Epochs:836\tstep:50\tbatch_loss:753.7095\n",
            "Epochs:836\tstep:100\tbatch_loss:788.9969\n",
            "Epochs:836\tstep:150\tbatch_loss:781.8141\n",
            "Epochs:836\tstep:200\tbatch_loss:774.5281\n",
            "Epochs:836\tstep:250\tbatch_loss:933.7526\n",
            "Epochs:836\tstep:300\tbatch_loss:805.0412\n",
            "Epochs:836\tstep:350\tbatch_loss:778.8228\n",
            "Epochs:836\tstep:400\tbatch_loss:780.7788\n",
            "Epochs:836\tstep:450\tbatch_loss:788.7714\n",
            "Epochs:836\tstep:500\tbatch_loss:733.2696\n",
            "Epochs:836\tstep:550\tbatch_loss:806.5463\n",
            "Epochs:836\tstep:600\tbatch_loss:742.9472\n",
            "Epoch loss: 491100.9826542812\n",
            "RMSE on dev data: 9.88345\n",
            "RMSE on train data: 10.00749\n",
            "Epochs:837\tstep:50\tbatch_loss:753.6527\n",
            "Epochs:837\tstep:100\tbatch_loss:788.9409\n",
            "Epochs:837\tstep:150\tbatch_loss:781.7589\n",
            "Epochs:837\tstep:200\tbatch_loss:774.4710\n",
            "Epochs:837\tstep:250\tbatch_loss:933.6948\n",
            "Epochs:837\tstep:300\tbatch_loss:804.9848\n",
            "Epochs:837\tstep:350\tbatch_loss:778.7669\n",
            "Epochs:837\tstep:400\tbatch_loss:780.7229\n",
            "Epochs:837\tstep:450\tbatch_loss:788.7144\n",
            "Epochs:837\tstep:500\tbatch_loss:733.2141\n",
            "Epochs:837\tstep:550\tbatch_loss:806.4901\n",
            "Epochs:837\tstep:600\tbatch_loss:742.8916\n",
            "Epoch loss: 491065.2573860304\n",
            "RMSE on dev data: 9.88343\n",
            "RMSE on train data: 10.00747\n",
            "Epochs:838\tstep:50\tbatch_loss:753.5960\n",
            "Epochs:838\tstep:100\tbatch_loss:788.8850\n",
            "Epochs:838\tstep:150\tbatch_loss:781.7036\n",
            "Epochs:838\tstep:200\tbatch_loss:774.4139\n",
            "Epochs:838\tstep:250\tbatch_loss:933.6373\n",
            "Epochs:838\tstep:300\tbatch_loss:804.9284\n",
            "Epochs:838\tstep:350\tbatch_loss:778.7111\n",
            "Epochs:838\tstep:400\tbatch_loss:780.6672\n",
            "Epochs:838\tstep:450\tbatch_loss:788.6575\n",
            "Epochs:838\tstep:500\tbatch_loss:733.1586\n",
            "Epochs:838\tstep:550\tbatch_loss:806.4338\n",
            "Epochs:838\tstep:600\tbatch_loss:742.8360\n",
            "Epoch loss: 491029.58237065095\n",
            "RMSE on dev data: 9.88340\n",
            "RMSE on train data: 10.00745\n",
            "Epochs:839\tstep:50\tbatch_loss:753.5393\n",
            "Epochs:839\tstep:100\tbatch_loss:788.8291\n",
            "Epochs:839\tstep:150\tbatch_loss:781.6484\n",
            "Epochs:839\tstep:200\tbatch_loss:774.3570\n",
            "Epochs:839\tstep:250\tbatch_loss:933.5797\n",
            "Epochs:839\tstep:300\tbatch_loss:804.8720\n",
            "Epochs:839\tstep:350\tbatch_loss:778.6553\n",
            "Epochs:839\tstep:400\tbatch_loss:780.6114\n",
            "Epochs:839\tstep:450\tbatch_loss:788.6006\n",
            "Epochs:839\tstep:500\tbatch_loss:733.1032\n",
            "Epochs:839\tstep:550\tbatch_loss:806.3776\n",
            "Epochs:839\tstep:600\tbatch_loss:742.7805\n",
            "Epoch loss: 490993.9313439247\n",
            "RMSE on dev data: 9.88338\n",
            "RMSE on train data: 10.00744\n",
            "Epochs:840\tstep:50\tbatch_loss:753.4828\n",
            "Epochs:840\tstep:100\tbatch_loss:788.7734\n",
            "Epochs:840\tstep:150\tbatch_loss:781.5934\n",
            "Epochs:840\tstep:200\tbatch_loss:774.3003\n",
            "Epochs:840\tstep:250\tbatch_loss:933.5221\n",
            "Epochs:840\tstep:300\tbatch_loss:804.8158\n",
            "Epochs:840\tstep:350\tbatch_loss:778.5996\n",
            "Epochs:840\tstep:400\tbatch_loss:780.5557\n",
            "Epochs:840\tstep:450\tbatch_loss:788.5438\n",
            "Epochs:840\tstep:500\tbatch_loss:733.0478\n",
            "Epochs:840\tstep:550\tbatch_loss:806.3216\n",
            "Epochs:840\tstep:600\tbatch_loss:742.7251\n",
            "Epoch loss: 490958.3620347319\n",
            "RMSE on dev data: 9.88335\n",
            "RMSE on train data: 10.00742\n",
            "Epochs:841\tstep:50\tbatch_loss:753.4263\n",
            "Epochs:841\tstep:100\tbatch_loss:788.7177\n",
            "Epochs:841\tstep:150\tbatch_loss:781.5382\n",
            "Epochs:841\tstep:200\tbatch_loss:774.2435\n",
            "Epochs:841\tstep:250\tbatch_loss:933.4649\n",
            "Epochs:841\tstep:300\tbatch_loss:804.7596\n",
            "Epochs:841\tstep:350\tbatch_loss:778.5439\n",
            "Epochs:841\tstep:400\tbatch_loss:780.5001\n",
            "Epochs:841\tstep:450\tbatch_loss:788.4871\n",
            "Epochs:841\tstep:500\tbatch_loss:732.9925\n",
            "Epochs:841\tstep:550\tbatch_loss:806.2655\n",
            "Epochs:841\tstep:600\tbatch_loss:742.6696\n",
            "Epoch loss: 490922.79353754106\n",
            "RMSE on dev data: 9.88333\n",
            "RMSE on train data: 10.00740\n",
            "Epochs:842\tstep:50\tbatch_loss:753.3699\n",
            "Epochs:842\tstep:100\tbatch_loss:788.6621\n",
            "Epochs:842\tstep:150\tbatch_loss:781.4833\n",
            "Epochs:842\tstep:200\tbatch_loss:774.1870\n",
            "Epochs:842\tstep:250\tbatch_loss:933.4076\n",
            "Epochs:842\tstep:300\tbatch_loss:804.7036\n",
            "Epochs:842\tstep:350\tbatch_loss:778.4883\n",
            "Epochs:842\tstep:400\tbatch_loss:780.4446\n",
            "Epochs:842\tstep:450\tbatch_loss:788.4305\n",
            "Epochs:842\tstep:500\tbatch_loss:732.9372\n",
            "Epochs:842\tstep:550\tbatch_loss:806.2096\n",
            "Epochs:842\tstep:600\tbatch_loss:742.6143\n",
            "Epoch loss: 490887.30453106016\n",
            "RMSE on dev data: 9.88331\n",
            "RMSE on train data: 10.00738\n",
            "Epochs:843\tstep:50\tbatch_loss:753.3135\n",
            "Epochs:843\tstep:100\tbatch_loss:788.6065\n",
            "Epochs:843\tstep:150\tbatch_loss:781.4283\n",
            "Epochs:843\tstep:200\tbatch_loss:774.1304\n",
            "Epochs:843\tstep:250\tbatch_loss:933.3503\n",
            "Epochs:843\tstep:300\tbatch_loss:804.6475\n",
            "Epochs:843\tstep:350\tbatch_loss:778.4327\n",
            "Epochs:843\tstep:400\tbatch_loss:780.3890\n",
            "Epochs:843\tstep:450\tbatch_loss:788.3738\n",
            "Epochs:843\tstep:500\tbatch_loss:732.8820\n",
            "Epochs:843\tstep:550\tbatch_loss:806.1536\n",
            "Epochs:843\tstep:600\tbatch_loss:742.5590\n",
            "Epoch loss: 490851.8058457338\n",
            "RMSE on dev data: 9.88329\n",
            "RMSE on train data: 10.00737\n",
            "Epochs:844\tstep:50\tbatch_loss:753.2572\n",
            "Epochs:844\tstep:100\tbatch_loss:788.5510\n",
            "Epochs:844\tstep:150\tbatch_loss:781.3735\n",
            "Epochs:844\tstep:200\tbatch_loss:774.0742\n",
            "Epochs:844\tstep:250\tbatch_loss:933.2933\n",
            "Epochs:844\tstep:300\tbatch_loss:804.5915\n",
            "Epochs:844\tstep:350\tbatch_loss:778.3772\n",
            "Epochs:844\tstep:400\tbatch_loss:780.3337\n",
            "Epochs:844\tstep:450\tbatch_loss:788.3174\n",
            "Epochs:844\tstep:500\tbatch_loss:732.8268\n",
            "Epochs:844\tstep:550\tbatch_loss:806.0978\n",
            "Epochs:844\tstep:600\tbatch_loss:742.5038\n",
            "Epoch loss: 490816.3985388522\n",
            "RMSE on dev data: 9.88326\n",
            "RMSE on train data: 10.00735\n",
            "Epochs:845\tstep:50\tbatch_loss:753.2010\n",
            "Epochs:845\tstep:100\tbatch_loss:788.4956\n",
            "Epochs:845\tstep:150\tbatch_loss:781.3186\n",
            "Epochs:845\tstep:200\tbatch_loss:774.0177\n",
            "Epochs:845\tstep:250\tbatch_loss:933.2362\n",
            "Epochs:845\tstep:300\tbatch_loss:804.5355\n",
            "Epochs:845\tstep:350\tbatch_loss:778.3217\n",
            "Epochs:845\tstep:400\tbatch_loss:780.2782\n",
            "Epochs:845\tstep:450\tbatch_loss:788.2609\n",
            "Epochs:845\tstep:500\tbatch_loss:732.7716\n",
            "Epochs:845\tstep:550\tbatch_loss:806.0419\n",
            "Epochs:845\tstep:600\tbatch_loss:742.4486\n",
            "Epoch loss: 490780.9714140721\n",
            "RMSE on dev data: 9.88324\n",
            "RMSE on train data: 10.00734\n",
            "Epochs:846\tstep:50\tbatch_loss:753.1447\n",
            "Epochs:846\tstep:100\tbatch_loss:788.4402\n",
            "Epochs:846\tstep:150\tbatch_loss:781.2638\n",
            "Epochs:846\tstep:200\tbatch_loss:773.9614\n",
            "Epochs:846\tstep:250\tbatch_loss:933.1792\n",
            "Epochs:846\tstep:300\tbatch_loss:804.4796\n",
            "Epochs:846\tstep:350\tbatch_loss:778.2662\n",
            "Epochs:846\tstep:400\tbatch_loss:780.2228\n",
            "Epochs:846\tstep:450\tbatch_loss:788.2045\n",
            "Epochs:846\tstep:500\tbatch_loss:732.7165\n",
            "Epochs:846\tstep:550\tbatch_loss:805.9862\n",
            "Epochs:846\tstep:600\tbatch_loss:742.3935\n",
            "Epoch loss: 490745.590941188\n",
            "RMSE on dev data: 9.88322\n",
            "RMSE on train data: 10.00732\n",
            "Epochs:847\tstep:50\tbatch_loss:753.0886\n",
            "Epochs:847\tstep:100\tbatch_loss:788.3848\n",
            "Epochs:847\tstep:150\tbatch_loss:781.2091\n",
            "Epochs:847\tstep:200\tbatch_loss:773.9054\n",
            "Epochs:847\tstep:250\tbatch_loss:933.1223\n",
            "Epochs:847\tstep:300\tbatch_loss:804.4239\n",
            "Epochs:847\tstep:350\tbatch_loss:778.2109\n",
            "Epochs:847\tstep:400\tbatch_loss:780.1676\n",
            "Epochs:847\tstep:450\tbatch_loss:788.1482\n",
            "Epochs:847\tstep:500\tbatch_loss:732.6615\n",
            "Epochs:847\tstep:550\tbatch_loss:805.9305\n",
            "Epochs:847\tstep:600\tbatch_loss:742.3384\n",
            "Epoch loss: 490710.2895280754\n",
            "RMSE on dev data: 9.88320\n",
            "RMSE on train data: 10.00731\n",
            "Epochs:848\tstep:50\tbatch_loss:753.0325\n",
            "Epochs:848\tstep:100\tbatch_loss:788.3296\n",
            "Epochs:848\tstep:150\tbatch_loss:781.1543\n",
            "Epochs:848\tstep:200\tbatch_loss:773.8492\n",
            "Epochs:848\tstep:250\tbatch_loss:933.0655\n",
            "Epochs:848\tstep:300\tbatch_loss:804.3680\n",
            "Epochs:848\tstep:350\tbatch_loss:778.1555\n",
            "Epochs:848\tstep:400\tbatch_loss:780.1123\n",
            "Epochs:848\tstep:450\tbatch_loss:788.0919\n",
            "Epochs:848\tstep:500\tbatch_loss:732.6064\n",
            "Epochs:848\tstep:550\tbatch_loss:805.8748\n",
            "Epochs:848\tstep:600\tbatch_loss:742.2834\n",
            "Epoch loss: 490674.96347601345\n",
            "RMSE on dev data: 9.88318\n",
            "RMSE on train data: 10.00729\n",
            "Epochs:849\tstep:50\tbatch_loss:752.9765\n",
            "Epochs:849\tstep:100\tbatch_loss:788.2744\n",
            "Epochs:849\tstep:150\tbatch_loss:781.0998\n",
            "Epochs:849\tstep:200\tbatch_loss:773.7933\n",
            "Epochs:849\tstep:250\tbatch_loss:933.0087\n",
            "Epochs:849\tstep:300\tbatch_loss:804.3124\n",
            "Epochs:849\tstep:350\tbatch_loss:778.1002\n",
            "Epochs:849\tstep:400\tbatch_loss:780.0571\n",
            "Epochs:849\tstep:450\tbatch_loss:788.0357\n",
            "Epochs:849\tstep:500\tbatch_loss:732.5515\n",
            "Epochs:849\tstep:550\tbatch_loss:805.8192\n",
            "Epochs:849\tstep:600\tbatch_loss:742.2284\n",
            "Epoch loss: 490639.717562103\n",
            "RMSE on dev data: 9.88316\n",
            "RMSE on train data: 10.00728\n",
            "Epochs:850\tstep:50\tbatch_loss:752.9205\n",
            "Epochs:850\tstep:100\tbatch_loss:788.2192\n",
            "Epochs:850\tstep:150\tbatch_loss:781.0451\n",
            "Epochs:850\tstep:200\tbatch_loss:773.7373\n",
            "Epochs:850\tstep:250\tbatch_loss:932.9520\n",
            "Epochs:850\tstep:300\tbatch_loss:804.2566\n",
            "Epochs:850\tstep:350\tbatch_loss:778.0449\n",
            "Epochs:850\tstep:400\tbatch_loss:780.0019\n",
            "Epochs:850\tstep:450\tbatch_loss:787.9795\n",
            "Epochs:850\tstep:500\tbatch_loss:732.4965\n",
            "Epochs:850\tstep:550\tbatch_loss:805.7636\n",
            "Epochs:850\tstep:600\tbatch_loss:742.1735\n",
            "Epoch loss: 490604.4530304515\n",
            "RMSE on dev data: 9.88314\n",
            "RMSE on train data: 10.00726\n",
            "Epochs:851\tstep:50\tbatch_loss:752.7939\n",
            "Epochs:851\tstep:100\tbatch_loss:788.2248\n",
            "Epochs:851\tstep:150\tbatch_loss:780.6311\n",
            "Epochs:851\tstep:200\tbatch_loss:773.4565\n",
            "Epochs:851\tstep:250\tbatch_loss:932.9273\n",
            "Epochs:851\tstep:300\tbatch_loss:804.3066\n",
            "Epochs:851\tstep:350\tbatch_loss:777.9212\n",
            "Epochs:851\tstep:400\tbatch_loss:780.0015\n",
            "Epochs:851\tstep:450\tbatch_loss:787.9619\n",
            "Epochs:851\tstep:500\tbatch_loss:732.6109\n",
            "Epochs:851\tstep:550\tbatch_loss:805.2314\n",
            "Epochs:851\tstep:600\tbatch_loss:742.3484\n",
            "Epoch loss: 490518.09042100713\n",
            "RMSE on dev data: 9.88290\n",
            "RMSE on train data: 10.00768\n",
            "Epochs:852\tstep:50\tbatch_loss:752.7040\n",
            "Epochs:852\tstep:100\tbatch_loss:788.1864\n",
            "Epochs:852\tstep:150\tbatch_loss:780.5714\n",
            "Epochs:852\tstep:200\tbatch_loss:773.3931\n",
            "Epochs:852\tstep:250\tbatch_loss:932.8836\n",
            "Epochs:852\tstep:300\tbatch_loss:804.2538\n",
            "Epochs:852\tstep:350\tbatch_loss:777.8721\n",
            "Epochs:852\tstep:400\tbatch_loss:779.9500\n",
            "Epochs:852\tstep:450\tbatch_loss:787.9096\n",
            "Epochs:852\tstep:500\tbatch_loss:732.5602\n",
            "Epochs:852\tstep:550\tbatch_loss:805.1809\n",
            "Epochs:852\tstep:600\tbatch_loss:742.2977\n",
            "Epoch loss: 490483.52951272525\n",
            "RMSE on dev data: 9.88286\n",
            "RMSE on train data: 10.00766\n",
            "Epochs:853\tstep:50\tbatch_loss:752.6524\n",
            "Epochs:853\tstep:100\tbatch_loss:788.1354\n",
            "Epochs:853\tstep:150\tbatch_loss:780.5214\n",
            "Epochs:853\tstep:200\tbatch_loss:773.3406\n",
            "Epochs:853\tstep:250\tbatch_loss:932.8308\n",
            "Epochs:853\tstep:300\tbatch_loss:804.2026\n",
            "Epochs:853\tstep:350\tbatch_loss:777.8215\n",
            "Epochs:853\tstep:400\tbatch_loss:779.8994\n",
            "Epochs:853\tstep:450\tbatch_loss:787.8577\n",
            "Epochs:853\tstep:500\tbatch_loss:732.5100\n",
            "Epochs:853\tstep:550\tbatch_loss:805.1299\n",
            "Epochs:853\tstep:600\tbatch_loss:742.2473\n",
            "Epoch loss: 490451.1220950807\n",
            "RMSE on dev data: 9.88283\n",
            "RMSE on train data: 10.00763\n",
            "Epochs:854\tstep:50\tbatch_loss:752.6010\n",
            "Epochs:854\tstep:100\tbatch_loss:788.0846\n",
            "Epochs:854\tstep:150\tbatch_loss:780.4713\n",
            "Epochs:854\tstep:200\tbatch_loss:773.2882\n",
            "Epochs:854\tstep:250\tbatch_loss:932.7783\n",
            "Epochs:854\tstep:300\tbatch_loss:804.1514\n",
            "Epochs:854\tstep:350\tbatch_loss:777.7711\n",
            "Epochs:854\tstep:400\tbatch_loss:779.8489\n",
            "Epochs:854\tstep:450\tbatch_loss:787.8059\n",
            "Epochs:854\tstep:500\tbatch_loss:732.4599\n",
            "Epochs:854\tstep:550\tbatch_loss:805.0789\n",
            "Epochs:854\tstep:600\tbatch_loss:742.1970\n",
            "Epoch loss: 490418.7424520857\n",
            "RMSE on dev data: 9.88280\n",
            "RMSE on train data: 10.00760\n",
            "Epochs:855\tstep:50\tbatch_loss:752.5497\n",
            "Epochs:855\tstep:100\tbatch_loss:788.0338\n",
            "Epochs:855\tstep:150\tbatch_loss:780.4213\n",
            "Epochs:855\tstep:200\tbatch_loss:773.2359\n",
            "Epochs:855\tstep:250\tbatch_loss:932.7258\n",
            "Epochs:855\tstep:300\tbatch_loss:804.1002\n",
            "Epochs:855\tstep:350\tbatch_loss:777.7207\n",
            "Epochs:855\tstep:400\tbatch_loss:779.7984\n",
            "Epochs:855\tstep:450\tbatch_loss:787.7542\n",
            "Epochs:855\tstep:500\tbatch_loss:732.4098\n",
            "Epochs:855\tstep:550\tbatch_loss:805.0280\n",
            "Epochs:855\tstep:600\tbatch_loss:742.1467\n",
            "Epoch loss: 490386.4217088304\n",
            "RMSE on dev data: 9.88276\n",
            "RMSE on train data: 10.00758\n",
            "Epochs:856\tstep:50\tbatch_loss:752.4985\n",
            "Epochs:856\tstep:100\tbatch_loss:787.9831\n",
            "Epochs:856\tstep:150\tbatch_loss:780.3715\n",
            "Epochs:856\tstep:200\tbatch_loss:773.1840\n",
            "Epochs:856\tstep:250\tbatch_loss:932.6734\n",
            "Epochs:856\tstep:300\tbatch_loss:804.0492\n",
            "Epochs:856\tstep:350\tbatch_loss:777.6704\n",
            "Epochs:856\tstep:400\tbatch_loss:779.7480\n",
            "Epochs:856\tstep:450\tbatch_loss:787.7026\n",
            "Epochs:856\tstep:500\tbatch_loss:732.3598\n",
            "Epochs:856\tstep:550\tbatch_loss:804.9772\n",
            "Epochs:856\tstep:600\tbatch_loss:742.0966\n",
            "Epoch loss: 490354.19153785997\n",
            "RMSE on dev data: 9.88273\n",
            "RMSE on train data: 10.00755\n",
            "Epochs:857\tstep:50\tbatch_loss:752.4473\n",
            "Epochs:857\tstep:100\tbatch_loss:787.9326\n",
            "Epochs:857\tstep:150\tbatch_loss:780.3216\n",
            "Epochs:857\tstep:200\tbatch_loss:773.1319\n",
            "Epochs:857\tstep:250\tbatch_loss:932.6212\n",
            "Epochs:857\tstep:300\tbatch_loss:803.9982\n",
            "Epochs:857\tstep:350\tbatch_loss:777.6201\n",
            "Epochs:857\tstep:400\tbatch_loss:779.6977\n",
            "Epochs:857\tstep:450\tbatch_loss:787.6511\n",
            "Epochs:857\tstep:500\tbatch_loss:732.3099\n",
            "Epochs:857\tstep:550\tbatch_loss:804.9264\n",
            "Epochs:857\tstep:600\tbatch_loss:742.0465\n",
            "Epoch loss: 490321.97943709354\n",
            "RMSE on dev data: 9.88270\n",
            "RMSE on train data: 10.00753\n",
            "Epochs:858\tstep:50\tbatch_loss:752.3962\n",
            "Epochs:858\tstep:100\tbatch_loss:787.8821\n",
            "Epochs:858\tstep:150\tbatch_loss:780.2720\n",
            "Epochs:858\tstep:200\tbatch_loss:773.0802\n",
            "Epochs:858\tstep:250\tbatch_loss:932.5690\n",
            "Epochs:858\tstep:300\tbatch_loss:803.9474\n",
            "Epochs:858\tstep:350\tbatch_loss:777.5699\n",
            "Epochs:858\tstep:400\tbatch_loss:779.6475\n",
            "Epochs:858\tstep:450\tbatch_loss:787.5997\n",
            "Epochs:858\tstep:500\tbatch_loss:732.2600\n",
            "Epochs:858\tstep:550\tbatch_loss:804.8757\n",
            "Epochs:858\tstep:600\tbatch_loss:741.9965\n",
            "Epoch loss: 490289.83391954243\n",
            "RMSE on dev data: 9.88267\n",
            "RMSE on train data: 10.00751\n",
            "Epochs:859\tstep:50\tbatch_loss:752.3452\n",
            "Epochs:859\tstep:100\tbatch_loss:787.8318\n",
            "Epochs:859\tstep:150\tbatch_loss:780.2223\n",
            "Epochs:859\tstep:200\tbatch_loss:773.0284\n",
            "Epochs:859\tstep:250\tbatch_loss:932.5170\n",
            "Epochs:859\tstep:300\tbatch_loss:803.8966\n",
            "Epochs:859\tstep:350\tbatch_loss:777.5197\n",
            "Epochs:859\tstep:400\tbatch_loss:779.5973\n",
            "Epochs:859\tstep:450\tbatch_loss:787.5484\n",
            "Epochs:859\tstep:500\tbatch_loss:732.2102\n",
            "Epochs:859\tstep:550\tbatch_loss:804.8251\n",
            "Epochs:859\tstep:600\tbatch_loss:741.9466\n",
            "Epoch loss: 490257.73471064266\n",
            "RMSE on dev data: 9.88264\n",
            "RMSE on train data: 10.00749\n",
            "Epochs:860\tstep:50\tbatch_loss:752.2942\n",
            "Epochs:860\tstep:100\tbatch_loss:787.7815\n",
            "Epochs:860\tstep:150\tbatch_loss:780.1727\n",
            "Epochs:860\tstep:200\tbatch_loss:772.9770\n",
            "Epochs:860\tstep:250\tbatch_loss:932.4651\n",
            "Epochs:860\tstep:300\tbatch_loss:803.8459\n",
            "Epochs:860\tstep:350\tbatch_loss:777.4696\n",
            "Epochs:860\tstep:400\tbatch_loss:779.5472\n",
            "Epochs:860\tstep:450\tbatch_loss:787.4972\n",
            "Epochs:860\tstep:500\tbatch_loss:732.1605\n",
            "Epochs:860\tstep:550\tbatch_loss:804.7746\n",
            "Epochs:860\tstep:600\tbatch_loss:741.8967\n",
            "Epoch loss: 490225.6934322495\n",
            "RMSE on dev data: 9.88261\n",
            "RMSE on train data: 10.00746\n",
            "Epochs:861\tstep:50\tbatch_loss:752.2434\n",
            "Epochs:861\tstep:100\tbatch_loss:787.7313\n",
            "Epochs:861\tstep:150\tbatch_loss:780.1231\n",
            "Epochs:861\tstep:200\tbatch_loss:772.9254\n",
            "Epochs:861\tstep:250\tbatch_loss:932.4133\n",
            "Epochs:861\tstep:300\tbatch_loss:803.7952\n",
            "Epochs:861\tstep:350\tbatch_loss:777.4196\n",
            "Epochs:861\tstep:400\tbatch_loss:779.4971\n",
            "Epochs:861\tstep:450\tbatch_loss:787.4460\n",
            "Epochs:861\tstep:500\tbatch_loss:732.1107\n",
            "Epochs:861\tstep:550\tbatch_loss:804.7241\n",
            "Epochs:861\tstep:600\tbatch_loss:741.8469\n",
            "Epoch loss: 490193.66606547916\n",
            "RMSE on dev data: 9.88258\n",
            "RMSE on train data: 10.00744\n",
            "Epochs:862\tstep:50\tbatch_loss:752.1926\n",
            "Epochs:862\tstep:100\tbatch_loss:787.6811\n",
            "Epochs:862\tstep:150\tbatch_loss:780.0737\n",
            "Epochs:862\tstep:200\tbatch_loss:772.8742\n",
            "Epochs:862\tstep:250\tbatch_loss:932.3615\n",
            "Epochs:862\tstep:300\tbatch_loss:803.7447\n",
            "Epochs:862\tstep:350\tbatch_loss:777.3696\n",
            "Epochs:862\tstep:400\tbatch_loss:779.4472\n",
            "Epochs:862\tstep:450\tbatch_loss:787.3950\n",
            "Epochs:862\tstep:500\tbatch_loss:732.0611\n",
            "Epochs:862\tstep:550\tbatch_loss:804.6737\n",
            "Epochs:862\tstep:600\tbatch_loss:741.7972\n",
            "Epoch loss: 490161.7258989398\n",
            "RMSE on dev data: 9.88256\n",
            "RMSE on train data: 10.00742\n",
            "Epochs:863\tstep:50\tbatch_loss:752.1419\n",
            "Epochs:863\tstep:100\tbatch_loss:787.6311\n",
            "Epochs:863\tstep:150\tbatch_loss:780.0242\n",
            "Epochs:863\tstep:200\tbatch_loss:772.8229\n",
            "Epochs:863\tstep:250\tbatch_loss:932.3100\n",
            "Epochs:863\tstep:300\tbatch_loss:803.6942\n",
            "Epochs:863\tstep:350\tbatch_loss:777.3197\n",
            "Epochs:863\tstep:400\tbatch_loss:779.3973\n",
            "Epochs:863\tstep:450\tbatch_loss:787.3440\n",
            "Epochs:863\tstep:500\tbatch_loss:732.0115\n",
            "Epochs:863\tstep:550\tbatch_loss:804.6234\n",
            "Epochs:863\tstep:600\tbatch_loss:741.7474\n",
            "Epoch loss: 490129.7935329975\n",
            "RMSE on dev data: 9.88253\n",
            "RMSE on train data: 10.00740\n",
            "Epochs:864\tstep:50\tbatch_loss:752.0912\n",
            "Epochs:864\tstep:100\tbatch_loss:787.5811\n",
            "Epochs:864\tstep:150\tbatch_loss:779.9749\n",
            "Epochs:864\tstep:200\tbatch_loss:772.7717\n",
            "Epochs:864\tstep:250\tbatch_loss:932.2584\n",
            "Epochs:864\tstep:300\tbatch_loss:803.6437\n",
            "Epochs:864\tstep:350\tbatch_loss:777.2697\n",
            "Epochs:864\tstep:400\tbatch_loss:779.3474\n",
            "Epochs:864\tstep:450\tbatch_loss:787.2930\n",
            "Epochs:864\tstep:500\tbatch_loss:731.9619\n",
            "Epochs:864\tstep:550\tbatch_loss:804.5730\n",
            "Epochs:864\tstep:600\tbatch_loss:741.6978\n",
            "Epoch loss: 490097.90154816007\n",
            "RMSE on dev data: 9.88251\n",
            "RMSE on train data: 10.00739\n",
            "Epochs:865\tstep:50\tbatch_loss:752.0405\n",
            "Epochs:865\tstep:100\tbatch_loss:787.5311\n",
            "Epochs:865\tstep:150\tbatch_loss:779.9256\n",
            "Epochs:865\tstep:200\tbatch_loss:772.7207\n",
            "Epochs:865\tstep:250\tbatch_loss:932.2069\n",
            "Epochs:865\tstep:300\tbatch_loss:803.5933\n",
            "Epochs:865\tstep:350\tbatch_loss:777.2199\n",
            "Epochs:865\tstep:400\tbatch_loss:779.2976\n",
            "Epochs:865\tstep:450\tbatch_loss:787.2422\n",
            "Epochs:865\tstep:500\tbatch_loss:731.9124\n",
            "Epochs:865\tstep:550\tbatch_loss:804.5228\n",
            "Epochs:865\tstep:600\tbatch_loss:741.6482\n",
            "Epoch loss: 490066.0644223984\n",
            "RMSE on dev data: 9.88248\n",
            "RMSE on train data: 10.00737\n",
            "Epochs:866\tstep:50\tbatch_loss:751.9900\n",
            "Epochs:866\tstep:100\tbatch_loss:787.4813\n",
            "Epochs:866\tstep:150\tbatch_loss:779.8763\n",
            "Epochs:866\tstep:200\tbatch_loss:772.6698\n",
            "Epochs:866\tstep:250\tbatch_loss:932.1556\n",
            "Epochs:866\tstep:300\tbatch_loss:803.5430\n",
            "Epochs:866\tstep:350\tbatch_loss:777.1701\n",
            "Epochs:866\tstep:400\tbatch_loss:779.2479\n",
            "Epochs:866\tstep:450\tbatch_loss:787.1914\n",
            "Epochs:866\tstep:500\tbatch_loss:731.8630\n",
            "Epochs:866\tstep:550\tbatch_loss:804.4727\n",
            "Epochs:866\tstep:600\tbatch_loss:741.5987\n",
            "Epoch loss: 490034.26936632255\n",
            "RMSE on dev data: 9.88246\n",
            "RMSE on train data: 10.00735\n",
            "Epochs:867\tstep:50\tbatch_loss:751.9395\n",
            "Epochs:867\tstep:100\tbatch_loss:787.4315\n",
            "Epochs:867\tstep:150\tbatch_loss:779.8272\n",
            "Epochs:867\tstep:200\tbatch_loss:772.6190\n",
            "Epochs:867\tstep:250\tbatch_loss:932.1042\n",
            "Epochs:867\tstep:300\tbatch_loss:803.4928\n",
            "Epochs:867\tstep:350\tbatch_loss:777.1204\n",
            "Epochs:867\tstep:400\tbatch_loss:779.1982\n",
            "Epochs:867\tstep:450\tbatch_loss:787.1407\n",
            "Epochs:867\tstep:500\tbatch_loss:731.8136\n",
            "Epochs:867\tstep:550\tbatch_loss:804.4225\n",
            "Epochs:867\tstep:600\tbatch_loss:741.5492\n",
            "Epoch loss: 490002.50390074064\n",
            "RMSE on dev data: 9.88243\n",
            "RMSE on train data: 10.00733\n",
            "Epochs:868\tstep:50\tbatch_loss:751.8891\n",
            "Epochs:868\tstep:100\tbatch_loss:787.3818\n",
            "Epochs:868\tstep:150\tbatch_loss:779.7780\n",
            "Epochs:868\tstep:200\tbatch_loss:772.5681\n",
            "Epochs:868\tstep:250\tbatch_loss:932.0530\n",
            "Epochs:868\tstep:300\tbatch_loss:803.4425\n",
            "Epochs:868\tstep:350\tbatch_loss:777.0707\n",
            "Epochs:868\tstep:400\tbatch_loss:779.1485\n",
            "Epochs:868\tstep:450\tbatch_loss:787.0900\n",
            "Epochs:868\tstep:500\tbatch_loss:731.7642\n",
            "Epochs:868\tstep:550\tbatch_loss:804.3725\n",
            "Epochs:868\tstep:600\tbatch_loss:741.4998\n",
            "Epoch loss: 489970.7511049493\n",
            "RMSE on dev data: 9.88241\n",
            "RMSE on train data: 10.00731\n",
            "Epochs:869\tstep:50\tbatch_loss:751.8387\n",
            "Epochs:869\tstep:100\tbatch_loss:787.3321\n",
            "Epochs:869\tstep:150\tbatch_loss:779.7289\n",
            "Epochs:869\tstep:200\tbatch_loss:772.5176\n",
            "Epochs:869\tstep:250\tbatch_loss:932.0018\n",
            "Epochs:869\tstep:300\tbatch_loss:803.3924\n",
            "Epochs:869\tstep:350\tbatch_loss:777.0210\n",
            "Epochs:869\tstep:400\tbatch_loss:779.0989\n",
            "Epochs:869\tstep:450\tbatch_loss:787.0395\n",
            "Epochs:869\tstep:500\tbatch_loss:731.7149\n",
            "Epochs:869\tstep:550\tbatch_loss:804.3225\n",
            "Epochs:869\tstep:600\tbatch_loss:741.4505\n",
            "Epoch loss: 489939.07642547996\n",
            "RMSE on dev data: 9.88239\n",
            "RMSE on train data: 10.00730\n",
            "Epochs:870\tstep:50\tbatch_loss:751.7883\n",
            "Epochs:870\tstep:100\tbatch_loss:787.2825\n",
            "Epochs:870\tstep:150\tbatch_loss:779.6798\n",
            "Epochs:870\tstep:200\tbatch_loss:772.4669\n",
            "Epochs:870\tstep:250\tbatch_loss:931.9508\n",
            "Epochs:870\tstep:300\tbatch_loss:803.3423\n",
            "Epochs:870\tstep:350\tbatch_loss:776.9714\n",
            "Epochs:870\tstep:400\tbatch_loss:779.0494\n",
            "Epochs:870\tstep:450\tbatch_loss:786.9889\n",
            "Epochs:870\tstep:500\tbatch_loss:731.6656\n",
            "Epochs:870\tstep:550\tbatch_loss:804.2725\n",
            "Epochs:870\tstep:600\tbatch_loss:741.4011\n",
            "Epoch loss: 489907.39711117477\n",
            "RMSE on dev data: 9.88236\n",
            "RMSE on train data: 10.00728\n",
            "Epochs:871\tstep:50\tbatch_loss:751.7380\n",
            "Epochs:871\tstep:100\tbatch_loss:787.2329\n",
            "Epochs:871\tstep:150\tbatch_loss:779.6308\n",
            "Epochs:871\tstep:200\tbatch_loss:772.4164\n",
            "Epochs:871\tstep:250\tbatch_loss:931.8997\n",
            "Epochs:871\tstep:300\tbatch_loss:803.2922\n",
            "Epochs:871\tstep:350\tbatch_loss:776.9218\n",
            "Epochs:871\tstep:400\tbatch_loss:778.9999\n",
            "Epochs:871\tstep:450\tbatch_loss:786.9384\n",
            "Epochs:871\tstep:500\tbatch_loss:731.6163\n",
            "Epochs:871\tstep:550\tbatch_loss:804.2226\n",
            "Epochs:871\tstep:600\tbatch_loss:741.3518\n",
            "Epoch loss: 489875.750429668\n",
            "RMSE on dev data: 9.88234\n",
            "RMSE on train data: 10.00727\n",
            "Epochs:872\tstep:50\tbatch_loss:751.6878\n",
            "Epochs:872\tstep:100\tbatch_loss:787.1834\n",
            "Epochs:872\tstep:150\tbatch_loss:779.5819\n",
            "Epochs:872\tstep:200\tbatch_loss:772.3661\n",
            "Epochs:872\tstep:250\tbatch_loss:931.8487\n",
            "Epochs:872\tstep:300\tbatch_loss:803.2423\n",
            "Epochs:872\tstep:350\tbatch_loss:776.8722\n",
            "Epochs:872\tstep:400\tbatch_loss:778.9504\n",
            "Epochs:872\tstep:450\tbatch_loss:786.8881\n",
            "Epochs:872\tstep:500\tbatch_loss:731.5671\n",
            "Epochs:872\tstep:550\tbatch_loss:804.1727\n",
            "Epochs:872\tstep:600\tbatch_loss:741.3026\n",
            "Epoch loss: 489844.16992291214\n",
            "RMSE on dev data: 9.88232\n",
            "RMSE on train data: 10.00725\n",
            "Epochs:873\tstep:50\tbatch_loss:751.6376\n",
            "Epochs:873\tstep:100\tbatch_loss:787.1340\n",
            "Epochs:873\tstep:150\tbatch_loss:779.5329\n",
            "Epochs:873\tstep:200\tbatch_loss:772.3157\n",
            "Epochs:873\tstep:250\tbatch_loss:931.7979\n",
            "Epochs:873\tstep:300\tbatch_loss:803.1923\n",
            "Epochs:873\tstep:350\tbatch_loss:776.8227\n",
            "Epochs:873\tstep:400\tbatch_loss:778.9010\n",
            "Epochs:873\tstep:450\tbatch_loss:786.8377\n",
            "Epochs:873\tstep:500\tbatch_loss:731.5179\n",
            "Epochs:873\tstep:550\tbatch_loss:804.1229\n",
            "Epochs:873\tstep:600\tbatch_loss:741.2534\n",
            "Epoch loss: 489812.5814442714\n",
            "RMSE on dev data: 9.88230\n",
            "RMSE on train data: 10.00723\n",
            "Epochs:874\tstep:50\tbatch_loss:751.5874\n",
            "Epochs:874\tstep:100\tbatch_loss:787.0846\n",
            "Epochs:874\tstep:150\tbatch_loss:779.4841\n",
            "Epochs:874\tstep:200\tbatch_loss:772.2654\n",
            "Epochs:874\tstep:250\tbatch_loss:931.7470\n",
            "Epochs:874\tstep:300\tbatch_loss:803.1424\n",
            "Epochs:874\tstep:350\tbatch_loss:776.7733\n",
            "Epochs:874\tstep:400\tbatch_loss:778.8517\n",
            "Epochs:874\tstep:450\tbatch_loss:786.7874\n",
            "Epochs:874\tstep:500\tbatch_loss:731.4688\n",
            "Epochs:874\tstep:550\tbatch_loss:804.0731\n",
            "Epochs:874\tstep:600\tbatch_loss:741.2042\n",
            "Epoch loss: 489781.03664106276\n",
            "RMSE on dev data: 9.88228\n",
            "RMSE on train data: 10.00722\n",
            "Epochs:875\tstep:50\tbatch_loss:751.5373\n",
            "Epochs:875\tstep:100\tbatch_loss:787.0352\n",
            "Epochs:875\tstep:150\tbatch_loss:779.4352\n",
            "Epochs:875\tstep:200\tbatch_loss:772.2151\n",
            "Epochs:875\tstep:250\tbatch_loss:931.6962\n",
            "Epochs:875\tstep:300\tbatch_loss:803.0925\n",
            "Epochs:875\tstep:350\tbatch_loss:776.7238\n",
            "Epochs:875\tstep:400\tbatch_loss:778.8023\n",
            "Epochs:875\tstep:450\tbatch_loss:786.7370\n",
            "Epochs:875\tstep:500\tbatch_loss:731.4196\n",
            "Epochs:875\tstep:550\tbatch_loss:804.0234\n",
            "Epochs:875\tstep:600\tbatch_loss:741.1551\n",
            "Epoch loss: 489749.4995452923\n",
            "RMSE on dev data: 9.88226\n",
            "RMSE on train data: 10.00721\n",
            "Epochs:876\tstep:50\tbatch_loss:751.4174\n",
            "Epochs:876\tstep:100\tbatch_loss:787.0518\n",
            "Epochs:876\tstep:150\tbatch_loss:779.0932\n",
            "Epochs:876\tstep:200\tbatch_loss:771.9217\n",
            "Epochs:876\tstep:250\tbatch_loss:931.7315\n",
            "Epochs:876\tstep:300\tbatch_loss:803.1517\n",
            "Epochs:876\tstep:350\tbatch_loss:776.6074\n",
            "Epochs:876\tstep:400\tbatch_loss:778.8111\n",
            "Epochs:876\tstep:450\tbatch_loss:786.7341\n",
            "Epochs:876\tstep:500\tbatch_loss:731.5520\n",
            "Epochs:876\tstep:550\tbatch_loss:803.4757\n",
            "Epochs:876\tstep:600\tbatch_loss:741.3446\n",
            "Epoch loss: 489669.23606822314\n",
            "RMSE on dev data: 9.88211\n",
            "RMSE on train data: 10.00750\n",
            "Epochs:877\tstep:50\tbatch_loss:751.3449\n",
            "Epochs:877\tstep:100\tbatch_loss:787.0160\n",
            "Epochs:877\tstep:150\tbatch_loss:779.0421\n",
            "Epochs:877\tstep:200\tbatch_loss:771.8658\n",
            "Epochs:877\tstep:250\tbatch_loss:931.6920\n",
            "Epochs:877\tstep:300\tbatch_loss:803.1042\n",
            "Epochs:877\tstep:350\tbatch_loss:776.5634\n",
            "Epochs:877\tstep:400\tbatch_loss:778.7649\n",
            "Epochs:877\tstep:450\tbatch_loss:786.6872\n",
            "Epochs:877\tstep:500\tbatch_loss:731.5065\n",
            "Epochs:877\tstep:550\tbatch_loss:803.4306\n",
            "Epochs:877\tstep:600\tbatch_loss:741.2992\n",
            "Epoch loss: 489638.69647407724\n",
            "RMSE on dev data: 9.88207\n",
            "RMSE on train data: 10.00747\n",
            "Epochs:878\tstep:50\tbatch_loss:751.2987\n",
            "Epochs:878\tstep:100\tbatch_loss:786.9703\n",
            "Epochs:878\tstep:150\tbatch_loss:778.9972\n",
            "Epochs:878\tstep:200\tbatch_loss:771.8186\n",
            "Epochs:878\tstep:250\tbatch_loss:931.6447\n",
            "Epochs:878\tstep:300\tbatch_loss:803.0582\n",
            "Epochs:878\tstep:350\tbatch_loss:776.5181\n",
            "Epochs:878\tstep:400\tbatch_loss:778.7196\n",
            "Epochs:878\tstep:450\tbatch_loss:786.6407\n",
            "Epochs:878\tstep:500\tbatch_loss:731.4616\n",
            "Epochs:878\tstep:550\tbatch_loss:803.3848\n",
            "Epochs:878\tstep:600\tbatch_loss:741.2541\n",
            "Epoch loss: 489609.6646863969\n",
            "RMSE on dev data: 9.88204\n",
            "RMSE on train data: 10.00745\n",
            "Epochs:879\tstep:50\tbatch_loss:751.2527\n",
            "Epochs:879\tstep:100\tbatch_loss:786.9248\n",
            "Epochs:879\tstep:150\tbatch_loss:778.9523\n",
            "Epochs:879\tstep:200\tbatch_loss:771.7715\n",
            "Epochs:879\tstep:250\tbatch_loss:931.5976\n",
            "Epochs:879\tstep:300\tbatch_loss:803.0123\n",
            "Epochs:879\tstep:350\tbatch_loss:776.4729\n",
            "Epochs:879\tstep:400\tbatch_loss:778.6744\n",
            "Epochs:879\tstep:450\tbatch_loss:786.5943\n",
            "Epochs:879\tstep:500\tbatch_loss:731.4167\n",
            "Epochs:879\tstep:550\tbatch_loss:803.3392\n",
            "Epochs:879\tstep:600\tbatch_loss:741.2091\n",
            "Epoch loss: 489580.6929017754\n",
            "RMSE on dev data: 9.88201\n",
            "RMSE on train data: 10.00742\n",
            "Epochs:880\tstep:50\tbatch_loss:751.2067\n",
            "Epochs:880\tstep:100\tbatch_loss:786.8793\n",
            "Epochs:880\tstep:150\tbatch_loss:778.9077\n",
            "Epochs:880\tstep:200\tbatch_loss:771.7245\n",
            "Epochs:880\tstep:250\tbatch_loss:931.5505\n",
            "Epochs:880\tstep:300\tbatch_loss:802.9665\n",
            "Epochs:880\tstep:350\tbatch_loss:776.4278\n",
            "Epochs:880\tstep:400\tbatch_loss:778.6292\n",
            "Epochs:880\tstep:450\tbatch_loss:786.5479\n",
            "Epochs:880\tstep:500\tbatch_loss:731.3719\n",
            "Epochs:880\tstep:550\tbatch_loss:803.2935\n",
            "Epochs:880\tstep:600\tbatch_loss:741.1641\n",
            "Epoch loss: 489551.7676650807\n",
            "RMSE on dev data: 9.88197\n",
            "RMSE on train data: 10.00740\n",
            "Epochs:881\tstep:50\tbatch_loss:751.1608\n",
            "Epochs:881\tstep:100\tbatch_loss:786.8339\n",
            "Epochs:881\tstep:150\tbatch_loss:778.8629\n",
            "Epochs:881\tstep:200\tbatch_loss:771.6776\n",
            "Epochs:881\tstep:250\tbatch_loss:931.5036\n",
            "Epochs:881\tstep:300\tbatch_loss:802.9207\n",
            "Epochs:881\tstep:350\tbatch_loss:776.3827\n",
            "Epochs:881\tstep:400\tbatch_loss:778.5840\n",
            "Epochs:881\tstep:450\tbatch_loss:786.5016\n",
            "Epochs:881\tstep:500\tbatch_loss:731.3272\n",
            "Epochs:881\tstep:550\tbatch_loss:803.2480\n",
            "Epochs:881\tstep:600\tbatch_loss:741.1192\n",
            "Epoch loss: 489522.87146407226\n",
            "RMSE on dev data: 9.88194\n",
            "RMSE on train data: 10.00737\n",
            "Epochs:882\tstep:50\tbatch_loss:751.1150\n",
            "Epochs:882\tstep:100\tbatch_loss:786.7886\n",
            "Epochs:882\tstep:150\tbatch_loss:778.8183\n",
            "Epochs:882\tstep:200\tbatch_loss:771.6309\n",
            "Epochs:882\tstep:250\tbatch_loss:931.4568\n",
            "Epochs:882\tstep:300\tbatch_loss:802.8749\n",
            "Epochs:882\tstep:350\tbatch_loss:776.3377\n",
            "Epochs:882\tstep:400\tbatch_loss:778.5390\n",
            "Epochs:882\tstep:450\tbatch_loss:786.4555\n",
            "Epochs:882\tstep:500\tbatch_loss:731.2825\n",
            "Epochs:882\tstep:550\tbatch_loss:803.2025\n",
            "Epochs:882\tstep:600\tbatch_loss:741.0744\n",
            "Epoch loss: 489494.0363022318\n",
            "RMSE on dev data: 9.88191\n",
            "RMSE on train data: 10.00735\n",
            "Epochs:883\tstep:50\tbatch_loss:751.0692\n",
            "Epochs:883\tstep:100\tbatch_loss:786.7434\n",
            "Epochs:883\tstep:150\tbatch_loss:778.7738\n",
            "Epochs:883\tstep:200\tbatch_loss:771.5843\n",
            "Epochs:883\tstep:250\tbatch_loss:931.4100\n",
            "Epochs:883\tstep:300\tbatch_loss:802.8294\n",
            "Epochs:883\tstep:350\tbatch_loss:776.2928\n",
            "Epochs:883\tstep:400\tbatch_loss:778.4940\n",
            "Epochs:883\tstep:450\tbatch_loss:786.4094\n",
            "Epochs:883\tstep:500\tbatch_loss:731.2379\n",
            "Epochs:883\tstep:550\tbatch_loss:803.1571\n",
            "Epochs:883\tstep:600\tbatch_loss:741.0297\n",
            "Epoch loss: 489465.2703937379\n",
            "RMSE on dev data: 9.88188\n",
            "RMSE on train data: 10.00733\n",
            "Epochs:884\tstep:50\tbatch_loss:751.0235\n",
            "Epochs:884\tstep:100\tbatch_loss:786.6983\n",
            "Epochs:884\tstep:150\tbatch_loss:778.7293\n",
            "Epochs:884\tstep:200\tbatch_loss:771.5377\n",
            "Epochs:884\tstep:250\tbatch_loss:931.3634\n",
            "Epochs:884\tstep:300\tbatch_loss:802.7838\n",
            "Epochs:884\tstep:350\tbatch_loss:776.2479\n",
            "Epochs:884\tstep:400\tbatch_loss:778.4491\n",
            "Epochs:884\tstep:450\tbatch_loss:786.3634\n",
            "Epochs:884\tstep:500\tbatch_loss:731.1933\n",
            "Epochs:884\tstep:550\tbatch_loss:803.1117\n",
            "Epochs:884\tstep:600\tbatch_loss:740.9850\n",
            "Epoch loss: 489436.50849867414\n",
            "RMSE on dev data: 9.88185\n",
            "RMSE on train data: 10.00730\n",
            "Epochs:885\tstep:50\tbatch_loss:750.9779\n",
            "Epochs:885\tstep:100\tbatch_loss:786.6532\n",
            "Epochs:885\tstep:150\tbatch_loss:778.6849\n",
            "Epochs:885\tstep:200\tbatch_loss:771.4914\n",
            "Epochs:885\tstep:250\tbatch_loss:931.3168\n",
            "Epochs:885\tstep:300\tbatch_loss:802.7383\n",
            "Epochs:885\tstep:350\tbatch_loss:776.2030\n",
            "Epochs:885\tstep:400\tbatch_loss:778.4042\n",
            "Epochs:885\tstep:450\tbatch_loss:786.3175\n",
            "Epochs:885\tstep:500\tbatch_loss:731.1488\n",
            "Epochs:885\tstep:550\tbatch_loss:803.0664\n",
            "Epochs:885\tstep:600\tbatch_loss:740.9403\n",
            "Epoch loss: 489407.8106593999\n",
            "RMSE on dev data: 9.88182\n",
            "RMSE on train data: 10.00728\n",
            "Epochs:886\tstep:50\tbatch_loss:750.9324\n",
            "Epochs:886\tstep:100\tbatch_loss:786.6083\n",
            "Epochs:886\tstep:150\tbatch_loss:778.6405\n",
            "Epochs:886\tstep:200\tbatch_loss:771.4451\n",
            "Epochs:886\tstep:250\tbatch_loss:931.2704\n",
            "Epochs:886\tstep:300\tbatch_loss:802.6929\n",
            "Epochs:886\tstep:350\tbatch_loss:776.1582\n",
            "Epochs:886\tstep:400\tbatch_loss:778.3595\n",
            "Epochs:886\tstep:450\tbatch_loss:786.2717\n",
            "Epochs:886\tstep:500\tbatch_loss:731.1043\n",
            "Epochs:886\tstep:550\tbatch_loss:803.0211\n",
            "Epochs:886\tstep:600\tbatch_loss:740.8958\n",
            "Epoch loss: 489379.15716849786\n",
            "RMSE on dev data: 9.88179\n",
            "RMSE on train data: 10.00726\n",
            "Epochs:887\tstep:50\tbatch_loss:750.8868\n",
            "Epochs:887\tstep:100\tbatch_loss:786.5633\n",
            "Epochs:887\tstep:150\tbatch_loss:778.5962\n",
            "Epochs:887\tstep:200\tbatch_loss:771.3990\n",
            "Epochs:887\tstep:250\tbatch_loss:931.2240\n",
            "Epochs:887\tstep:300\tbatch_loss:802.6475\n",
            "Epochs:887\tstep:350\tbatch_loss:776.1134\n",
            "Epochs:887\tstep:400\tbatch_loss:778.3147\n",
            "Epochs:887\tstep:450\tbatch_loss:786.2259\n",
            "Epochs:887\tstep:500\tbatch_loss:731.0599\n",
            "Epochs:887\tstep:550\tbatch_loss:802.9760\n",
            "Epochs:887\tstep:600\tbatch_loss:740.8513\n",
            "Epoch loss: 489350.5388391918\n",
            "RMSE on dev data: 9.88176\n",
            "RMSE on train data: 10.00724\n",
            "Epochs:888\tstep:50\tbatch_loss:750.8414\n",
            "Epochs:888\tstep:100\tbatch_loss:786.5185\n",
            "Epochs:888\tstep:150\tbatch_loss:778.5519\n",
            "Epochs:888\tstep:200\tbatch_loss:771.3528\n",
            "Epochs:888\tstep:250\tbatch_loss:931.1777\n",
            "Epochs:888\tstep:300\tbatch_loss:802.6022\n",
            "Epochs:888\tstep:350\tbatch_loss:776.0687\n",
            "Epochs:888\tstep:400\tbatch_loss:778.2700\n",
            "Epochs:888\tstep:450\tbatch_loss:786.1801\n",
            "Epochs:888\tstep:500\tbatch_loss:731.0155\n",
            "Epochs:888\tstep:550\tbatch_loss:802.9308\n",
            "Epochs:888\tstep:600\tbatch_loss:740.8068\n",
            "Epoch loss: 489321.93821912736\n",
            "RMSE on dev data: 9.88174\n",
            "RMSE on train data: 10.00722\n",
            "Epochs:889\tstep:50\tbatch_loss:750.7959\n",
            "Epochs:889\tstep:100\tbatch_loss:786.4737\n",
            "Epochs:889\tstep:150\tbatch_loss:778.5077\n",
            "Epochs:889\tstep:200\tbatch_loss:771.3069\n",
            "Epochs:889\tstep:250\tbatch_loss:931.1315\n",
            "Epochs:889\tstep:300\tbatch_loss:802.5570\n",
            "Epochs:889\tstep:350\tbatch_loss:776.0240\n",
            "Epochs:889\tstep:400\tbatch_loss:778.2254\n",
            "Epochs:889\tstep:450\tbatch_loss:786.1345\n",
            "Epochs:889\tstep:500\tbatch_loss:730.9711\n",
            "Epochs:889\tstep:550\tbatch_loss:802.8857\n",
            "Epochs:889\tstep:600\tbatch_loss:740.7623\n",
            "Epoch loss: 489293.40060064656\n",
            "RMSE on dev data: 9.88171\n",
            "RMSE on train data: 10.00720\n",
            "Epochs:890\tstep:50\tbatch_loss:750.7506\n",
            "Epochs:890\tstep:100\tbatch_loss:786.4290\n",
            "Epochs:890\tstep:150\tbatch_loss:778.4635\n",
            "Epochs:890\tstep:200\tbatch_loss:771.2609\n",
            "Epochs:890\tstep:250\tbatch_loss:931.0853\n",
            "Epochs:890\tstep:300\tbatch_loss:802.5118\n",
            "Epochs:890\tstep:350\tbatch_loss:775.9794\n",
            "Epochs:890\tstep:400\tbatch_loss:778.1808\n",
            "Epochs:890\tstep:450\tbatch_loss:786.0889\n",
            "Epochs:890\tstep:500\tbatch_loss:730.9268\n",
            "Epochs:890\tstep:550\tbatch_loss:802.8407\n",
            "Epochs:890\tstep:600\tbatch_loss:740.7179\n",
            "Epoch loss: 489264.8705183916\n",
            "RMSE on dev data: 9.88168\n",
            "RMSE on train data: 10.00718\n",
            "Epochs:891\tstep:50\tbatch_loss:750.7053\n",
            "Epochs:891\tstep:100\tbatch_loss:786.3843\n",
            "Epochs:891\tstep:150\tbatch_loss:778.4194\n",
            "Epochs:891\tstep:200\tbatch_loss:771.2151\n",
            "Epochs:891\tstep:250\tbatch_loss:931.0392\n",
            "Epochs:891\tstep:300\tbatch_loss:802.4666\n",
            "Epochs:891\tstep:350\tbatch_loss:775.9348\n",
            "Epochs:891\tstep:400\tbatch_loss:778.1362\n",
            "Epochs:891\tstep:450\tbatch_loss:786.0434\n",
            "Epochs:891\tstep:500\tbatch_loss:730.8826\n",
            "Epochs:891\tstep:550\tbatch_loss:802.7957\n",
            "Epochs:891\tstep:600\tbatch_loss:740.6736\n",
            "Epoch loss: 489236.3999494486\n",
            "RMSE on dev data: 9.88166\n",
            "RMSE on train data: 10.00716\n",
            "Epochs:892\tstep:50\tbatch_loss:750.6601\n",
            "Epochs:892\tstep:100\tbatch_loss:786.3397\n",
            "Epochs:892\tstep:150\tbatch_loss:778.3754\n",
            "Epochs:892\tstep:200\tbatch_loss:771.1694\n",
            "Epochs:892\tstep:250\tbatch_loss:930.9933\n",
            "Epochs:892\tstep:300\tbatch_loss:802.4216\n",
            "Epochs:892\tstep:350\tbatch_loss:775.8902\n",
            "Epochs:892\tstep:400\tbatch_loss:778.0918\n",
            "Epochs:892\tstep:450\tbatch_loss:785.9980\n",
            "Epochs:892\tstep:500\tbatch_loss:730.8384\n",
            "Epochs:892\tstep:550\tbatch_loss:802.7508\n",
            "Epochs:892\tstep:600\tbatch_loss:740.6294\n",
            "Epoch loss: 489207.9645561436\n",
            "RMSE on dev data: 9.88163\n",
            "RMSE on train data: 10.00715\n",
            "Epochs:893\tstep:50\tbatch_loss:750.6149\n",
            "Epochs:893\tstep:100\tbatch_loss:786.2951\n",
            "Epochs:893\tstep:150\tbatch_loss:778.3313\n",
            "Epochs:893\tstep:200\tbatch_loss:771.1237\n",
            "Epochs:893\tstep:250\tbatch_loss:930.9473\n",
            "Epochs:893\tstep:300\tbatch_loss:802.3765\n",
            "Epochs:893\tstep:350\tbatch_loss:775.8457\n",
            "Epochs:893\tstep:400\tbatch_loss:778.0473\n",
            "Epochs:893\tstep:450\tbatch_loss:785.9526\n",
            "Epochs:893\tstep:500\tbatch_loss:730.7942\n",
            "Epochs:893\tstep:550\tbatch_loss:802.7059\n",
            "Epochs:893\tstep:600\tbatch_loss:740.5851\n",
            "Epoch loss: 489179.5338262037\n",
            "RMSE on dev data: 9.88161\n",
            "RMSE on train data: 10.00713\n",
            "Epochs:894\tstep:50\tbatch_loss:750.5697\n",
            "Epochs:894\tstep:100\tbatch_loss:786.2506\n",
            "Epochs:894\tstep:150\tbatch_loss:778.2874\n",
            "Epochs:894\tstep:200\tbatch_loss:771.0782\n",
            "Epochs:894\tstep:250\tbatch_loss:930.9014\n",
            "Epochs:894\tstep:300\tbatch_loss:802.3316\n",
            "Epochs:894\tstep:350\tbatch_loss:775.8013\n",
            "Epochs:894\tstep:400\tbatch_loss:778.0029\n",
            "Epochs:894\tstep:450\tbatch_loss:785.9072\n",
            "Epochs:894\tstep:500\tbatch_loss:730.7500\n",
            "Epochs:894\tstep:550\tbatch_loss:802.6611\n",
            "Epochs:894\tstep:600\tbatch_loss:740.5409\n",
            "Epoch loss: 489151.15520157554\n",
            "RMSE on dev data: 9.88159\n",
            "RMSE on train data: 10.00711\n",
            "Epochs:895\tstep:50\tbatch_loss:750.5246\n",
            "Epochs:895\tstep:100\tbatch_loss:786.2062\n",
            "Epochs:895\tstep:150\tbatch_loss:778.2434\n",
            "Epochs:895\tstep:200\tbatch_loss:771.0327\n",
            "Epochs:895\tstep:250\tbatch_loss:930.8557\n",
            "Epochs:895\tstep:300\tbatch_loss:802.2866\n",
            "Epochs:895\tstep:350\tbatch_loss:775.7568\n",
            "Epochs:895\tstep:400\tbatch_loss:777.9585\n",
            "Epochs:895\tstep:450\tbatch_loss:785.8619\n",
            "Epochs:895\tstep:500\tbatch_loss:730.7059\n",
            "Epochs:895\tstep:550\tbatch_loss:802.6163\n",
            "Epochs:895\tstep:600\tbatch_loss:740.4968\n",
            "Epoch loss: 489122.79201984406\n",
            "RMSE on dev data: 9.88156\n",
            "RMSE on train data: 10.00709\n",
            "Epochs:896\tstep:50\tbatch_loss:750.4796\n",
            "Epochs:896\tstep:100\tbatch_loss:786.1618\n",
            "Epochs:896\tstep:150\tbatch_loss:778.1996\n",
            "Epochs:896\tstep:200\tbatch_loss:770.9873\n",
            "Epochs:896\tstep:250\tbatch_loss:930.8099\n",
            "Epochs:896\tstep:300\tbatch_loss:802.2418\n",
            "Epochs:896\tstep:350\tbatch_loss:775.7124\n",
            "Epochs:896\tstep:400\tbatch_loss:777.9142\n",
            "Epochs:896\tstep:450\tbatch_loss:785.8167\n",
            "Epochs:896\tstep:500\tbatch_loss:730.6618\n",
            "Epochs:896\tstep:550\tbatch_loss:802.5716\n",
            "Epochs:896\tstep:600\tbatch_loss:740.4527\n",
            "Epoch loss: 489094.4720439295\n",
            "RMSE on dev data: 9.88154\n",
            "RMSE on train data: 10.00708\n",
            "Epochs:897\tstep:50\tbatch_loss:750.4346\n",
            "Epochs:897\tstep:100\tbatch_loss:786.1175\n",
            "Epochs:897\tstep:150\tbatch_loss:778.1557\n",
            "Epochs:897\tstep:200\tbatch_loss:770.9419\n",
            "Epochs:897\tstep:250\tbatch_loss:930.7643\n",
            "Epochs:897\tstep:300\tbatch_loss:802.1970\n",
            "Epochs:897\tstep:350\tbatch_loss:775.6681\n",
            "Epochs:897\tstep:400\tbatch_loss:777.8699\n",
            "Epochs:897\tstep:450\tbatch_loss:785.7715\n",
            "Epochs:897\tstep:500\tbatch_loss:730.6178\n",
            "Epochs:897\tstep:550\tbatch_loss:802.5269\n",
            "Epochs:897\tstep:600\tbatch_loss:740.4086\n",
            "Epoch loss: 489066.17638944724\n",
            "RMSE on dev data: 9.88152\n",
            "RMSE on train data: 10.00706\n",
            "Epochs:898\tstep:50\tbatch_loss:750.3896\n",
            "Epochs:898\tstep:100\tbatch_loss:786.0732\n",
            "Epochs:898\tstep:150\tbatch_loss:778.1118\n",
            "Epochs:898\tstep:200\tbatch_loss:770.8967\n",
            "Epochs:898\tstep:250\tbatch_loss:930.7187\n",
            "Epochs:898\tstep:300\tbatch_loss:802.1522\n",
            "Epochs:898\tstep:350\tbatch_loss:775.6237\n",
            "Epochs:898\tstep:400\tbatch_loss:777.8257\n",
            "Epochs:898\tstep:450\tbatch_loss:785.7264\n",
            "Epochs:898\tstep:500\tbatch_loss:730.5738\n",
            "Epochs:898\tstep:550\tbatch_loss:802.4822\n",
            "Epochs:898\tstep:600\tbatch_loss:740.3645\n",
            "Epoch loss: 489037.9029298967\n",
            "RMSE on dev data: 9.88150\n",
            "RMSE on train data: 10.00705\n",
            "Epochs:899\tstep:50\tbatch_loss:750.3447\n",
            "Epochs:899\tstep:100\tbatch_loss:786.0289\n",
            "Epochs:899\tstep:150\tbatch_loss:778.0681\n",
            "Epochs:899\tstep:200\tbatch_loss:770.8515\n",
            "Epochs:899\tstep:250\tbatch_loss:930.6731\n",
            "Epochs:899\tstep:300\tbatch_loss:802.1074\n",
            "Epochs:899\tstep:350\tbatch_loss:775.5794\n",
            "Epochs:899\tstep:400\tbatch_loss:777.7815\n",
            "Epochs:899\tstep:450\tbatch_loss:785.6813\n",
            "Epochs:899\tstep:500\tbatch_loss:730.5298\n",
            "Epochs:899\tstep:550\tbatch_loss:802.4376\n",
            "Epochs:899\tstep:600\tbatch_loss:740.3206\n",
            "Epoch loss: 489009.6632787722\n",
            "RMSE on dev data: 9.88148\n",
            "RMSE on train data: 10.00703\n",
            "Epochs:900\tstep:50\tbatch_loss:750.2998\n",
            "Epochs:900\tstep:100\tbatch_loss:785.9847\n",
            "Epochs:900\tstep:150\tbatch_loss:778.0243\n",
            "Epochs:900\tstep:200\tbatch_loss:770.8063\n",
            "Epochs:900\tstep:250\tbatch_loss:930.6276\n",
            "Epochs:900\tstep:300\tbatch_loss:802.0627\n",
            "Epochs:900\tstep:350\tbatch_loss:775.5352\n",
            "Epochs:900\tstep:400\tbatch_loss:777.7373\n",
            "Epochs:900\tstep:450\tbatch_loss:785.6362\n",
            "Epochs:900\tstep:500\tbatch_loss:730.4859\n",
            "Epochs:900\tstep:550\tbatch_loss:802.3930\n",
            "Epochs:900\tstep:600\tbatch_loss:740.2766\n",
            "Epoch loss: 488981.4281836231\n",
            "RMSE on dev data: 9.88146\n",
            "RMSE on train data: 10.00702\n",
            "Epochs:901\tstep:50\tbatch_loss:750.1872\n",
            "Epochs:901\tstep:100\tbatch_loss:786.0105\n",
            "Epochs:901\tstep:150\tbatch_loss:777.7487\n",
            "Epochs:901\tstep:200\tbatch_loss:770.5071\n",
            "Epochs:901\tstep:250\tbatch_loss:930.7199\n",
            "Epochs:901\tstep:300\tbatch_loss:802.1224\n",
            "Epochs:901\tstep:350\tbatch_loss:775.4298\n",
            "Epochs:901\tstep:400\tbatch_loss:777.7518\n",
            "Epochs:901\tstep:450\tbatch_loss:785.6459\n",
            "Epochs:901\tstep:500\tbatch_loss:730.6332\n",
            "Epochs:901\tstep:550\tbatch_loss:801.8371\n",
            "Epochs:901\tstep:600\tbatch_loss:740.4815\n",
            "Epoch loss: 488906.9971540201\n",
            "RMSE on dev data: 9.88137\n",
            "RMSE on train data: 10.00716\n",
            "Epochs:902\tstep:50\tbatch_loss:750.1325\n",
            "Epochs:902\tstep:100\tbatch_loss:785.9751\n",
            "Epochs:902\tstep:150\tbatch_loss:777.7055\n",
            "Epochs:902\tstep:200\tbatch_loss:770.4596\n",
            "Epochs:902\tstep:250\tbatch_loss:930.6825\n",
            "Epochs:902\tstep:300\tbatch_loss:802.0799\n",
            "Epochs:902\tstep:350\tbatch_loss:775.3900\n",
            "Epochs:902\tstep:400\tbatch_loss:777.7106\n",
            "Epochs:902\tstep:450\tbatch_loss:785.6038\n",
            "Epochs:902\tstep:500\tbatch_loss:730.5925\n",
            "Epochs:902\tstep:550\tbatch_loss:801.7966\n",
            "Epochs:902\tstep:600\tbatch_loss:740.4408\n",
            "Epoch loss: 488880.1739405331\n",
            "RMSE on dev data: 9.88133\n",
            "RMSE on train data: 10.00713\n",
            "Epochs:903\tstep:50\tbatch_loss:750.0910\n",
            "Epochs:903\tstep:100\tbatch_loss:785.9342\n",
            "Epochs:903\tstep:150\tbatch_loss:777.6653\n",
            "Epochs:903\tstep:200\tbatch_loss:770.4171\n",
            "Epochs:903\tstep:250\tbatch_loss:930.6400\n",
            "Epochs:903\tstep:300\tbatch_loss:802.0387\n",
            "Epochs:903\tstep:350\tbatch_loss:775.3495\n",
            "Epochs:903\tstep:400\tbatch_loss:777.6699\n",
            "Epochs:903\tstep:450\tbatch_loss:785.5621\n",
            "Epochs:903\tstep:500\tbatch_loss:730.5523\n",
            "Epochs:903\tstep:550\tbatch_loss:801.7555\n",
            "Epochs:903\tstep:600\tbatch_loss:740.4005\n",
            "Epoch loss: 488854.162767874\n",
            "RMSE on dev data: 9.88130\n",
            "RMSE on train data: 10.00710\n",
            "Epochs:904\tstep:50\tbatch_loss:750.0497\n",
            "Epochs:904\tstep:100\tbatch_loss:785.8933\n",
            "Epochs:904\tstep:150\tbatch_loss:777.6250\n",
            "Epochs:904\tstep:200\tbatch_loss:770.3746\n",
            "Epochs:904\tstep:250\tbatch_loss:930.5977\n",
            "Epochs:904\tstep:300\tbatch_loss:801.9974\n",
            "Epochs:904\tstep:350\tbatch_loss:775.3090\n",
            "Epochs:904\tstep:400\tbatch_loss:777.6293\n",
            "Epochs:904\tstep:450\tbatch_loss:785.5204\n",
            "Epochs:904\tstep:500\tbatch_loss:730.5121\n",
            "Epochs:904\tstep:550\tbatch_loss:801.7145\n",
            "Epochs:904\tstep:600\tbatch_loss:740.3601\n",
            "Epoch loss: 488828.1671790715\n",
            "RMSE on dev data: 9.88127\n",
            "RMSE on train data: 10.00708\n",
            "Epochs:905\tstep:50\tbatch_loss:750.0085\n",
            "Epochs:905\tstep:100\tbatch_loss:785.8525\n",
            "Epochs:905\tstep:150\tbatch_loss:777.5850\n",
            "Epochs:905\tstep:200\tbatch_loss:770.3323\n",
            "Epochs:905\tstep:250\tbatch_loss:930.5555\n",
            "Epochs:905\tstep:300\tbatch_loss:801.9562\n",
            "Epochs:905\tstep:350\tbatch_loss:775.2685\n",
            "Epochs:905\tstep:400\tbatch_loss:777.5889\n",
            "Epochs:905\tstep:450\tbatch_loss:785.4789\n",
            "Epochs:905\tstep:500\tbatch_loss:730.4720\n",
            "Epochs:905\tstep:550\tbatch_loss:801.6736\n",
            "Epochs:905\tstep:600\tbatch_loss:740.3198\n",
            "Epoch loss: 488802.24190056254\n",
            "RMSE on dev data: 9.88123\n",
            "RMSE on train data: 10.00705\n",
            "Epochs:906\tstep:50\tbatch_loss:749.9673\n",
            "Epochs:906\tstep:100\tbatch_loss:785.8118\n",
            "Epochs:906\tstep:150\tbatch_loss:777.5448\n",
            "Epochs:906\tstep:200\tbatch_loss:770.2900\n",
            "Epochs:906\tstep:250\tbatch_loss:930.5134\n",
            "Epochs:906\tstep:300\tbatch_loss:801.9151\n",
            "Epochs:906\tstep:350\tbatch_loss:775.2281\n",
            "Epochs:906\tstep:400\tbatch_loss:777.5484\n",
            "Epochs:906\tstep:450\tbatch_loss:785.4373\n",
            "Epochs:906\tstep:500\tbatch_loss:730.4319\n",
            "Epochs:906\tstep:550\tbatch_loss:801.6327\n",
            "Epochs:906\tstep:600\tbatch_loss:740.2796\n",
            "Epoch loss: 488776.33406462043\n",
            "RMSE on dev data: 9.88120\n",
            "RMSE on train data: 10.00703\n",
            "Epochs:907\tstep:50\tbatch_loss:749.9262\n",
            "Epochs:907\tstep:100\tbatch_loss:785.7711\n",
            "Epochs:907\tstep:150\tbatch_loss:777.5049\n",
            "Epochs:907\tstep:200\tbatch_loss:770.2479\n",
            "Epochs:907\tstep:250\tbatch_loss:930.4712\n",
            "Epochs:907\tstep:300\tbatch_loss:801.8740\n",
            "Epochs:907\tstep:350\tbatch_loss:775.1877\n",
            "Epochs:907\tstep:400\tbatch_loss:777.5080\n",
            "Epochs:907\tstep:450\tbatch_loss:785.3959\n",
            "Epochs:907\tstep:500\tbatch_loss:730.3919\n",
            "Epochs:907\tstep:550\tbatch_loss:801.5919\n",
            "Epochs:907\tstep:600\tbatch_loss:740.2395\n",
            "Epoch loss: 488750.4869233935\n",
            "RMSE on dev data: 9.88117\n",
            "RMSE on train data: 10.00700\n",
            "Epochs:908\tstep:50\tbatch_loss:749.8851\n",
            "Epochs:908\tstep:100\tbatch_loss:785.7305\n",
            "Epochs:908\tstep:150\tbatch_loss:777.4649\n",
            "Epochs:908\tstep:200\tbatch_loss:770.2059\n",
            "Epochs:908\tstep:250\tbatch_loss:930.4294\n",
            "Epochs:908\tstep:300\tbatch_loss:801.8330\n",
            "Epochs:908\tstep:350\tbatch_loss:775.1474\n",
            "Epochs:908\tstep:400\tbatch_loss:777.4677\n",
            "Epochs:908\tstep:450\tbatch_loss:785.3545\n",
            "Epochs:908\tstep:500\tbatch_loss:730.3519\n",
            "Epochs:908\tstep:550\tbatch_loss:801.5511\n",
            "Epochs:908\tstep:600\tbatch_loss:740.1994\n",
            "Epoch loss: 488724.6674423089\n",
            "RMSE on dev data: 9.88114\n",
            "RMSE on train data: 10.00698\n",
            "Epochs:909\tstep:50\tbatch_loss:749.8441\n",
            "Epochs:909\tstep:100\tbatch_loss:785.6900\n",
            "Epochs:909\tstep:150\tbatch_loss:777.4250\n",
            "Epochs:909\tstep:200\tbatch_loss:770.1640\n",
            "Epochs:909\tstep:250\tbatch_loss:930.3874\n",
            "Epochs:909\tstep:300\tbatch_loss:801.7921\n",
            "Epochs:909\tstep:350\tbatch_loss:775.1072\n",
            "Epochs:909\tstep:400\tbatch_loss:777.4274\n",
            "Epochs:909\tstep:450\tbatch_loss:785.3132\n",
            "Epochs:909\tstep:500\tbatch_loss:730.3119\n",
            "Epochs:909\tstep:550\tbatch_loss:801.5104\n",
            "Epochs:909\tstep:600\tbatch_loss:740.1593\n",
            "Epoch loss: 488698.89860874735\n",
            "RMSE on dev data: 9.88111\n",
            "RMSE on train data: 10.00696\n",
            "Epochs:910\tstep:50\tbatch_loss:749.8032\n",
            "Epochs:910\tstep:100\tbatch_loss:785.6496\n",
            "Epochs:910\tstep:150\tbatch_loss:777.3851\n",
            "Epochs:910\tstep:200\tbatch_loss:770.1222\n",
            "Epochs:910\tstep:250\tbatch_loss:930.3457\n",
            "Epochs:910\tstep:300\tbatch_loss:801.7512\n",
            "Epochs:910\tstep:350\tbatch_loss:775.0670\n",
            "Epochs:910\tstep:400\tbatch_loss:777.3872\n",
            "Epochs:910\tstep:450\tbatch_loss:785.2720\n",
            "Epochs:910\tstep:500\tbatch_loss:730.2720\n",
            "Epochs:910\tstep:550\tbatch_loss:801.4698\n",
            "Epochs:910\tstep:600\tbatch_loss:740.1193\n",
            "Epoch loss: 488673.1648173633\n",
            "RMSE on dev data: 9.88108\n",
            "RMSE on train data: 10.00694\n",
            "Epochs:911\tstep:50\tbatch_loss:749.7623\n",
            "Epochs:911\tstep:100\tbatch_loss:785.6092\n",
            "Epochs:911\tstep:150\tbatch_loss:777.3453\n",
            "Epochs:911\tstep:200\tbatch_loss:770.0805\n",
            "Epochs:911\tstep:250\tbatch_loss:930.3040\n",
            "Epochs:911\tstep:300\tbatch_loss:801.7104\n",
            "Epochs:911\tstep:350\tbatch_loss:775.0268\n",
            "Epochs:911\tstep:400\tbatch_loss:777.3470\n",
            "Epochs:911\tstep:450\tbatch_loss:785.2309\n",
            "Epochs:911\tstep:500\tbatch_loss:730.2322\n",
            "Epochs:911\tstep:550\tbatch_loss:801.4292\n",
            "Epochs:911\tstep:600\tbatch_loss:740.0794\n",
            "Epoch loss: 488647.4638719661\n",
            "RMSE on dev data: 9.88105\n",
            "RMSE on train data: 10.00692\n",
            "Epochs:912\tstep:50\tbatch_loss:749.7215\n",
            "Epochs:912\tstep:100\tbatch_loss:785.5689\n",
            "Epochs:912\tstep:150\tbatch_loss:777.3056\n",
            "Epochs:912\tstep:200\tbatch_loss:770.0389\n",
            "Epochs:912\tstep:250\tbatch_loss:930.2623\n",
            "Epochs:912\tstep:300\tbatch_loss:801.6697\n",
            "Epochs:912\tstep:350\tbatch_loss:774.9867\n",
            "Epochs:912\tstep:400\tbatch_loss:777.3069\n",
            "Epochs:912\tstep:450\tbatch_loss:785.1898\n",
            "Epochs:912\tstep:500\tbatch_loss:730.1924\n",
            "Epochs:912\tstep:550\tbatch_loss:801.3886\n",
            "Epochs:912\tstep:600\tbatch_loss:740.0395\n",
            "Epoch loss: 488621.8038972386\n",
            "RMSE on dev data: 9.88102\n",
            "RMSE on train data: 10.00689\n",
            "Epochs:913\tstep:50\tbatch_loss:749.6807\n",
            "Epochs:913\tstep:100\tbatch_loss:785.5286\n",
            "Epochs:913\tstep:150\tbatch_loss:777.2659\n",
            "Epochs:913\tstep:200\tbatch_loss:769.9973\n",
            "Epochs:913\tstep:250\tbatch_loss:930.2207\n",
            "Epochs:913\tstep:300\tbatch_loss:801.6290\n",
            "Epochs:913\tstep:350\tbatch_loss:774.9466\n",
            "Epochs:913\tstep:400\tbatch_loss:777.2668\n",
            "Epochs:913\tstep:450\tbatch_loss:785.1487\n",
            "Epochs:913\tstep:500\tbatch_loss:730.1526\n",
            "Epochs:913\tstep:550\tbatch_loss:801.3481\n",
            "Epochs:913\tstep:600\tbatch_loss:739.9996\n",
            "Epoch loss: 488596.15941742854\n",
            "RMSE on dev data: 9.88099\n",
            "RMSE on train data: 10.00687\n",
            "Epochs:914\tstep:50\tbatch_loss:749.6400\n",
            "Epochs:914\tstep:100\tbatch_loss:785.4884\n",
            "Epochs:914\tstep:150\tbatch_loss:777.2263\n",
            "Epochs:914\tstep:200\tbatch_loss:769.9560\n",
            "Epochs:914\tstep:250\tbatch_loss:930.1792\n",
            "Epochs:914\tstep:300\tbatch_loss:801.5884\n",
            "Epochs:914\tstep:350\tbatch_loss:774.9065\n",
            "Epochs:914\tstep:400\tbatch_loss:777.2268\n",
            "Epochs:914\tstep:450\tbatch_loss:785.1078\n",
            "Epochs:914\tstep:500\tbatch_loss:730.1129\n",
            "Epochs:914\tstep:550\tbatch_loss:801.3077\n",
            "Epochs:914\tstep:600\tbatch_loss:739.9598\n",
            "Epoch loss: 488570.57195246837\n",
            "RMSE on dev data: 9.88097\n",
            "RMSE on train data: 10.00685\n",
            "Epochs:915\tstep:50\tbatch_loss:749.5993\n",
            "Epochs:915\tstep:100\tbatch_loss:785.4483\n",
            "Epochs:915\tstep:150\tbatch_loss:777.1866\n",
            "Epochs:915\tstep:200\tbatch_loss:769.9145\n",
            "Epochs:915\tstep:250\tbatch_loss:930.1378\n",
            "Epochs:915\tstep:300\tbatch_loss:801.5478\n",
            "Epochs:915\tstep:350\tbatch_loss:774.8665\n",
            "Epochs:915\tstep:400\tbatch_loss:777.1868\n",
            "Epochs:915\tstep:450\tbatch_loss:785.0668\n",
            "Epochs:915\tstep:500\tbatch_loss:730.0732\n",
            "Epochs:915\tstep:550\tbatch_loss:801.2672\n",
            "Epochs:915\tstep:600\tbatch_loss:739.9200\n",
            "Epoch loss: 488544.99207923235\n",
            "RMSE on dev data: 9.88094\n",
            "RMSE on train data: 10.00684\n",
            "Epochs:916\tstep:50\tbatch_loss:749.5587\n",
            "Epochs:916\tstep:100\tbatch_loss:785.4082\n",
            "Epochs:916\tstep:150\tbatch_loss:777.1471\n",
            "Epochs:916\tstep:200\tbatch_loss:769.8733\n",
            "Epochs:916\tstep:250\tbatch_loss:930.0964\n",
            "Epochs:916\tstep:300\tbatch_loss:801.5073\n",
            "Epochs:916\tstep:350\tbatch_loss:774.8265\n",
            "Epochs:916\tstep:400\tbatch_loss:777.1468\n",
            "Epochs:916\tstep:450\tbatch_loss:785.0260\n",
            "Epochs:916\tstep:500\tbatch_loss:730.0335\n",
            "Epochs:916\tstep:550\tbatch_loss:801.2269\n",
            "Epochs:916\tstep:600\tbatch_loss:739.8803\n",
            "Epoch loss: 488519.4614082843\n",
            "RMSE on dev data: 9.88091\n",
            "RMSE on train data: 10.00682\n",
            "Epochs:917\tstep:50\tbatch_loss:749.5181\n",
            "Epochs:917\tstep:100\tbatch_loss:785.3682\n",
            "Epochs:917\tstep:150\tbatch_loss:777.1076\n",
            "Epochs:917\tstep:200\tbatch_loss:769.8321\n",
            "Epochs:917\tstep:250\tbatch_loss:930.0551\n",
            "Epochs:917\tstep:300\tbatch_loss:801.4668\n",
            "Epochs:917\tstep:350\tbatch_loss:774.7866\n",
            "Epochs:917\tstep:400\tbatch_loss:777.1069\n",
            "Epochs:917\tstep:450\tbatch_loss:784.9852\n",
            "Epochs:917\tstep:500\tbatch_loss:729.9939\n",
            "Epochs:917\tstep:550\tbatch_loss:801.1866\n",
            "Epochs:917\tstep:600\tbatch_loss:739.8407\n",
            "Epoch loss: 488493.95761168463\n",
            "RMSE on dev data: 9.88089\n",
            "RMSE on train data: 10.00680\n",
            "Epochs:918\tstep:50\tbatch_loss:749.4776\n",
            "Epochs:918\tstep:100\tbatch_loss:785.3282\n",
            "Epochs:918\tstep:150\tbatch_loss:777.0681\n",
            "Epochs:918\tstep:200\tbatch_loss:769.7911\n",
            "Epochs:918\tstep:250\tbatch_loss:930.0139\n",
            "Epochs:918\tstep:300\tbatch_loss:801.4264\n",
            "Epochs:918\tstep:350\tbatch_loss:774.7467\n",
            "Epochs:918\tstep:400\tbatch_loss:777.0671\n",
            "Epochs:918\tstep:450\tbatch_loss:784.9445\n",
            "Epochs:918\tstep:500\tbatch_loss:729.9543\n",
            "Epochs:918\tstep:550\tbatch_loss:801.1463\n",
            "Epochs:918\tstep:600\tbatch_loss:739.8010\n",
            "Epoch loss: 488468.49111328385\n",
            "RMSE on dev data: 9.88086\n",
            "RMSE on train data: 10.00678\n",
            "Epochs:919\tstep:50\tbatch_loss:749.4371\n",
            "Epochs:919\tstep:100\tbatch_loss:785.2883\n",
            "Epochs:919\tstep:150\tbatch_loss:777.0286\n",
            "Epochs:919\tstep:200\tbatch_loss:769.7500\n",
            "Epochs:919\tstep:250\tbatch_loss:929.9728\n",
            "Epochs:919\tstep:300\tbatch_loss:801.3859\n",
            "Epochs:919\tstep:350\tbatch_loss:774.7068\n",
            "Epochs:919\tstep:400\tbatch_loss:777.0273\n",
            "Epochs:919\tstep:450\tbatch_loss:784.9038\n",
            "Epochs:919\tstep:500\tbatch_loss:729.9148\n",
            "Epochs:919\tstep:550\tbatch_loss:801.1061\n",
            "Epochs:919\tstep:600\tbatch_loss:739.7614\n",
            "Epoch loss: 488443.0277104542\n",
            "RMSE on dev data: 9.88084\n",
            "RMSE on train data: 10.00676\n",
            "Epochs:920\tstep:50\tbatch_loss:749.3966\n",
            "Epochs:920\tstep:100\tbatch_loss:785.2484\n",
            "Epochs:920\tstep:150\tbatch_loss:776.9892\n",
            "Epochs:920\tstep:200\tbatch_loss:769.7090\n",
            "Epochs:920\tstep:250\tbatch_loss:929.9316\n",
            "Epochs:920\tstep:300\tbatch_loss:801.3456\n",
            "Epochs:920\tstep:350\tbatch_loss:774.6670\n",
            "Epochs:920\tstep:400\tbatch_loss:776.9875\n",
            "Epochs:920\tstep:450\tbatch_loss:784.8631\n",
            "Epochs:920\tstep:500\tbatch_loss:729.8752\n",
            "Epochs:920\tstep:550\tbatch_loss:801.0659\n",
            "Epochs:920\tstep:600\tbatch_loss:739.7219\n",
            "Epoch loss: 488417.59877677547\n",
            "RMSE on dev data: 9.88082\n",
            "RMSE on train data: 10.00674\n",
            "Epochs:921\tstep:50\tbatch_loss:749.3562\n",
            "Epochs:921\tstep:100\tbatch_loss:785.2086\n",
            "Epochs:921\tstep:150\tbatch_loss:776.9499\n",
            "Epochs:921\tstep:200\tbatch_loss:769.6681\n",
            "Epochs:921\tstep:250\tbatch_loss:929.8906\n",
            "Epochs:921\tstep:300\tbatch_loss:801.3053\n",
            "Epochs:921\tstep:350\tbatch_loss:774.6272\n",
            "Epochs:921\tstep:400\tbatch_loss:776.9478\n",
            "Epochs:921\tstep:450\tbatch_loss:784.8225\n",
            "Epochs:921\tstep:500\tbatch_loss:729.8358\n",
            "Epochs:921\tstep:550\tbatch_loss:801.0257\n",
            "Epochs:921\tstep:600\tbatch_loss:739.6823\n",
            "Epoch loss: 488392.2138435345\n",
            "RMSE on dev data: 9.88079\n",
            "RMSE on train data: 10.00673\n",
            "Epochs:922\tstep:50\tbatch_loss:749.3158\n",
            "Epochs:922\tstep:100\tbatch_loss:785.1688\n",
            "Epochs:922\tstep:150\tbatch_loss:776.9105\n",
            "Epochs:922\tstep:200\tbatch_loss:769.6272\n",
            "Epochs:922\tstep:250\tbatch_loss:929.8496\n",
            "Epochs:922\tstep:300\tbatch_loss:801.2650\n",
            "Epochs:922\tstep:350\tbatch_loss:774.5874\n",
            "Epochs:922\tstep:400\tbatch_loss:776.9081\n",
            "Epochs:922\tstep:450\tbatch_loss:784.7820\n",
            "Epochs:922\tstep:500\tbatch_loss:729.7963\n",
            "Epochs:922\tstep:550\tbatch_loss:800.9856\n",
            "Epochs:922\tstep:600\tbatch_loss:739.6428\n",
            "Epoch loss: 488366.83034306567\n",
            "RMSE on dev data: 9.88077\n",
            "RMSE on train data: 10.00671\n",
            "Epochs:923\tstep:50\tbatch_loss:749.2755\n",
            "Epochs:923\tstep:100\tbatch_loss:785.1290\n",
            "Epochs:923\tstep:150\tbatch_loss:776.8713\n",
            "Epochs:923\tstep:200\tbatch_loss:769.5865\n",
            "Epochs:923\tstep:250\tbatch_loss:929.8086\n",
            "Epochs:923\tstep:300\tbatch_loss:801.2248\n",
            "Epochs:923\tstep:350\tbatch_loss:774.5477\n",
            "Epochs:923\tstep:400\tbatch_loss:776.8684\n",
            "Epochs:923\tstep:450\tbatch_loss:784.7415\n",
            "Epochs:923\tstep:500\tbatch_loss:729.7569\n",
            "Epochs:923\tstep:550\tbatch_loss:800.9456\n",
            "Epochs:923\tstep:600\tbatch_loss:739.6034\n",
            "Epoch loss: 488341.4908120189\n",
            "RMSE on dev data: 9.88075\n",
            "RMSE on train data: 10.00669\n",
            "Epochs:924\tstep:50\tbatch_loss:749.2352\n",
            "Epochs:924\tstep:100\tbatch_loss:785.0894\n",
            "Epochs:924\tstep:150\tbatch_loss:776.8320\n",
            "Epochs:924\tstep:200\tbatch_loss:769.5458\n",
            "Epochs:924\tstep:250\tbatch_loss:929.7678\n",
            "Epochs:924\tstep:300\tbatch_loss:801.1847\n",
            "Epochs:924\tstep:350\tbatch_loss:774.5080\n",
            "Epochs:924\tstep:400\tbatch_loss:776.8288\n",
            "Epochs:924\tstep:450\tbatch_loss:784.7010\n",
            "Epochs:924\tstep:500\tbatch_loss:729.7175\n",
            "Epochs:924\tstep:550\tbatch_loss:800.9056\n",
            "Epochs:924\tstep:600\tbatch_loss:739.5640\n",
            "Epoch loss: 488316.1778183986\n",
            "RMSE on dev data: 9.88073\n",
            "RMSE on train data: 10.00668\n",
            "Epochs:925\tstep:50\tbatch_loss:749.1949\n",
            "Epochs:925\tstep:100\tbatch_loss:785.0497\n",
            "Epochs:925\tstep:150\tbatch_loss:776.7928\n",
            "Epochs:925\tstep:200\tbatch_loss:769.5052\n",
            "Epochs:925\tstep:250\tbatch_loss:929.7269\n",
            "Epochs:925\tstep:300\tbatch_loss:801.1446\n",
            "Epochs:925\tstep:350\tbatch_loss:774.4683\n",
            "Epochs:925\tstep:400\tbatch_loss:776.7892\n",
            "Epochs:925\tstep:450\tbatch_loss:784.6606\n",
            "Epochs:925\tstep:500\tbatch_loss:729.6781\n",
            "Epochs:925\tstep:550\tbatch_loss:800.8656\n",
            "Epochs:925\tstep:600\tbatch_loss:739.5246\n",
            "Epoch loss: 488290.8844741789\n",
            "RMSE on dev data: 9.88070\n",
            "RMSE on train data: 10.00666\n",
            "Epochs:926\tstep:50\tbatch_loss:749.0901\n",
            "Epochs:926\tstep:100\tbatch_loss:785.0829\n",
            "Epochs:926\tstep:150\tbatch_loss:776.5772\n",
            "Epochs:926\tstep:200\tbatch_loss:769.2070\n",
            "Epochs:926\tstep:250\tbatch_loss:929.8727\n",
            "Epochs:926\tstep:300\tbatch_loss:801.1978\n",
            "Epochs:926\tstep:350\tbatch_loss:774.3765\n",
            "Epochs:926\tstep:400\tbatch_loss:776.8060\n",
            "Epochs:926\tstep:450\tbatch_loss:784.6805\n",
            "Epochs:926\tstep:500\tbatch_loss:729.8366\n",
            "Epochs:926\tstep:550\tbatch_loss:800.3099\n",
            "Epochs:926\tstep:600\tbatch_loss:739.7446\n",
            "Epoch loss: 488222.1831219937\n",
            "RMSE on dev data: 9.88067\n",
            "RMSE on train data: 10.00664\n",
            "Epochs:927\tstep:50\tbatch_loss:749.0527\n",
            "Epochs:927\tstep:100\tbatch_loss:785.0461\n",
            "Epochs:927\tstep:150\tbatch_loss:776.5410\n",
            "Epochs:927\tstep:200\tbatch_loss:769.1685\n",
            "Epochs:927\tstep:250\tbatch_loss:929.8345\n",
            "Epochs:927\tstep:300\tbatch_loss:801.1605\n",
            "Epochs:927\tstep:350\tbatch_loss:774.3401\n",
            "Epochs:927\tstep:400\tbatch_loss:776.7694\n",
            "Epochs:927\tstep:450\tbatch_loss:784.6429\n",
            "Epochs:927\tstep:500\tbatch_loss:729.8005\n",
            "Epochs:927\tstep:550\tbatch_loss:800.2730\n",
            "Epochs:927\tstep:600\tbatch_loss:739.7084\n",
            "Epoch loss: 488198.7893485246\n",
            "RMSE on dev data: 9.88063\n",
            "RMSE on train data: 10.00661\n",
            "Epochs:928\tstep:50\tbatch_loss:749.0156\n",
            "Epochs:928\tstep:100\tbatch_loss:785.0093\n",
            "Epochs:928\tstep:150\tbatch_loss:776.5048\n",
            "Epochs:928\tstep:200\tbatch_loss:769.1301\n",
            "Epochs:928\tstep:250\tbatch_loss:929.7965\n",
            "Epochs:928\tstep:300\tbatch_loss:801.1234\n",
            "Epochs:928\tstep:350\tbatch_loss:774.3037\n",
            "Epochs:928\tstep:400\tbatch_loss:776.7330\n",
            "Epochs:928\tstep:450\tbatch_loss:784.6054\n",
            "Epochs:928\tstep:500\tbatch_loss:729.7644\n",
            "Epochs:928\tstep:550\tbatch_loss:800.2362\n",
            "Epochs:928\tstep:600\tbatch_loss:739.6722\n",
            "Epoch loss: 488175.43864169484\n",
            "RMSE on dev data: 9.88060\n",
            "RMSE on train data: 10.00659\n",
            "Epochs:929\tstep:50\tbatch_loss:748.9785\n",
            "Epochs:929\tstep:100\tbatch_loss:784.9726\n",
            "Epochs:929\tstep:150\tbatch_loss:776.4688\n",
            "Epochs:929\tstep:200\tbatch_loss:769.0919\n",
            "Epochs:929\tstep:250\tbatch_loss:929.7585\n",
            "Epochs:929\tstep:300\tbatch_loss:801.0863\n",
            "Epochs:929\tstep:350\tbatch_loss:774.2673\n",
            "Epochs:929\tstep:400\tbatch_loss:776.6966\n",
            "Epochs:929\tstep:450\tbatch_loss:784.5679\n",
            "Epochs:929\tstep:500\tbatch_loss:729.7284\n",
            "Epochs:929\tstep:550\tbatch_loss:800.1994\n",
            "Epochs:929\tstep:600\tbatch_loss:739.6360\n",
            "Epoch loss: 488152.12755173136\n",
            "RMSE on dev data: 9.88056\n",
            "RMSE on train data: 10.00656\n",
            "Epochs:930\tstep:50\tbatch_loss:748.9415\n",
            "Epochs:930\tstep:100\tbatch_loss:784.9359\n",
            "Epochs:930\tstep:150\tbatch_loss:776.4328\n",
            "Epochs:930\tstep:200\tbatch_loss:769.0538\n",
            "Epochs:930\tstep:250\tbatch_loss:929.7205\n",
            "Epochs:930\tstep:300\tbatch_loss:801.0493\n",
            "Epochs:930\tstep:350\tbatch_loss:774.2311\n",
            "Epochs:930\tstep:400\tbatch_loss:776.6602\n",
            "Epochs:930\tstep:450\tbatch_loss:784.5306\n",
            "Epochs:930\tstep:500\tbatch_loss:729.6924\n",
            "Epochs:930\tstep:550\tbatch_loss:800.1626\n",
            "Epochs:930\tstep:600\tbatch_loss:739.5999\n",
            "Epoch loss: 488128.86630399246\n",
            "RMSE on dev data: 9.88053\n",
            "RMSE on train data: 10.00654\n",
            "Epochs:931\tstep:50\tbatch_loss:748.9045\n",
            "Epochs:931\tstep:100\tbatch_loss:784.8993\n",
            "Epochs:931\tstep:150\tbatch_loss:776.3968\n",
            "Epochs:931\tstep:200\tbatch_loss:769.0157\n",
            "Epochs:931\tstep:250\tbatch_loss:929.6827\n",
            "Epochs:931\tstep:300\tbatch_loss:801.0123\n",
            "Epochs:931\tstep:350\tbatch_loss:774.1948\n",
            "Epochs:931\tstep:400\tbatch_loss:776.6240\n",
            "Epochs:931\tstep:450\tbatch_loss:784.4933\n",
            "Epochs:931\tstep:500\tbatch_loss:729.6565\n",
            "Epochs:931\tstep:550\tbatch_loss:800.1259\n",
            "Epochs:931\tstep:600\tbatch_loss:739.5638\n",
            "Epoch loss: 488105.63196504494\n",
            "RMSE on dev data: 9.88050\n",
            "RMSE on train data: 10.00651\n",
            "Epochs:932\tstep:50\tbatch_loss:748.8676\n",
            "Epochs:932\tstep:100\tbatch_loss:784.8628\n",
            "Epochs:932\tstep:150\tbatch_loss:776.3609\n",
            "Epochs:932\tstep:200\tbatch_loss:768.9778\n",
            "Epochs:932\tstep:250\tbatch_loss:929.6449\n",
            "Epochs:932\tstep:300\tbatch_loss:800.9755\n",
            "Epochs:932\tstep:350\tbatch_loss:774.1586\n",
            "Epochs:932\tstep:400\tbatch_loss:776.5878\n",
            "Epochs:932\tstep:450\tbatch_loss:784.4561\n",
            "Epochs:932\tstep:500\tbatch_loss:729.6206\n",
            "Epochs:932\tstep:550\tbatch_loss:800.0893\n",
            "Epochs:932\tstep:600\tbatch_loss:739.5279\n",
            "Epoch loss: 488082.4582864786\n",
            "RMSE on dev data: 9.88047\n",
            "RMSE on train data: 10.00649\n",
            "Epochs:933\tstep:50\tbatch_loss:748.8308\n",
            "Epochs:933\tstep:100\tbatch_loss:784.8264\n",
            "Epochs:933\tstep:150\tbatch_loss:776.3250\n",
            "Epochs:933\tstep:200\tbatch_loss:768.9399\n",
            "Epochs:933\tstep:250\tbatch_loss:929.6072\n",
            "Epochs:933\tstep:300\tbatch_loss:800.9386\n",
            "Epochs:933\tstep:350\tbatch_loss:774.1225\n",
            "Epochs:933\tstep:400\tbatch_loss:776.5515\n",
            "Epochs:933\tstep:450\tbatch_loss:784.4189\n",
            "Epochs:933\tstep:500\tbatch_loss:729.5847\n",
            "Epochs:933\tstep:550\tbatch_loss:800.0527\n",
            "Epochs:933\tstep:600\tbatch_loss:739.4919\n",
            "Epoch loss: 488059.2883455944\n",
            "RMSE on dev data: 9.88044\n",
            "RMSE on train data: 10.00647\n",
            "Epochs:934\tstep:50\tbatch_loss:748.7940\n",
            "Epochs:934\tstep:100\tbatch_loss:784.7900\n",
            "Epochs:934\tstep:150\tbatch_loss:776.2892\n",
            "Epochs:934\tstep:200\tbatch_loss:768.9022\n",
            "Epochs:934\tstep:250\tbatch_loss:929.5696\n",
            "Epochs:934\tstep:300\tbatch_loss:800.9018\n",
            "Epochs:934\tstep:350\tbatch_loss:774.0863\n",
            "Epochs:934\tstep:400\tbatch_loss:776.5154\n",
            "Epochs:934\tstep:450\tbatch_loss:784.3819\n",
            "Epochs:934\tstep:500\tbatch_loss:729.5489\n",
            "Epochs:934\tstep:550\tbatch_loss:800.0162\n",
            "Epochs:934\tstep:600\tbatch_loss:739.4560\n",
            "Epoch loss: 488036.1745686151\n",
            "RMSE on dev data: 9.88041\n",
            "RMSE on train data: 10.00645\n",
            "Epochs:935\tstep:50\tbatch_loss:748.7572\n",
            "Epochs:935\tstep:100\tbatch_loss:784.7537\n",
            "Epochs:935\tstep:150\tbatch_loss:776.2534\n",
            "Epochs:935\tstep:200\tbatch_loss:768.8644\n",
            "Epochs:935\tstep:250\tbatch_loss:929.5321\n",
            "Epochs:935\tstep:300\tbatch_loss:800.8651\n",
            "Epochs:935\tstep:350\tbatch_loss:774.0503\n",
            "Epochs:935\tstep:400\tbatch_loss:776.4793\n",
            "Epochs:935\tstep:450\tbatch_loss:784.3448\n",
            "Epochs:935\tstep:500\tbatch_loss:729.5131\n",
            "Epochs:935\tstep:550\tbatch_loss:799.9797\n",
            "Epochs:935\tstep:600\tbatch_loss:739.4201\n",
            "Epoch loss: 488013.07222535886\n",
            "RMSE on dev data: 9.88038\n",
            "RMSE on train data: 10.00642\n",
            "Epochs:936\tstep:50\tbatch_loss:748.7205\n",
            "Epochs:936\tstep:100\tbatch_loss:784.7174\n",
            "Epochs:936\tstep:150\tbatch_loss:776.2177\n",
            "Epochs:936\tstep:200\tbatch_loss:768.8269\n",
            "Epochs:936\tstep:250\tbatch_loss:929.4946\n",
            "Epochs:936\tstep:300\tbatch_loss:800.8284\n",
            "Epochs:936\tstep:350\tbatch_loss:774.0142\n",
            "Epochs:936\tstep:400\tbatch_loss:776.4433\n",
            "Epochs:936\tstep:450\tbatch_loss:784.3079\n",
            "Epochs:936\tstep:500\tbatch_loss:729.4774\n",
            "Epochs:936\tstep:550\tbatch_loss:799.9432\n",
            "Epochs:936\tstep:600\tbatch_loss:739.3843\n",
            "Epoch loss: 487990.0187924355\n",
            "RMSE on dev data: 9.88035\n",
            "RMSE on train data: 10.00640\n",
            "Epochs:937\tstep:50\tbatch_loss:748.6839\n",
            "Epochs:937\tstep:100\tbatch_loss:784.6812\n",
            "Epochs:937\tstep:150\tbatch_loss:776.1820\n",
            "Epochs:937\tstep:200\tbatch_loss:768.7893\n",
            "Epochs:937\tstep:250\tbatch_loss:929.4572\n",
            "Epochs:937\tstep:300\tbatch_loss:800.7918\n",
            "Epochs:937\tstep:350\tbatch_loss:773.9782\n",
            "Epochs:937\tstep:400\tbatch_loss:776.4072\n",
            "Epochs:937\tstep:450\tbatch_loss:784.2709\n",
            "Epochs:937\tstep:500\tbatch_loss:729.4417\n",
            "Epochs:937\tstep:550\tbatch_loss:799.9068\n",
            "Epochs:937\tstep:600\tbatch_loss:739.3485\n",
            "Epoch loss: 487966.97934076836\n",
            "RMSE on dev data: 9.88032\n",
            "RMSE on train data: 10.00638\n",
            "Epochs:938\tstep:50\tbatch_loss:748.6473\n",
            "Epochs:938\tstep:100\tbatch_loss:784.6450\n",
            "Epochs:938\tstep:150\tbatch_loss:776.1464\n",
            "Epochs:938\tstep:200\tbatch_loss:768.7519\n",
            "Epochs:938\tstep:250\tbatch_loss:929.4199\n",
            "Epochs:938\tstep:300\tbatch_loss:800.7552\n",
            "Epochs:938\tstep:350\tbatch_loss:773.9422\n",
            "Epochs:938\tstep:400\tbatch_loss:776.3713\n",
            "Epochs:938\tstep:450\tbatch_loss:784.2341\n",
            "Epochs:938\tstep:500\tbatch_loss:729.4060\n",
            "Epochs:938\tstep:550\tbatch_loss:799.8704\n",
            "Epochs:938\tstep:600\tbatch_loss:739.3128\n",
            "Epoch loss: 487943.9814049773\n",
            "RMSE on dev data: 9.88029\n",
            "RMSE on train data: 10.00636\n",
            "Epochs:939\tstep:50\tbatch_loss:748.6107\n",
            "Epochs:939\tstep:100\tbatch_loss:784.6089\n",
            "Epochs:939\tstep:150\tbatch_loss:776.1108\n",
            "Epochs:939\tstep:200\tbatch_loss:768.7146\n",
            "Epochs:939\tstep:250\tbatch_loss:929.3826\n",
            "Epochs:939\tstep:300\tbatch_loss:800.7187\n",
            "Epochs:939\tstep:350\tbatch_loss:773.9063\n",
            "Epochs:939\tstep:400\tbatch_loss:776.3354\n",
            "Epochs:939\tstep:450\tbatch_loss:784.1973\n",
            "Epochs:939\tstep:500\tbatch_loss:729.3704\n",
            "Epochs:939\tstep:550\tbatch_loss:799.8341\n",
            "Epochs:939\tstep:600\tbatch_loss:739.2771\n",
            "Epoch loss: 487921.03212821996\n",
            "RMSE on dev data: 9.88027\n",
            "RMSE on train data: 10.00634\n",
            "Epochs:940\tstep:50\tbatch_loss:748.5742\n",
            "Epochs:940\tstep:100\tbatch_loss:784.5729\n",
            "Epochs:940\tstep:150\tbatch_loss:776.0753\n",
            "Epochs:940\tstep:200\tbatch_loss:768.6772\n",
            "Epochs:940\tstep:250\tbatch_loss:929.3454\n",
            "Epochs:940\tstep:300\tbatch_loss:800.6822\n",
            "Epochs:940\tstep:350\tbatch_loss:773.8704\n",
            "Epochs:940\tstep:400\tbatch_loss:776.2995\n",
            "Epochs:940\tstep:450\tbatch_loss:784.1605\n",
            "Epochs:940\tstep:500\tbatch_loss:729.3348\n",
            "Epochs:940\tstep:550\tbatch_loss:799.7978\n",
            "Epochs:940\tstep:600\tbatch_loss:739.2415\n",
            "Epoch loss: 487898.0803587461\n",
            "RMSE on dev data: 9.88024\n",
            "RMSE on train data: 10.00632\n",
            "Epochs:941\tstep:50\tbatch_loss:748.5377\n",
            "Epochs:941\tstep:100\tbatch_loss:784.5369\n",
            "Epochs:941\tstep:150\tbatch_loss:776.0398\n",
            "Epochs:941\tstep:200\tbatch_loss:768.6401\n",
            "Epochs:941\tstep:250\tbatch_loss:929.3082\n",
            "Epochs:941\tstep:300\tbatch_loss:800.6458\n",
            "Epochs:941\tstep:350\tbatch_loss:773.8345\n",
            "Epochs:941\tstep:400\tbatch_loss:776.2637\n",
            "Epochs:941\tstep:450\tbatch_loss:784.1239\n",
            "Epochs:941\tstep:500\tbatch_loss:729.2993\n",
            "Epochs:941\tstep:550\tbatch_loss:799.7616\n",
            "Epochs:941\tstep:600\tbatch_loss:739.2059\n",
            "Epoch loss: 487875.1777967748\n",
            "RMSE on dev data: 9.88021\n",
            "RMSE on train data: 10.00630\n",
            "Epochs:942\tstep:50\tbatch_loss:748.5013\n",
            "Epochs:942\tstep:100\tbatch_loss:784.5009\n",
            "Epochs:942\tstep:150\tbatch_loss:776.0043\n",
            "Epochs:942\tstep:200\tbatch_loss:768.6029\n",
            "Epochs:942\tstep:250\tbatch_loss:929.2712\n",
            "Epochs:942\tstep:300\tbatch_loss:800.6093\n",
            "Epochs:942\tstep:350\tbatch_loss:773.7987\n",
            "Epochs:942\tstep:400\tbatch_loss:776.2279\n",
            "Epochs:942\tstep:450\tbatch_loss:784.0872\n",
            "Epochs:942\tstep:500\tbatch_loss:729.2637\n",
            "Epochs:942\tstep:550\tbatch_loss:799.7254\n",
            "Epochs:942\tstep:600\tbatch_loss:739.1703\n",
            "Epoch loss: 487852.27953344496\n",
            "RMSE on dev data: 9.88019\n",
            "RMSE on train data: 10.00628\n",
            "Epochs:943\tstep:50\tbatch_loss:748.4649\n",
            "Epochs:943\tstep:100\tbatch_loss:784.4650\n",
            "Epochs:943\tstep:150\tbatch_loss:775.9689\n",
            "Epochs:943\tstep:200\tbatch_loss:768.5659\n",
            "Epochs:943\tstep:250\tbatch_loss:929.2341\n",
            "Epochs:943\tstep:300\tbatch_loss:800.5730\n",
            "Epochs:943\tstep:350\tbatch_loss:773.7629\n",
            "Epochs:943\tstep:400\tbatch_loss:776.1921\n",
            "Epochs:943\tstep:450\tbatch_loss:784.0506\n",
            "Epochs:943\tstep:500\tbatch_loss:729.2283\n",
            "Epochs:943\tstep:550\tbatch_loss:799.6892\n",
            "Epochs:943\tstep:600\tbatch_loss:739.1347\n",
            "Epoch loss: 487829.4252604663\n",
            "RMSE on dev data: 9.88016\n",
            "RMSE on train data: 10.00627\n",
            "Epochs:944\tstep:50\tbatch_loss:748.4285\n",
            "Epochs:944\tstep:100\tbatch_loss:784.4291\n",
            "Epochs:944\tstep:150\tbatch_loss:775.9334\n",
            "Epochs:944\tstep:200\tbatch_loss:768.5289\n",
            "Epochs:944\tstep:250\tbatch_loss:929.1972\n",
            "Epochs:944\tstep:300\tbatch_loss:800.5367\n",
            "Epochs:944\tstep:350\tbatch_loss:773.7271\n",
            "Epochs:944\tstep:400\tbatch_loss:776.1564\n",
            "Epochs:944\tstep:450\tbatch_loss:784.0141\n",
            "Epochs:944\tstep:500\tbatch_loss:729.1928\n",
            "Epochs:944\tstep:550\tbatch_loss:799.6531\n",
            "Epochs:944\tstep:600\tbatch_loss:739.0992\n",
            "Epoch loss: 487806.58500703063\n",
            "RMSE on dev data: 9.88014\n",
            "RMSE on train data: 10.00625\n",
            "Epochs:945\tstep:50\tbatch_loss:748.3922\n",
            "Epochs:945\tstep:100\tbatch_loss:784.3933\n",
            "Epochs:945\tstep:150\tbatch_loss:775.8981\n",
            "Epochs:945\tstep:200\tbatch_loss:768.4920\n",
            "Epochs:945\tstep:250\tbatch_loss:929.1602\n",
            "Epochs:945\tstep:300\tbatch_loss:800.5005\n",
            "Epochs:945\tstep:350\tbatch_loss:773.6914\n",
            "Epochs:945\tstep:400\tbatch_loss:776.1207\n",
            "Epochs:945\tstep:450\tbatch_loss:783.9776\n",
            "Epochs:945\tstep:500\tbatch_loss:729.1574\n",
            "Epochs:945\tstep:550\tbatch_loss:799.6170\n",
            "Epochs:945\tstep:600\tbatch_loss:739.0637\n",
            "Epoch loss: 487783.77938086865\n",
            "RMSE on dev data: 9.88011\n",
            "RMSE on train data: 10.00623\n",
            "Epochs:946\tstep:50\tbatch_loss:748.3559\n",
            "Epochs:946\tstep:100\tbatch_loss:784.3575\n",
            "Epochs:946\tstep:150\tbatch_loss:775.8627\n",
            "Epochs:946\tstep:200\tbatch_loss:768.4551\n",
            "Epochs:946\tstep:250\tbatch_loss:929.1233\n",
            "Epochs:946\tstep:300\tbatch_loss:800.4642\n",
            "Epochs:946\tstep:350\tbatch_loss:773.6557\n",
            "Epochs:946\tstep:400\tbatch_loss:776.0850\n",
            "Epochs:946\tstep:450\tbatch_loss:783.9411\n",
            "Epochs:946\tstep:500\tbatch_loss:729.1219\n",
            "Epochs:946\tstep:550\tbatch_loss:799.5810\n",
            "Epochs:946\tstep:600\tbatch_loss:739.0283\n",
            "Epoch loss: 487760.98151317384\n",
            "RMSE on dev data: 9.88009\n",
            "RMSE on train data: 10.00621\n",
            "Epochs:947\tstep:50\tbatch_loss:748.3197\n",
            "Epochs:947\tstep:100\tbatch_loss:784.3218\n",
            "Epochs:947\tstep:150\tbatch_loss:775.8274\n",
            "Epochs:947\tstep:200\tbatch_loss:768.4183\n",
            "Epochs:947\tstep:250\tbatch_loss:929.0865\n",
            "Epochs:947\tstep:300\tbatch_loss:800.4280\n",
            "Epochs:947\tstep:350\tbatch_loss:773.6200\n",
            "Epochs:947\tstep:400\tbatch_loss:776.0494\n",
            "Epochs:947\tstep:450\tbatch_loss:783.9047\n",
            "Epochs:947\tstep:500\tbatch_loss:729.0866\n",
            "Epochs:947\tstep:550\tbatch_loss:799.5450\n",
            "Epochs:947\tstep:600\tbatch_loss:738.9929\n",
            "Epoch loss: 487738.22775495215\n",
            "RMSE on dev data: 9.88007\n",
            "RMSE on train data: 10.00620\n",
            "Epochs:948\tstep:50\tbatch_loss:748.2835\n",
            "Epochs:948\tstep:100\tbatch_loss:784.2861\n",
            "Epochs:948\tstep:150\tbatch_loss:775.7922\n",
            "Epochs:948\tstep:200\tbatch_loss:768.3816\n",
            "Epochs:948\tstep:250\tbatch_loss:929.0497\n",
            "Epochs:948\tstep:300\tbatch_loss:800.3919\n",
            "Epochs:948\tstep:350\tbatch_loss:773.5844\n",
            "Epochs:948\tstep:400\tbatch_loss:776.0139\n",
            "Epochs:948\tstep:450\tbatch_loss:783.8683\n",
            "Epochs:948\tstep:500\tbatch_loss:729.0512\n",
            "Epochs:948\tstep:550\tbatch_loss:799.5090\n",
            "Epochs:948\tstep:600\tbatch_loss:738.9575\n",
            "Epoch loss: 487715.4968406215\n",
            "RMSE on dev data: 9.88005\n",
            "RMSE on train data: 10.00618\n",
            "Epochs:949\tstep:50\tbatch_loss:748.2473\n",
            "Epochs:949\tstep:100\tbatch_loss:784.2505\n",
            "Epochs:949\tstep:150\tbatch_loss:775.7569\n",
            "Epochs:949\tstep:200\tbatch_loss:768.3449\n",
            "Epochs:949\tstep:250\tbatch_loss:929.0130\n",
            "Epochs:949\tstep:300\tbatch_loss:800.3558\n",
            "Epochs:949\tstep:350\tbatch_loss:773.5488\n",
            "Epochs:949\tstep:400\tbatch_loss:775.9783\n",
            "Epochs:949\tstep:450\tbatch_loss:783.8320\n",
            "Epochs:949\tstep:500\tbatch_loss:729.0159\n",
            "Epochs:949\tstep:550\tbatch_loss:799.4730\n",
            "Epochs:949\tstep:600\tbatch_loss:738.9222\n",
            "Epoch loss: 487692.7654340005\n",
            "RMSE on dev data: 9.88002\n",
            "RMSE on train data: 10.00616\n",
            "Epochs:950\tstep:50\tbatch_loss:748.2112\n",
            "Epochs:950\tstep:100\tbatch_loss:784.2148\n",
            "Epochs:950\tstep:150\tbatch_loss:775.7218\n",
            "Epochs:950\tstep:200\tbatch_loss:768.3083\n",
            "Epochs:950\tstep:250\tbatch_loss:928.9763\n",
            "Epochs:950\tstep:300\tbatch_loss:800.3197\n",
            "Epochs:950\tstep:350\tbatch_loss:773.5132\n",
            "Epochs:950\tstep:400\tbatch_loss:775.9427\n",
            "Epochs:950\tstep:450\tbatch_loss:783.7957\n",
            "Epochs:950\tstep:500\tbatch_loss:728.9806\n",
            "Epochs:950\tstep:550\tbatch_loss:799.4371\n",
            "Epochs:950\tstep:600\tbatch_loss:738.8869\n",
            "Epoch loss: 487670.07376112\n",
            "RMSE on dev data: 9.88000\n",
            "RMSE on train data: 10.00615\n",
            "Epochs:951\tstep:50\tbatch_loss:748.1142\n",
            "Epochs:951\tstep:100\tbatch_loss:784.2533\n",
            "Epochs:951\tstep:150\tbatch_loss:775.5594\n",
            "Epochs:951\tstep:200\tbatch_loss:768.0173\n",
            "Epochs:951\tstep:250\tbatch_loss:929.1702\n",
            "Epochs:951\tstep:300\tbatch_loss:800.3617\n",
            "Epochs:951\tstep:350\tbatch_loss:773.4363\n",
            "Epochs:951\tstep:400\tbatch_loss:775.9585\n",
            "Epochs:951\tstep:450\tbatch_loss:783.8229\n",
            "Epochs:951\tstep:500\tbatch_loss:729.1455\n",
            "Epochs:951\tstep:550\tbatch_loss:798.8910\n",
            "Epochs:951\tstep:600\tbatch_loss:739.1204\n",
            "Epoch loss: 487607.04078116507\n",
            "RMSE on dev data: 9.88001\n",
            "RMSE on train data: 10.00597\n",
            "Epochs:952\tstep:50\tbatch_loss:748.0931\n",
            "Epochs:952\tstep:100\tbatch_loss:784.2138\n",
            "Epochs:952\tstep:150\tbatch_loss:775.5287\n",
            "Epochs:952\tstep:200\tbatch_loss:767.9879\n",
            "Epochs:952\tstep:250\tbatch_loss:929.1291\n",
            "Epochs:952\tstep:300\tbatch_loss:800.3299\n",
            "Epochs:952\tstep:350\tbatch_loss:773.4023\n",
            "Epochs:952\tstep:400\tbatch_loss:775.9267\n",
            "Epochs:952\tstep:450\tbatch_loss:783.7896\n",
            "Epochs:952\tstep:500\tbatch_loss:729.1140\n",
            "Epochs:952\tstep:550\tbatch_loss:798.8566\n",
            "Epochs:952\tstep:600\tbatch_loss:739.0884\n",
            "Epoch loss: 487586.76908670197\n",
            "RMSE on dev data: 9.87998\n",
            "RMSE on train data: 10.00595\n",
            "Epochs:953\tstep:50\tbatch_loss:748.0600\n",
            "Epochs:953\tstep:100\tbatch_loss:784.1807\n",
            "Epochs:953\tstep:150\tbatch_loss:775.4963\n",
            "Epochs:953\tstep:200\tbatch_loss:767.9534\n",
            "Epochs:953\tstep:250\tbatch_loss:929.0948\n",
            "Epochs:953\tstep:300\tbatch_loss:800.2965\n",
            "Epochs:953\tstep:350\tbatch_loss:773.3696\n",
            "Epochs:953\tstep:400\tbatch_loss:775.8940\n",
            "Epochs:953\tstep:450\tbatch_loss:783.7559\n",
            "Epochs:953\tstep:500\tbatch_loss:729.0816\n",
            "Epochs:953\tstep:550\tbatch_loss:798.8235\n",
            "Epochs:953\tstep:600\tbatch_loss:739.0559\n",
            "Epoch loss: 487565.82308180653\n",
            "RMSE on dev data: 9.87995\n",
            "RMSE on train data: 10.00592\n",
            "Epochs:954\tstep:50\tbatch_loss:748.0267\n",
            "Epochs:954\tstep:100\tbatch_loss:784.1477\n",
            "Epochs:954\tstep:150\tbatch_loss:775.4639\n",
            "Epochs:954\tstep:200\tbatch_loss:767.9190\n",
            "Epochs:954\tstep:250\tbatch_loss:929.0607\n",
            "Epochs:954\tstep:300\tbatch_loss:800.2632\n",
            "Epochs:954\tstep:350\tbatch_loss:773.3370\n",
            "Epochs:954\tstep:400\tbatch_loss:775.8614\n",
            "Epochs:954\tstep:450\tbatch_loss:783.7223\n",
            "Epochs:954\tstep:500\tbatch_loss:729.0494\n",
            "Epochs:954\tstep:550\tbatch_loss:798.7905\n",
            "Epochs:954\tstep:600\tbatch_loss:739.0235\n",
            "Epoch loss: 487544.93239896087\n",
            "RMSE on dev data: 9.87991\n",
            "RMSE on train data: 10.00590\n",
            "Epochs:955\tstep:50\tbatch_loss:747.9935\n",
            "Epochs:955\tstep:100\tbatch_loss:784.1148\n",
            "Epochs:955\tstep:150\tbatch_loss:775.4316\n",
            "Epochs:955\tstep:200\tbatch_loss:767.8845\n",
            "Epochs:955\tstep:250\tbatch_loss:929.0266\n",
            "Epochs:955\tstep:300\tbatch_loss:800.2299\n",
            "Epochs:955\tstep:350\tbatch_loss:773.3045\n",
            "Epochs:955\tstep:400\tbatch_loss:775.8287\n",
            "Epochs:955\tstep:450\tbatch_loss:783.6887\n",
            "Epochs:955\tstep:500\tbatch_loss:729.0171\n",
            "Epochs:955\tstep:550\tbatch_loss:798.7575\n",
            "Epochs:955\tstep:600\tbatch_loss:738.9912\n",
            "Epoch loss: 487524.04142121493\n",
            "RMSE on dev data: 9.87988\n",
            "RMSE on train data: 10.00587\n",
            "Epochs:956\tstep:50\tbatch_loss:747.9603\n",
            "Epochs:956\tstep:100\tbatch_loss:784.0819\n",
            "Epochs:956\tstep:150\tbatch_loss:775.3993\n",
            "Epochs:956\tstep:200\tbatch_loss:767.8503\n",
            "Epochs:956\tstep:250\tbatch_loss:928.9926\n",
            "Epochs:956\tstep:300\tbatch_loss:800.1967\n",
            "Epochs:956\tstep:350\tbatch_loss:773.2719\n",
            "Epochs:956\tstep:400\tbatch_loss:775.7962\n",
            "Epochs:956\tstep:450\tbatch_loss:783.6552\n",
            "Epochs:956\tstep:500\tbatch_loss:728.9848\n",
            "Epochs:956\tstep:550\tbatch_loss:798.7245\n",
            "Epochs:956\tstep:600\tbatch_loss:738.9588\n",
            "Epoch loss: 487503.1975393941\n",
            "RMSE on dev data: 9.87985\n",
            "RMSE on train data: 10.00585\n",
            "Epochs:957\tstep:50\tbatch_loss:747.9271\n",
            "Epochs:957\tstep:100\tbatch_loss:784.0491\n",
            "Epochs:957\tstep:150\tbatch_loss:775.3670\n",
            "Epochs:957\tstep:200\tbatch_loss:767.8160\n",
            "Epochs:957\tstep:250\tbatch_loss:928.9587\n",
            "Epochs:957\tstep:300\tbatch_loss:800.1635\n",
            "Epochs:957\tstep:350\tbatch_loss:773.2394\n",
            "Epochs:957\tstep:400\tbatch_loss:775.7636\n",
            "Epochs:957\tstep:450\tbatch_loss:783.6218\n",
            "Epochs:957\tstep:500\tbatch_loss:728.9526\n",
            "Epochs:957\tstep:550\tbatch_loss:798.6916\n",
            "Epochs:957\tstep:600\tbatch_loss:738.9265\n",
            "Epoch loss: 487482.37059552385\n",
            "RMSE on dev data: 9.87982\n",
            "RMSE on train data: 10.00583\n",
            "Epochs:958\tstep:50\tbatch_loss:747.8940\n",
            "Epochs:958\tstep:100\tbatch_loss:784.0163\n",
            "Epochs:958\tstep:150\tbatch_loss:775.3348\n",
            "Epochs:958\tstep:200\tbatch_loss:767.7819\n",
            "Epochs:958\tstep:250\tbatch_loss:928.9248\n",
            "Epochs:958\tstep:300\tbatch_loss:800.1304\n",
            "Epochs:958\tstep:350\tbatch_loss:773.2070\n",
            "Epochs:958\tstep:400\tbatch_loss:775.7312\n",
            "Epochs:958\tstep:450\tbatch_loss:783.5884\n",
            "Epochs:958\tstep:500\tbatch_loss:728.9205\n",
            "Epochs:958\tstep:550\tbatch_loss:798.6587\n",
            "Epochs:958\tstep:600\tbatch_loss:738.8943\n",
            "Epoch loss: 487461.5940237539\n",
            "RMSE on dev data: 9.87979\n",
            "RMSE on train data: 10.00581\n",
            "Epochs:959\tstep:50\tbatch_loss:747.8610\n",
            "Epochs:959\tstep:100\tbatch_loss:783.9836\n",
            "Epochs:959\tstep:150\tbatch_loss:775.3026\n",
            "Epochs:959\tstep:200\tbatch_loss:767.7478\n",
            "Epochs:959\tstep:250\tbatch_loss:928.8910\n",
            "Epochs:959\tstep:300\tbatch_loss:800.0973\n",
            "Epochs:959\tstep:350\tbatch_loss:773.1746\n",
            "Epochs:959\tstep:400\tbatch_loss:775.6987\n",
            "Epochs:959\tstep:450\tbatch_loss:783.5551\n",
            "Epochs:959\tstep:500\tbatch_loss:728.8883\n",
            "Epochs:959\tstep:550\tbatch_loss:798.6259\n",
            "Epochs:959\tstep:600\tbatch_loss:738.8621\n",
            "Epoch loss: 487440.8272458308\n",
            "RMSE on dev data: 9.87976\n",
            "RMSE on train data: 10.00578\n",
            "Epochs:960\tstep:50\tbatch_loss:747.8280\n",
            "Epochs:960\tstep:100\tbatch_loss:783.9510\n",
            "Epochs:960\tstep:150\tbatch_loss:775.2705\n",
            "Epochs:960\tstep:200\tbatch_loss:767.7138\n",
            "Epochs:960\tstep:250\tbatch_loss:928.8573\n",
            "Epochs:960\tstep:300\tbatch_loss:800.0642\n",
            "Epochs:960\tstep:350\tbatch_loss:773.1422\n",
            "Epochs:960\tstep:400\tbatch_loss:775.6663\n",
            "Epochs:960\tstep:450\tbatch_loss:783.5218\n",
            "Epochs:960\tstep:500\tbatch_loss:728.8562\n",
            "Epochs:960\tstep:550\tbatch_loss:798.5931\n",
            "Epochs:960\tstep:600\tbatch_loss:738.8299\n",
            "Epoch loss: 487420.0945457682\n",
            "RMSE on dev data: 9.87973\n",
            "RMSE on train data: 10.00576\n",
            "Epochs:961\tstep:50\tbatch_loss:747.7950\n",
            "Epochs:961\tstep:100\tbatch_loss:783.9184\n",
            "Epochs:961\tstep:150\tbatch_loss:775.2384\n",
            "Epochs:961\tstep:200\tbatch_loss:767.6800\n",
            "Epochs:961\tstep:250\tbatch_loss:928.8236\n",
            "Epochs:961\tstep:300\tbatch_loss:800.0313\n",
            "Epochs:961\tstep:350\tbatch_loss:773.1098\n",
            "Epochs:961\tstep:400\tbatch_loss:775.6339\n",
            "Epochs:961\tstep:450\tbatch_loss:783.4886\n",
            "Epochs:961\tstep:500\tbatch_loss:728.8242\n",
            "Epochs:961\tstep:550\tbatch_loss:798.5604\n",
            "Epochs:961\tstep:600\tbatch_loss:738.7978\n",
            "Epoch loss: 487399.40498354094\n",
            "RMSE on dev data: 9.87970\n",
            "RMSE on train data: 10.00574\n",
            "Epochs:962\tstep:50\tbatch_loss:747.7621\n",
            "Epochs:962\tstep:100\tbatch_loss:783.8859\n",
            "Epochs:962\tstep:150\tbatch_loss:775.2064\n",
            "Epochs:962\tstep:200\tbatch_loss:767.6462\n",
            "Epochs:962\tstep:250\tbatch_loss:928.7900\n",
            "Epochs:962\tstep:300\tbatch_loss:799.9983\n",
            "Epochs:962\tstep:350\tbatch_loss:773.0775\n",
            "Epochs:962\tstep:400\tbatch_loss:775.6016\n",
            "Epochs:962\tstep:450\tbatch_loss:783.4554\n",
            "Epochs:962\tstep:500\tbatch_loss:728.7922\n",
            "Epochs:962\tstep:550\tbatch_loss:798.5277\n",
            "Epochs:962\tstep:600\tbatch_loss:738.7657\n",
            "Epoch loss: 487378.74076185125\n",
            "RMSE on dev data: 9.87967\n",
            "RMSE on train data: 10.00572\n",
            "Epochs:963\tstep:50\tbatch_loss:747.7292\n",
            "Epochs:963\tstep:100\tbatch_loss:783.8534\n",
            "Epochs:963\tstep:150\tbatch_loss:775.1744\n",
            "Epochs:963\tstep:200\tbatch_loss:767.6125\n",
            "Epochs:963\tstep:250\tbatch_loss:928.7564\n",
            "Epochs:963\tstep:300\tbatch_loss:799.9655\n",
            "Epochs:963\tstep:350\tbatch_loss:773.0452\n",
            "Epochs:963\tstep:400\tbatch_loss:775.5693\n",
            "Epochs:963\tstep:450\tbatch_loss:783.4223\n",
            "Epochs:963\tstep:500\tbatch_loss:728.7602\n",
            "Epochs:963\tstep:550\tbatch_loss:798.4950\n",
            "Epochs:963\tstep:600\tbatch_loss:738.7337\n",
            "Epoch loss: 487358.1042237718\n",
            "RMSE on dev data: 9.87965\n",
            "RMSE on train data: 10.00570\n",
            "Epochs:964\tstep:50\tbatch_loss:747.6964\n",
            "Epochs:964\tstep:100\tbatch_loss:783.8209\n",
            "Epochs:964\tstep:150\tbatch_loss:775.1424\n",
            "Epochs:964\tstep:200\tbatch_loss:767.5787\n",
            "Epochs:964\tstep:250\tbatch_loss:928.7229\n",
            "Epochs:964\tstep:300\tbatch_loss:799.9326\n",
            "Epochs:964\tstep:350\tbatch_loss:773.0130\n",
            "Epochs:964\tstep:400\tbatch_loss:775.5370\n",
            "Epochs:964\tstep:450\tbatch_loss:783.3892\n",
            "Epochs:964\tstep:500\tbatch_loss:728.7282\n",
            "Epochs:964\tstep:550\tbatch_loss:798.4624\n",
            "Epochs:964\tstep:600\tbatch_loss:738.7016\n",
            "Epoch loss: 487337.4789723773\n",
            "RMSE on dev data: 9.87962\n",
            "RMSE on train data: 10.00568\n",
            "Epochs:965\tstep:50\tbatch_loss:747.6636\n",
            "Epochs:965\tstep:100\tbatch_loss:783.7885\n",
            "Epochs:965\tstep:150\tbatch_loss:775.1105\n",
            "Epochs:965\tstep:200\tbatch_loss:767.5452\n",
            "Epochs:965\tstep:250\tbatch_loss:928.6895\n",
            "Epochs:965\tstep:300\tbatch_loss:799.8998\n",
            "Epochs:965\tstep:350\tbatch_loss:772.9807\n",
            "Epochs:965\tstep:400\tbatch_loss:775.5049\n",
            "Epochs:965\tstep:450\tbatch_loss:783.3562\n",
            "Epochs:965\tstep:500\tbatch_loss:728.6963\n",
            "Epochs:965\tstep:550\tbatch_loss:798.4298\n",
            "Epochs:965\tstep:600\tbatch_loss:738.6697\n",
            "Epoch loss: 487316.8976536432\n",
            "RMSE on dev data: 9.87959\n",
            "RMSE on train data: 10.00566\n",
            "Epochs:966\tstep:50\tbatch_loss:747.6309\n",
            "Epochs:966\tstep:100\tbatch_loss:783.7562\n",
            "Epochs:966\tstep:150\tbatch_loss:775.0786\n",
            "Epochs:966\tstep:200\tbatch_loss:767.5116\n",
            "Epochs:966\tstep:250\tbatch_loss:928.6562\n",
            "Epochs:966\tstep:300\tbatch_loss:799.8670\n",
            "Epochs:966\tstep:350\tbatch_loss:772.9485\n",
            "Epochs:966\tstep:400\tbatch_loss:775.4727\n",
            "Epochs:966\tstep:450\tbatch_loss:783.3232\n",
            "Epochs:966\tstep:500\tbatch_loss:728.6644\n",
            "Epochs:966\tstep:550\tbatch_loss:798.3973\n",
            "Epochs:966\tstep:600\tbatch_loss:738.6377\n",
            "Epoch loss: 487296.3214223874\n",
            "RMSE on dev data: 9.87957\n",
            "RMSE on train data: 10.00564\n",
            "Epochs:967\tstep:50\tbatch_loss:747.5981\n",
            "Epochs:967\tstep:100\tbatch_loss:783.7239\n",
            "Epochs:967\tstep:150\tbatch_loss:775.0467\n",
            "Epochs:967\tstep:200\tbatch_loss:767.4782\n",
            "Epochs:967\tstep:250\tbatch_loss:928.6228\n",
            "Epochs:967\tstep:300\tbatch_loss:799.8343\n",
            "Epochs:967\tstep:350\tbatch_loss:772.9164\n",
            "Epochs:967\tstep:400\tbatch_loss:775.4405\n",
            "Epochs:967\tstep:450\tbatch_loss:783.2903\n",
            "Epochs:967\tstep:500\tbatch_loss:728.6325\n",
            "Epochs:967\tstep:550\tbatch_loss:798.3648\n",
            "Epochs:967\tstep:600\tbatch_loss:738.6058\n",
            "Epoch loss: 487275.7839808179\n",
            "RMSE on dev data: 9.87954\n",
            "RMSE on train data: 10.00562\n",
            "Epochs:968\tstep:50\tbatch_loss:747.5654\n",
            "Epochs:968\tstep:100\tbatch_loss:783.6916\n",
            "Epochs:968\tstep:150\tbatch_loss:775.0149\n",
            "Epochs:968\tstep:200\tbatch_loss:767.4447\n",
            "Epochs:968\tstep:250\tbatch_loss:928.5896\n",
            "Epochs:968\tstep:300\tbatch_loss:799.8016\n",
            "Epochs:968\tstep:350\tbatch_loss:772.8842\n",
            "Epochs:968\tstep:400\tbatch_loss:775.4084\n",
            "Epochs:968\tstep:450\tbatch_loss:783.2574\n",
            "Epochs:968\tstep:500\tbatch_loss:728.6007\n",
            "Epochs:968\tstep:550\tbatch_loss:798.3323\n",
            "Epochs:968\tstep:600\tbatch_loss:738.5739\n",
            "Epoch loss: 487255.2595337289\n",
            "RMSE on dev data: 9.87952\n",
            "RMSE on train data: 10.00561\n",
            "Epochs:969\tstep:50\tbatch_loss:747.5328\n",
            "Epochs:969\tstep:100\tbatch_loss:783.6594\n",
            "Epochs:969\tstep:150\tbatch_loss:774.9831\n",
            "Epochs:969\tstep:200\tbatch_loss:767.4114\n",
            "Epochs:969\tstep:250\tbatch_loss:928.5564\n",
            "Epochs:969\tstep:300\tbatch_loss:799.7690\n",
            "Epochs:969\tstep:350\tbatch_loss:772.8521\n",
            "Epochs:969\tstep:400\tbatch_loss:775.3764\n",
            "Epochs:969\tstep:450\tbatch_loss:783.2245\n",
            "Epochs:969\tstep:500\tbatch_loss:728.5689\n",
            "Epochs:969\tstep:550\tbatch_loss:798.2998\n",
            "Epochs:969\tstep:600\tbatch_loss:738.5420\n",
            "Epoch loss: 487234.75697856786\n",
            "RMSE on dev data: 9.87949\n",
            "RMSE on train data: 10.00559\n",
            "Epochs:970\tstep:50\tbatch_loss:747.5002\n",
            "Epochs:970\tstep:100\tbatch_loss:783.6272\n",
            "Epochs:970\tstep:150\tbatch_loss:774.9513\n",
            "Epochs:970\tstep:200\tbatch_loss:767.3781\n",
            "Epochs:970\tstep:250\tbatch_loss:928.5231\n",
            "Epochs:970\tstep:300\tbatch_loss:799.7364\n",
            "Epochs:970\tstep:350\tbatch_loss:772.8200\n",
            "Epochs:970\tstep:400\tbatch_loss:775.3443\n",
            "Epochs:970\tstep:450\tbatch_loss:783.1918\n",
            "Epochs:970\tstep:500\tbatch_loss:728.5371\n",
            "Epochs:970\tstep:550\tbatch_loss:798.2674\n",
            "Epochs:970\tstep:600\tbatch_loss:738.5103\n",
            "Epoch loss: 487214.29848244396\n",
            "RMSE on dev data: 9.87947\n",
            "RMSE on train data: 10.00557\n",
            "Epochs:971\tstep:50\tbatch_loss:747.4676\n",
            "Epochs:971\tstep:100\tbatch_loss:783.5951\n",
            "Epochs:971\tstep:150\tbatch_loss:774.9196\n",
            "Epochs:971\tstep:200\tbatch_loss:767.3449\n",
            "Epochs:971\tstep:250\tbatch_loss:928.4901\n",
            "Epochs:971\tstep:300\tbatch_loss:799.7038\n",
            "Epochs:971\tstep:350\tbatch_loss:772.7880\n",
            "Epochs:971\tstep:400\tbatch_loss:775.3123\n",
            "Epochs:971\tstep:450\tbatch_loss:783.1590\n",
            "Epochs:971\tstep:500\tbatch_loss:728.5053\n",
            "Epochs:971\tstep:550\tbatch_loss:798.2351\n",
            "Epochs:971\tstep:600\tbatch_loss:738.4784\n",
            "Epoch loss: 487193.8384166486\n",
            "RMSE on dev data: 9.87945\n",
            "RMSE on train data: 10.00555\n",
            "Epochs:972\tstep:50\tbatch_loss:747.4350\n",
            "Epochs:972\tstep:100\tbatch_loss:783.5629\n",
            "Epochs:972\tstep:150\tbatch_loss:774.8879\n",
            "Epochs:972\tstep:200\tbatch_loss:767.3117\n",
            "Epochs:972\tstep:250\tbatch_loss:928.4569\n",
            "Epochs:972\tstep:300\tbatch_loss:799.6713\n",
            "Epochs:972\tstep:350\tbatch_loss:772.7560\n",
            "Epochs:972\tstep:400\tbatch_loss:775.2803\n",
            "Epochs:972\tstep:450\tbatch_loss:783.1263\n",
            "Epochs:972\tstep:500\tbatch_loss:728.4736\n",
            "Epochs:972\tstep:550\tbatch_loss:798.2027\n",
            "Epochs:972\tstep:600\tbatch_loss:738.4467\n",
            "Epoch loss: 487173.4089242917\n",
            "RMSE on dev data: 9.87942\n",
            "RMSE on train data: 10.00554\n",
            "Epochs:973\tstep:50\tbatch_loss:747.4025\n",
            "Epochs:973\tstep:100\tbatch_loss:783.5309\n",
            "Epochs:973\tstep:150\tbatch_loss:774.8562\n",
            "Epochs:973\tstep:200\tbatch_loss:767.2786\n",
            "Epochs:973\tstep:250\tbatch_loss:928.4239\n",
            "Epochs:973\tstep:300\tbatch_loss:799.6387\n",
            "Epochs:973\tstep:350\tbatch_loss:772.7240\n",
            "Epochs:973\tstep:400\tbatch_loss:775.2484\n",
            "Epochs:973\tstep:450\tbatch_loss:783.0936\n",
            "Epochs:973\tstep:500\tbatch_loss:728.4419\n",
            "Epochs:973\tstep:550\tbatch_loss:798.1704\n",
            "Epochs:973\tstep:600\tbatch_loss:738.4149\n",
            "Epoch loss: 487152.9843642306\n",
            "RMSE on dev data: 9.87940\n",
            "RMSE on train data: 10.00552\n",
            "Epochs:974\tstep:50\tbatch_loss:747.3700\n",
            "Epochs:974\tstep:100\tbatch_loss:783.4988\n",
            "Epochs:974\tstep:150\tbatch_loss:774.8246\n",
            "Epochs:974\tstep:200\tbatch_loss:767.2455\n",
            "Epochs:974\tstep:250\tbatch_loss:928.3909\n",
            "Epochs:974\tstep:300\tbatch_loss:799.6063\n",
            "Epochs:974\tstep:350\tbatch_loss:772.6920\n",
            "Epochs:974\tstep:400\tbatch_loss:775.2164\n",
            "Epochs:974\tstep:450\tbatch_loss:783.0609\n",
            "Epochs:974\tstep:500\tbatch_loss:728.4102\n",
            "Epochs:974\tstep:550\tbatch_loss:798.1381\n",
            "Epochs:974\tstep:600\tbatch_loss:738.3833\n",
            "Epoch loss: 487132.59470019996\n",
            "RMSE on dev data: 9.87938\n",
            "RMSE on train data: 10.00551\n",
            "Epochs:975\tstep:50\tbatch_loss:747.3376\n",
            "Epochs:975\tstep:100\tbatch_loss:783.4668\n",
            "Epochs:975\tstep:150\tbatch_loss:774.7929\n",
            "Epochs:975\tstep:200\tbatch_loss:767.2125\n",
            "Epochs:975\tstep:250\tbatch_loss:928.3580\n",
            "Epochs:975\tstep:300\tbatch_loss:799.5738\n",
            "Epochs:975\tstep:350\tbatch_loss:772.6600\n",
            "Epochs:975\tstep:400\tbatch_loss:775.1846\n",
            "Epochs:975\tstep:450\tbatch_loss:783.0283\n",
            "Epochs:975\tstep:500\tbatch_loss:728.3785\n",
            "Epochs:975\tstep:550\tbatch_loss:798.1059\n",
            "Epochs:975\tstep:600\tbatch_loss:738.3515\n",
            "Epoch loss: 487112.21506109304\n",
            "RMSE on dev data: 9.87936\n",
            "RMSE on train data: 10.00549\n",
            "Epochs:976\tstep:50\tbatch_loss:747.2484\n",
            "Epochs:976\tstep:100\tbatch_loss:783.5087\n",
            "Epochs:976\tstep:150\tbatch_loss:774.6768\n",
            "Epochs:976\tstep:200\tbatch_loss:766.9337\n",
            "Epochs:976\tstep:250\tbatch_loss:928.5935\n",
            "Epochs:976\tstep:300\tbatch_loss:799.6020\n",
            "Epochs:976\tstep:350\tbatch_loss:772.5984\n",
            "Epochs:976\tstep:400\tbatch_loss:775.1965\n",
            "Epochs:976\tstep:450\tbatch_loss:783.0599\n",
            "Epochs:976\tstep:500\tbatch_loss:728.5449\n",
            "Epochs:976\tstep:550\tbatch_loss:797.5781\n",
            "Epochs:976\tstep:600\tbatch_loss:738.5959\n",
            "Epoch loss: 487054.7466281776\n",
            "RMSE on dev data: 9.87942\n",
            "RMSE on train data: 10.00520\n",
            "Epochs:977\tstep:50\tbatch_loss:747.2421\n",
            "Epochs:977\tstep:100\tbatch_loss:783.4658\n",
            "Epochs:977\tstep:150\tbatch_loss:774.6500\n",
            "Epochs:977\tstep:200\tbatch_loss:766.9131\n",
            "Epochs:977\tstep:250\tbatch_loss:928.5476\n",
            "Epochs:977\tstep:300\tbatch_loss:799.5757\n",
            "Epochs:977\tstep:350\tbatch_loss:772.5662\n",
            "Epochs:977\tstep:400\tbatch_loss:775.1694\n",
            "Epochs:977\tstep:450\tbatch_loss:783.0307\n",
            "Epochs:977\tstep:500\tbatch_loss:728.5182\n",
            "Epochs:977\tstep:550\tbatch_loss:797.5450\n",
            "Epochs:977\tstep:600\tbatch_loss:738.5681\n",
            "Epoch loss: 487037.28430856793\n",
            "RMSE on dev data: 9.87939\n",
            "RMSE on train data: 10.00517\n",
            "Epochs:978\tstep:50\tbatch_loss:747.2127\n",
            "Epochs:978\tstep:100\tbatch_loss:783.4358\n",
            "Epochs:978\tstep:150\tbatch_loss:774.6209\n",
            "Epochs:978\tstep:200\tbatch_loss:766.8823\n",
            "Epochs:978\tstep:250\tbatch_loss:928.5165\n",
            "Epochs:978\tstep:300\tbatch_loss:799.5458\n",
            "Epochs:978\tstep:350\tbatch_loss:772.5368\n",
            "Epochs:978\tstep:400\tbatch_loss:775.1402\n",
            "Epochs:978\tstep:450\tbatch_loss:783.0005\n",
            "Epochs:978\tstep:500\tbatch_loss:728.4892\n",
            "Epochs:978\tstep:550\tbatch_loss:797.5152\n",
            "Epochs:978\tstep:600\tbatch_loss:738.5390\n",
            "Epoch loss: 487018.52616665815\n",
            "RMSE on dev data: 9.87936\n",
            "RMSE on train data: 10.00514\n",
            "Epochs:979\tstep:50\tbatch_loss:747.1828\n",
            "Epochs:979\tstep:100\tbatch_loss:783.4062\n",
            "Epochs:979\tstep:150\tbatch_loss:774.5918\n",
            "Epochs:979\tstep:200\tbatch_loss:766.8512\n",
            "Epochs:979\tstep:250\tbatch_loss:928.4858\n",
            "Epochs:979\tstep:300\tbatch_loss:799.5158\n",
            "Epochs:979\tstep:350\tbatch_loss:772.5075\n",
            "Epochs:979\tstep:400\tbatch_loss:775.1108\n",
            "Epochs:979\tstep:450\tbatch_loss:782.9703\n",
            "Epochs:979\tstep:500\tbatch_loss:728.4602\n",
            "Epochs:979\tstep:550\tbatch_loss:797.4856\n",
            "Epochs:979\tstep:600\tbatch_loss:738.5099\n",
            "Epoch loss: 486999.7599852006\n",
            "RMSE on dev data: 9.87933\n",
            "RMSE on train data: 10.00512\n",
            "Epochs:980\tstep:50\tbatch_loss:747.1530\n",
            "Epochs:980\tstep:100\tbatch_loss:783.3766\n",
            "Epochs:980\tstep:150\tbatch_loss:774.5628\n",
            "Epochs:980\tstep:200\tbatch_loss:766.8202\n",
            "Epochs:980\tstep:250\tbatch_loss:928.4552\n",
            "Epochs:980\tstep:300\tbatch_loss:799.4859\n",
            "Epochs:980\tstep:350\tbatch_loss:772.4783\n",
            "Epochs:980\tstep:400\tbatch_loss:775.0815\n",
            "Epochs:980\tstep:450\tbatch_loss:782.9402\n",
            "Epochs:980\tstep:500\tbatch_loss:728.4312\n",
            "Epochs:980\tstep:550\tbatch_loss:797.4559\n",
            "Epochs:980\tstep:600\tbatch_loss:738.4809\n",
            "Epoch loss: 486981.01427873375\n",
            "RMSE on dev data: 9.87930\n",
            "RMSE on train data: 10.00510\n",
            "Epochs:981\tstep:50\tbatch_loss:747.1232\n",
            "Epochs:981\tstep:100\tbatch_loss:783.3470\n",
            "Epochs:981\tstep:150\tbatch_loss:774.5338\n",
            "Epochs:981\tstep:200\tbatch_loss:766.7893\n",
            "Epochs:981\tstep:250\tbatch_loss:928.4247\n",
            "Epochs:981\tstep:300\tbatch_loss:799.4560\n",
            "Epochs:981\tstep:350\tbatch_loss:772.4490\n",
            "Epochs:981\tstep:400\tbatch_loss:775.0523\n",
            "Epochs:981\tstep:450\tbatch_loss:782.9101\n",
            "Epochs:981\tstep:500\tbatch_loss:728.4023\n",
            "Epochs:981\tstep:550\tbatch_loss:797.4263\n",
            "Epochs:981\tstep:600\tbatch_loss:738.4519\n",
            "Epoch loss: 486962.30907004856\n",
            "RMSE on dev data: 9.87927\n",
            "RMSE on train data: 10.00508\n",
            "Epochs:982\tstep:50\tbatch_loss:747.0934\n",
            "Epochs:982\tstep:100\tbatch_loss:783.3175\n",
            "Epochs:982\tstep:150\tbatch_loss:774.5048\n",
            "Epochs:982\tstep:200\tbatch_loss:766.7584\n",
            "Epochs:982\tstep:250\tbatch_loss:928.3942\n",
            "Epochs:982\tstep:300\tbatch_loss:799.4262\n",
            "Epochs:982\tstep:350\tbatch_loss:772.4199\n",
            "Epochs:982\tstep:400\tbatch_loss:775.0231\n",
            "Epochs:982\tstep:450\tbatch_loss:782.8800\n",
            "Epochs:982\tstep:500\tbatch_loss:728.3734\n",
            "Epochs:982\tstep:550\tbatch_loss:797.3968\n",
            "Epochs:982\tstep:600\tbatch_loss:738.4229\n",
            "Epoch loss: 486943.61240224\n",
            "RMSE on dev data: 9.87924\n",
            "RMSE on train data: 10.00506\n",
            "Epochs:983\tstep:50\tbatch_loss:747.0637\n",
            "Epochs:983\tstep:100\tbatch_loss:783.2881\n",
            "Epochs:983\tstep:150\tbatch_loss:774.4758\n",
            "Epochs:983\tstep:200\tbatch_loss:766.7277\n",
            "Epochs:983\tstep:250\tbatch_loss:928.3637\n",
            "Epochs:983\tstep:300\tbatch_loss:799.3964\n",
            "Epochs:983\tstep:350\tbatch_loss:772.3907\n",
            "Epochs:983\tstep:400\tbatch_loss:774.9939\n",
            "Epochs:983\tstep:450\tbatch_loss:782.8500\n",
            "Epochs:983\tstep:500\tbatch_loss:728.3446\n",
            "Epochs:983\tstep:550\tbatch_loss:797.3672\n",
            "Epochs:983\tstep:600\tbatch_loss:738.3940\n",
            "Epoch loss: 486924.9548313866\n",
            "RMSE on dev data: 9.87921\n",
            "RMSE on train data: 10.00503\n",
            "Epochs:984\tstep:50\tbatch_loss:747.0340\n",
            "Epochs:984\tstep:100\tbatch_loss:783.2587\n",
            "Epochs:984\tstep:150\tbatch_loss:774.4469\n",
            "Epochs:984\tstep:200\tbatch_loss:766.6970\n",
            "Epochs:984\tstep:250\tbatch_loss:928.3334\n",
            "Epochs:984\tstep:300\tbatch_loss:799.3666\n",
            "Epochs:984\tstep:350\tbatch_loss:772.3616\n",
            "Epochs:984\tstep:400\tbatch_loss:774.9647\n",
            "Epochs:984\tstep:450\tbatch_loss:782.8200\n",
            "Epochs:984\tstep:500\tbatch_loss:728.3157\n",
            "Epochs:984\tstep:550\tbatch_loss:797.3377\n",
            "Epochs:984\tstep:600\tbatch_loss:738.3651\n",
            "Epoch loss: 486906.31000935833\n",
            "RMSE on dev data: 9.87918\n",
            "RMSE on train data: 10.00501\n",
            "Epochs:985\tstep:50\tbatch_loss:747.0043\n",
            "Epochs:985\tstep:100\tbatch_loss:783.2293\n",
            "Epochs:985\tstep:150\tbatch_loss:774.4180\n",
            "Epochs:985\tstep:200\tbatch_loss:766.6663\n",
            "Epochs:985\tstep:250\tbatch_loss:928.3031\n",
            "Epochs:985\tstep:300\tbatch_loss:799.3369\n",
            "Epochs:985\tstep:350\tbatch_loss:772.3325\n",
            "Epochs:985\tstep:400\tbatch_loss:774.9356\n",
            "Epochs:985\tstep:450\tbatch_loss:782.7901\n",
            "Epochs:985\tstep:500\tbatch_loss:728.2869\n",
            "Epochs:985\tstep:550\tbatch_loss:797.3083\n",
            "Epochs:985\tstep:600\tbatch_loss:738.3362\n",
            "Epoch loss: 486887.69879864756\n",
            "RMSE on dev data: 9.87915\n",
            "RMSE on train data: 10.00499\n",
            "Epochs:986\tstep:50\tbatch_loss:746.9747\n",
            "Epochs:986\tstep:100\tbatch_loss:783.2000\n",
            "Epochs:986\tstep:150\tbatch_loss:774.3892\n",
            "Epochs:986\tstep:200\tbatch_loss:766.6358\n",
            "Epochs:986\tstep:250\tbatch_loss:928.2728\n",
            "Epochs:986\tstep:300\tbatch_loss:799.3072\n",
            "Epochs:986\tstep:350\tbatch_loss:772.3035\n",
            "Epochs:986\tstep:400\tbatch_loss:774.9066\n",
            "Epochs:986\tstep:450\tbatch_loss:782.7603\n",
            "Epochs:986\tstep:500\tbatch_loss:728.2581\n",
            "Epochs:986\tstep:550\tbatch_loss:797.2788\n",
            "Epochs:986\tstep:600\tbatch_loss:738.3074\n",
            "Epoch loss: 486869.11519461294\n",
            "RMSE on dev data: 9.87912\n",
            "RMSE on train data: 10.00497\n",
            "Epochs:987\tstep:50\tbatch_loss:746.9452\n",
            "Epochs:987\tstep:100\tbatch_loss:783.1707\n",
            "Epochs:987\tstep:150\tbatch_loss:774.3604\n",
            "Epochs:987\tstep:200\tbatch_loss:766.6053\n",
            "Epochs:987\tstep:250\tbatch_loss:928.2426\n",
            "Epochs:987\tstep:300\tbatch_loss:799.2776\n",
            "Epochs:987\tstep:350\tbatch_loss:772.2744\n",
            "Epochs:987\tstep:400\tbatch_loss:774.8775\n",
            "Epochs:987\tstep:450\tbatch_loss:782.7305\n",
            "Epochs:987\tstep:500\tbatch_loss:728.2294\n",
            "Epochs:987\tstep:550\tbatch_loss:797.2495\n",
            "Epochs:987\tstep:600\tbatch_loss:738.2786\n",
            "Epoch loss: 486850.5537635393\n",
            "RMSE on dev data: 9.87909\n",
            "RMSE on train data: 10.00495\n",
            "Epochs:988\tstep:50\tbatch_loss:746.9156\n",
            "Epochs:988\tstep:100\tbatch_loss:783.1415\n",
            "Epochs:988\tstep:150\tbatch_loss:774.3316\n",
            "Epochs:988\tstep:200\tbatch_loss:766.5749\n",
            "Epochs:988\tstep:250\tbatch_loss:928.2125\n",
            "Epochs:988\tstep:300\tbatch_loss:799.2480\n",
            "Epochs:988\tstep:350\tbatch_loss:772.2454\n",
            "Epochs:988\tstep:400\tbatch_loss:774.8485\n",
            "Epochs:988\tstep:450\tbatch_loss:782.7007\n",
            "Epochs:988\tstep:500\tbatch_loss:728.2007\n",
            "Epochs:988\tstep:550\tbatch_loss:797.2201\n",
            "Epochs:988\tstep:600\tbatch_loss:738.2498\n",
            "Epoch loss: 486832.01154253876\n",
            "RMSE on dev data: 9.87907\n",
            "RMSE on train data: 10.00493\n",
            "Epochs:989\tstep:50\tbatch_loss:746.8861\n",
            "Epochs:989\tstep:100\tbatch_loss:783.1123\n",
            "Epochs:989\tstep:150\tbatch_loss:774.3029\n",
            "Epochs:989\tstep:200\tbatch_loss:766.5445\n",
            "Epochs:989\tstep:250\tbatch_loss:928.1823\n",
            "Epochs:989\tstep:300\tbatch_loss:799.2184\n",
            "Epochs:989\tstep:350\tbatch_loss:772.2165\n",
            "Epochs:989\tstep:400\tbatch_loss:774.8195\n",
            "Epochs:989\tstep:450\tbatch_loss:782.6709\n",
            "Epochs:989\tstep:500\tbatch_loss:728.1720\n",
            "Epochs:989\tstep:550\tbatch_loss:797.1908\n",
            "Epochs:989\tstep:600\tbatch_loss:738.2211\n",
            "Epoch loss: 486813.4874570433\n",
            "RMSE on dev data: 9.87904\n",
            "RMSE on train data: 10.00491\n",
            "Epochs:990\tstep:50\tbatch_loss:746.8566\n",
            "Epochs:990\tstep:100\tbatch_loss:783.0832\n",
            "Epochs:990\tstep:150\tbatch_loss:774.2742\n",
            "Epochs:990\tstep:200\tbatch_loss:766.5142\n",
            "Epochs:990\tstep:250\tbatch_loss:928.1522\n",
            "Epochs:990\tstep:300\tbatch_loss:799.1889\n",
            "Epochs:990\tstep:350\tbatch_loss:772.1875\n",
            "Epochs:990\tstep:400\tbatch_loss:774.7906\n",
            "Epochs:990\tstep:450\tbatch_loss:782.6413\n",
            "Epochs:990\tstep:500\tbatch_loss:728.1433\n",
            "Epochs:990\tstep:550\tbatch_loss:797.1615\n",
            "Epochs:990\tstep:600\tbatch_loss:738.1924\n",
            "Epoch loss: 486794.9947663642\n",
            "RMSE on dev data: 9.87902\n",
            "RMSE on train data: 10.00489\n",
            "Epochs:991\tstep:50\tbatch_loss:746.8272\n",
            "Epochs:991\tstep:100\tbatch_loss:783.0541\n",
            "Epochs:991\tstep:150\tbatch_loss:774.2455\n",
            "Epochs:991\tstep:200\tbatch_loss:766.4839\n",
            "Epochs:991\tstep:250\tbatch_loss:928.1223\n",
            "Epochs:991\tstep:300\tbatch_loss:799.1594\n",
            "Epochs:991\tstep:350\tbatch_loss:772.1586\n",
            "Epochs:991\tstep:400\tbatch_loss:774.7617\n",
            "Epochs:991\tstep:450\tbatch_loss:782.6116\n",
            "Epochs:991\tstep:500\tbatch_loss:728.1147\n",
            "Epochs:991\tstep:550\tbatch_loss:797.1323\n",
            "Epochs:991\tstep:600\tbatch_loss:738.1637\n",
            "Epoch loss: 486776.5146204292\n",
            "RMSE on dev data: 9.87899\n",
            "RMSE on train data: 10.00488\n",
            "Epochs:992\tstep:50\tbatch_loss:746.7977\n",
            "Epochs:992\tstep:100\tbatch_loss:783.0250\n",
            "Epochs:992\tstep:150\tbatch_loss:774.2169\n",
            "Epochs:992\tstep:200\tbatch_loss:766.4537\n",
            "Epochs:992\tstep:250\tbatch_loss:928.0923\n",
            "Epochs:992\tstep:300\tbatch_loss:799.1299\n",
            "Epochs:992\tstep:350\tbatch_loss:772.1297\n",
            "Epochs:992\tstep:400\tbatch_loss:774.7328\n",
            "Epochs:992\tstep:450\tbatch_loss:782.5820\n",
            "Epochs:992\tstep:500\tbatch_loss:728.0861\n",
            "Epochs:992\tstep:550\tbatch_loss:797.1030\n",
            "Epochs:992\tstep:600\tbatch_loss:738.1350\n",
            "Epoch loss: 486758.0551070246\n",
            "RMSE on dev data: 9.87897\n",
            "RMSE on train data: 10.00486\n",
            "Epochs:993\tstep:50\tbatch_loss:746.7683\n",
            "Epochs:993\tstep:100\tbatch_loss:782.9960\n",
            "Epochs:993\tstep:150\tbatch_loss:774.1883\n",
            "Epochs:993\tstep:200\tbatch_loss:766.4235\n",
            "Epochs:993\tstep:250\tbatch_loss:928.0624\n",
            "Epochs:993\tstep:300\tbatch_loss:799.1005\n",
            "Epochs:993\tstep:350\tbatch_loss:772.1008\n",
            "Epochs:993\tstep:400\tbatch_loss:774.7039\n",
            "Epochs:993\tstep:450\tbatch_loss:782.5524\n",
            "Epochs:993\tstep:500\tbatch_loss:728.0575\n",
            "Epochs:993\tstep:550\tbatch_loss:797.0738\n",
            "Epochs:993\tstep:600\tbatch_loss:738.1064\n",
            "Epoch loss: 486739.61588194367\n",
            "RMSE on dev data: 9.87894\n",
            "RMSE on train data: 10.00484\n",
            "Epochs:994\tstep:50\tbatch_loss:746.7390\n",
            "Epochs:994\tstep:100\tbatch_loss:782.9670\n",
            "Epochs:994\tstep:150\tbatch_loss:774.1597\n",
            "Epochs:994\tstep:200\tbatch_loss:766.3935\n",
            "Epochs:994\tstep:250\tbatch_loss:928.0325\n",
            "Epochs:994\tstep:300\tbatch_loss:799.0711\n",
            "Epochs:994\tstep:350\tbatch_loss:772.0720\n",
            "Epochs:994\tstep:400\tbatch_loss:774.6751\n",
            "Epochs:994\tstep:450\tbatch_loss:782.5229\n",
            "Epochs:994\tstep:500\tbatch_loss:728.0289\n",
            "Epochs:994\tstep:550\tbatch_loss:797.0447\n",
            "Epochs:994\tstep:600\tbatch_loss:738.0778\n",
            "Epoch loss: 486721.2051240896\n",
            "RMSE on dev data: 9.87892\n",
            "RMSE on train data: 10.00482\n",
            "Epochs:995\tstep:50\tbatch_loss:746.7097\n",
            "Epochs:995\tstep:100\tbatch_loss:782.9380\n",
            "Epochs:995\tstep:150\tbatch_loss:774.1311\n",
            "Epochs:995\tstep:200\tbatch_loss:766.3634\n",
            "Epochs:995\tstep:250\tbatch_loss:928.0027\n",
            "Epochs:995\tstep:300\tbatch_loss:799.0418\n",
            "Epochs:995\tstep:350\tbatch_loss:772.0432\n",
            "Epochs:995\tstep:400\tbatch_loss:774.6463\n",
            "Epochs:995\tstep:450\tbatch_loss:782.4934\n",
            "Epochs:995\tstep:500\tbatch_loss:728.0004\n",
            "Epochs:995\tstep:550\tbatch_loss:797.0156\n",
            "Epochs:995\tstep:600\tbatch_loss:738.0492\n",
            "Epoch loss: 486702.8166539685\n",
            "RMSE on dev data: 9.87889\n",
            "RMSE on train data: 10.00480\n",
            "Epochs:996\tstep:50\tbatch_loss:746.6804\n",
            "Epochs:996\tstep:100\tbatch_loss:782.9091\n",
            "Epochs:996\tstep:150\tbatch_loss:774.1026\n",
            "Epochs:996\tstep:200\tbatch_loss:766.3334\n",
            "Epochs:996\tstep:250\tbatch_loss:927.9729\n",
            "Epochs:996\tstep:300\tbatch_loss:799.0124\n",
            "Epochs:996\tstep:350\tbatch_loss:772.0144\n",
            "Epochs:996\tstep:400\tbatch_loss:774.6175\n",
            "Epochs:996\tstep:450\tbatch_loss:782.4639\n",
            "Epochs:996\tstep:500\tbatch_loss:727.9718\n",
            "Epochs:996\tstep:550\tbatch_loss:796.9864\n",
            "Epochs:996\tstep:600\tbatch_loss:738.0207\n",
            "Epoch loss: 486684.43490889895\n",
            "RMSE on dev data: 9.87887\n",
            "RMSE on train data: 10.00479\n",
            "Epochs:997\tstep:50\tbatch_loss:746.6511\n",
            "Epochs:997\tstep:100\tbatch_loss:782.8802\n",
            "Epochs:997\tstep:150\tbatch_loss:774.0741\n",
            "Epochs:997\tstep:200\tbatch_loss:766.3035\n",
            "Epochs:997\tstep:250\tbatch_loss:927.9431\n",
            "Epochs:997\tstep:300\tbatch_loss:798.9831\n",
            "Epochs:997\tstep:350\tbatch_loss:771.9856\n",
            "Epochs:997\tstep:400\tbatch_loss:774.5888\n",
            "Epochs:997\tstep:450\tbatch_loss:782.4344\n",
            "Epochs:997\tstep:500\tbatch_loss:727.9433\n",
            "Epochs:997\tstep:550\tbatch_loss:796.9574\n",
            "Epochs:997\tstep:600\tbatch_loss:737.9922\n",
            "Epoch loss: 486666.07275780657\n",
            "RMSE on dev data: 9.87885\n",
            "RMSE on train data: 10.00477\n",
            "Epochs:998\tstep:50\tbatch_loss:746.6219\n",
            "Epochs:998\tstep:100\tbatch_loss:782.8514\n",
            "Epochs:998\tstep:150\tbatch_loss:774.0456\n",
            "Epochs:998\tstep:200\tbatch_loss:766.2736\n",
            "Epochs:998\tstep:250\tbatch_loss:927.9135\n",
            "Epochs:998\tstep:300\tbatch_loss:798.9539\n",
            "Epochs:998\tstep:350\tbatch_loss:771.9568\n",
            "Epochs:998\tstep:400\tbatch_loss:774.5601\n",
            "Epochs:998\tstep:450\tbatch_loss:782.4050\n",
            "Epochs:998\tstep:500\tbatch_loss:727.9149\n",
            "Epochs:998\tstep:550\tbatch_loss:796.9283\n",
            "Epochs:998\tstep:600\tbatch_loss:737.9637\n",
            "Epoch loss: 486647.7329600289\n",
            "RMSE on dev data: 9.87882\n",
            "RMSE on train data: 10.00475\n",
            "Epochs:999\tstep:50\tbatch_loss:746.5927\n",
            "Epochs:999\tstep:100\tbatch_loss:782.8225\n",
            "Epochs:999\tstep:150\tbatch_loss:774.0172\n",
            "Epochs:999\tstep:200\tbatch_loss:766.2438\n",
            "Epochs:999\tstep:250\tbatch_loss:927.8838\n",
            "Epochs:999\tstep:300\tbatch_loss:798.9247\n",
            "Epochs:999\tstep:350\tbatch_loss:771.9281\n",
            "Epochs:999\tstep:400\tbatch_loss:774.5314\n",
            "Epochs:999\tstep:450\tbatch_loss:782.3757\n",
            "Epochs:999\tstep:500\tbatch_loss:727.8864\n",
            "Epochs:999\tstep:550\tbatch_loss:796.8993\n",
            "Epochs:999\tstep:600\tbatch_loss:737.9352\n",
            "Epoch loss: 486629.4141898945\n",
            "RMSE on dev data: 9.87880\n",
            "RMSE on train data: 10.00474\n",
            "Epochs:1000\tstep:50\tbatch_loss:746.5635\n",
            "Epochs:1000\tstep:100\tbatch_loss:782.7937\n",
            "Epochs:1000\tstep:150\tbatch_loss:773.9887\n",
            "Epochs:1000\tstep:200\tbatch_loss:766.2140\n",
            "Epochs:1000\tstep:250\tbatch_loss:927.8542\n",
            "Epochs:1000\tstep:300\tbatch_loss:798.8955\n",
            "Epochs:1000\tstep:350\tbatch_loss:771.8994\n",
            "Epochs:1000\tstep:400\tbatch_loss:774.5027\n",
            "Epochs:1000\tstep:450\tbatch_loss:782.3463\n",
            "Epochs:1000\tstep:500\tbatch_loss:727.8580\n",
            "Epochs:1000\tstep:550\tbatch_loss:796.8703\n",
            "Epochs:1000\tstep:600\tbatch_loss:737.9067\n",
            "Epoch loss: 486611.0997390731\n",
            "RMSE on dev data: 9.87878\n",
            "RMSE on train data: 10.00472\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aG5GMZp5hBSR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
